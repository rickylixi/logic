%!TEX program = xelatex
%!Tex encoding = UTF-8
\RequirePackage{fix-cm} %优化字体
\documentclass[aspectratio=43,11pt,UTF8,colorlinks,compress,openany]{beamer} %aspectratio=169 %169宽屏 43窄屏[handout]

\input{preamble-beamer}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Universal Artificial Intelligence}
\author{
	{\includegraphics[width=0.15\textwidth,angle=0,origin=c]{img/lixi.pdf}}\\[1mm]
	{\includegraphics[width=0.15\textwidth,angle=0,origin=c]{img/wechat.jpg}}\\[1mm]
	\normalsize Department of Philosophy\\
	\normalsize Central South University\\
	\normalsize xieshenlixi@163.com
}
\date{\today}
\maketitle
%%%%%%%%%%%% 节前目录 %%%%%%%%%%
\AtBeginSection[]
{
\frame[shrink]{{Contents}
%\begin{multicols}{2}
\tableofcontents[currentsection,hideallsubsections]
%\end{multicols}
}
\addtocounter{framenumber}{-1} %目录页不计页码
}
\AtBeginSubsection[]
{
\frame[shrink]{{Contents}
%\begin{multicols}{2}
\tableofcontents[sectionstyle=show/shaded,subsectionstyle=show/shaded/hide,subsubsectionstyle=hide]
%\end{multicols}
}
\addtocounter{framenumber}{-1} %目录页不计页码
}







\frame{{Philosophy}%\initclock
	\only<1>{\begin{center}
			I'm a philosopher.\\
			What do you expect from me?
	\end{center}}\pause
	\only<2>{\begin{alertblock}{Best philosophy in my eyes}
			\begin{itemize}
				\item Leibniz --- {\em What are the extent and limits of reason?} --- Universal Characteristic \& Rational Calculus.
				\item Cantor --- {\em What is `infinity'?} --- Diagonal Method.
				\item Hilbert --- {\em completeness/conservation/consistency/decidability.} --- G\"odel Incompleteness Theorems/Meta-Math.
				\item Turing --- {\em What is `effective procedure'?} --- Computability.
				\item Kolmogorov --- {\em What is `simplicity'/`randomness'?} --- AIT.
				\item Solomonoff --- {\em What is learnable? How to learn?} --- Universal Induction.
				\item Hutter --- {\em What is `intelligence'?} --- UAI.
			\end{itemize}
	\end{alertblock}}
}

\frame{{Philosophy}
\begin{center}
Could a tadpole hope that it will survive to be a frog?
\end{center}
\begin{itemize}
	\item Metaphysics: What kinds of things exist? (e.g. tadpole, material things, mental states, relationships)
	\item Epistemology: What can we know and how do we know it? Can we ever know about the contents of ``other'' minds?
	\item Philosophy of mind: What are mental states and processes? Are certain material states sufficient to produce mental states?
	\item Ethics: How should we think about the rights of a tadpole? Do we have the right to cause them pain?
	\item Philosophy of science: What are scientific theories? Explanations? Evidence? Can theories ever be proved, or refuted, and if so how? What's the relationship between the development of new concepts and the development of new theories?
	\item Conceptual analysis: What do we mean by $X$? What does it mean to say that a tadpole hope hopes for something?
	\item \dots
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{History of AI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{{Questions}
\begin{itemize}
	\item What is (artificial) intelligence?
	\item What does an intelligent system look like?
	\item Can there be a behavioural criterion for intelligence?
	\item Can computers think?
	\item Can connectionist networks think?
	\item Can physical symbol systems think?
	\item Do computing systems have ``emergent'' properties?
	\item Isn't changing the weight on a neural
link a sort of symbol manipulation?
	\item Do computers have to be conscious to think?
	\item Are thinking computers mathematically possible?
	\item How will we know if we've done it?
	\item If we can do it, should we?
\end{itemize}
}

\frame{{What is (Artificial) Intelligence?}
\begin{table}
\begin{tabu}{c|c|c}
\hline
\textbf{What is AI?} &\textbf{humanly} &\textbf{rationally}\\
\hline
\textbf{Think} &Cognitive Science &Laws of Thought\\
\hline
\textbf{Act} &Turing test, Behaviorism &Doing the Right Thing\\
\hline
\end{tabu}
\end{table}
}

\frame{{Think like a human}
\begin{itemize}
	\item \textbf{Cognitive science:} Models of the human thinking processes.
	\item \textbf{Advantages:} Models of the human thinking processes./Intelligible.
	\item \textbf{Difficulties:} The best artificial design for an intelligent system need not mirror the human mind.
\end{itemize}
\textbf{Physical symbol system hypothesis(Newell \& Simon):} a physical symbol system has the necessary and sufficient means for general intelligent action.\\
Any system (human or machine) exhibiting intelligence must operate by manipulating data structures composed of symbols.
}

\frame{{Act like a human: Turing Test}
\begin{itemize}
	\item Alan M. Turing, ``Computing Machinery and Intelligence''
	\item John R. Searle, ``Minds, Brains, and Programs''
\end{itemize}
\begin{itemize}
	\item Interrogator in one room, human in another, system in a third.
	\item Interrogator in one room, human in another, system in a third.
	\item Interrogator tries to guess which is which.
	\item Interrogator tries to guess which is which.
	\item Chinese Room argument. --- Syntax is insufficient for dealing with semantics.
\end{itemize}
\textbf{Needs:} natural language, knowledge representation, automated reasoning, machine learning.\\
\textbf{Difficulties:} Ambiguous./Not constructive./Cannot be formalized mathematically.\\
\centering If we don't use the Turing test, what measure should we use?
}

\frame{{Think rationally: Logicist AI}
\begin{itemize}
	\item \textbf{Logic:} Automatic reasoning procedure.
	\item \textbf{Advantages:} Precise./Search algorithm.
	\item \textbf{Difficulties:} Formalization of informal knowledge./Computational Cost.
\end{itemize}
}

\frame{{Act rationally: Agents}
\begin{itemize}
	\item \textbf{Rational agent:} Autonomous system, capable of perceiving and interacting with its environment, of exploration (information gathering), learning and adaptation, of formulating goals and designing plans to reach those goals. The agent is rational, in the sense that it acts to achieve the best (expected) outcome, according to a performance measure, conditioned to its knowledge of the world and given computational resources.
\end{itemize}
}

\frame{{How do computers discover new knowledge?}
\begin{itemize}
	\item Fill in gaps in existing knowledge
	\item Emulate the brain
	\item Simulate evolution
	\item Systematically reduce uncertainty
	\item Notice similarities between old and new
\end{itemize}
}

\frame{{Machine Learning}
\begin{itemize}
	\item Supervised Learning
		\begin{itemize}
			\item Learn the relationship between ``input'' $x$ and ``output'' $y$: search for a function $f$, such that $y\approx f(x)$
			\item There is training data with labels available\\
			\textbf{Regression:} $y$ is metric variable (with values in $\mathbb{R}$)\\\textbf{Classification:} $y$ is categorical variable (unordered, discrete).
			\item Semi-supervised learning: also uses available unlabeled data, e.g. assumes that similar inputs have similar outputs.
		\end{itemize}
	\item Unsupervised Learning
		\begin{itemize}
			\item There exist no outputs, search for patterns within the inputs $x$\\
			\textbf{Clustering:} find groups of similar items\\
			\textbf{Dimensionality reduction:} describe data in fewer features\\
			\textbf{Outlier detection:} what is out of the ordinary?\\
			\textbf{Association rules:} which things often happen together?
		\end{itemize}
	\item Reinforcement learning
\end{itemize}
}

\frame{{The Five Tribes of Machine Learning}
\begin{table}
\begin{tabu}{l|l|l}
\hline
\textbf{Tribe} &\textbf{Origins} &\textbf{Master Algorithm}\\
\hline
Symbolists &Logic, philosophy &Inverse deduction\\
\hline
Connectionists &Neuroscience &Backpropagation\\
\hline
Evolutionaries &Evolutionary biology &Genetic programming\\
\hline
Bayesians &Statistics &Probabilistic inference\\
\hline
Analogizers &Psychology &Kernel machines\\
\hline
\end{tabu}
\end{table}
}

\frame{{Symbolists}
\begin{itemize}
	\item All intelligence can be reduced to manipulating symbols
	\item Logic, Decision trees
	\item Inverse deduction
	\item Easy to add knowledge
	\item Can combine knowledge, data, to fill in gaps
	\item Impossible to code everything in rules
\end{itemize}
\begin{table}
\begin{tabu}{l|l}
\hline
Representation &Rules, trees, first order logic rules\\
\hline
Evaluation &Accuracy, information gain\\
\hline
Optimization &Top-down induction, inverse deduction\\
\hline
Algorithms &Decision trees, Logic programs\\
\hline
\end{tabu}
\end{table}
}

\frame{{Connectionists}
\begin{itemize}
	\item Learning is what the brain does: reverse-engineer it
	\item Hebbian learning: Neurons that fire together, wire together
	\item Neural networks
	\item Backpropagation
	\item Can handle raw, high-dimensional data, constructs it own features
	\item Hard to add reasoning/explanations
\end{itemize}
\begin{table}
\begin{tabu}{l|l}
\hline
Representation &Neural network\\
\hline
Evaluation &Squared error\\
\hline
Optimization &Gradient descent\\
\hline
Algorithms &Backpropagation\\
\hline
\end{tabu}
\end{table}
}

\frame{{Evolutionaries}
\begin{itemize}
	\item Natural selection is the mother of all learning
	\item Evolutionary algorithms
	\item Crossover, mutation
	\item Can learn structure, wide hypothesis space
	\item Needs a way to `fill' the structure
\end{itemize}
\begin{table}
\begin{tabu}{l|l}
\hline
Representation &Genetic programs (often trees)\\
\hline
Evaluation &Fitness function\\
\hline
Optimization &Genetic search\\
\hline
Algorithms &Genetic programming (crossover, mutation)\\
\hline
\end{tabu}
\end{table}
}

\frame{{Bayesians}
\begin{itemize}
	\item Learning is a form of uncertain inference
	\item Graphical models, Gaussian processes, HMMs, Kalman filter
	\item Uses Bayes theorem to incorporate new evidence into our beliefs
	\item Can deal with noisy, incomplete, contradictory data
	\item Depends on the prior
	\item Hard to do unite logic and probability
\end{itemize}
\begin{table}
\begin{tabu}{l|l}
\hline
Representation &Graphical models, Markov networks\\
\hline
Evaluation &Posterior probability\\
\hline
Optimization &Probabilistic inference\\
\hline
Algorithms &Bayes theorem and derivates\\
\hline
\end{tabu}
\end{table}
}

\frame{{Analogizers}
\begin{itemize}
	\item You are what you resemble
	\item Recognizes similarities between situations and infers other similarities
	\item k-Nearest Neighbor, Support Vector Machines
	\item Transfer solution from previous situations to new situations
	\item Hard to do rules and structure
\end{itemize}
\begin{table}
\begin{tabu}{l|l}
\hline
Representation &Memory, support vectors\\
\hline
Evaluation &Margin\\
\hline
Optimization &Kernel machines\\
\hline
Algorithms &k-Nearest Neighbor, Support Vector Machines\\
\hline
\end{tabu}
\end{table}
}

\frame{{The Master Algorithm?}
\begin{table}
\begin{tabu}{l|l|l}
\hline
\textbf{Tribe} &\textbf{Problem} &\textbf{Solution}\\
\hline
Symbolists &Knowledge composition &Inverse deduction\\
\hline
Connectionists &Credit assignment &Backpropagation\\
\hline
Evolutionaries &Structure discovery &Genetic programming\\
\hline
Bayesians &Uncertainty &Probabilistic inference\\
\hline
Analogizers &Similarity &Kernel machines\\
\hline
\end{tabu}
\end{table}
\begin{itemize}
	\item Representation: The hypothesis space.
		\begin{itemize}
			\item Probabilistic logic
			\item Weighted formulas $\to$ Distribution over states
		\end{itemize}
	\item Evaluation: How to choose one hypothesis over the other?
		\begin{itemize}
			\item Posterior probability
			\item User-defined objective function
		\end{itemize}
	\item Optimization: How do we search the hypothesis space?
		\begin{itemize}
			\item Formula discovery: Genetic programming
			\item Weight learning: Backpropagation
		\end{itemize}
\end{itemize}
\begin{center}
Elegant/Extensible/Expressive/Efficient/Educable/Evolvable?
\end{center}
}

\frame{{Learn to Learn}
\begin{enumerate}
	\item Good Old-Fashioned AI
		\begin{itemize}
			\item Handcraft predictions
			\item Learn nothing
		\end{itemize}
	\item Shallow Learning
		\begin{itemize}
			\item Handcraft features
			\item Learn predictions
		\end{itemize}
	\item Deep Learning
		\begin{itemize}
			\item Handcraft algorithm (optimiser, target, architecture, \dots)
			\item Learn features and predictions end-to-end
			\end{itemize}
	\item Meta Learning
		\begin{itemize}
			\item Handcraft nothing
			\item Learn algorithm and features and predictions end-to-end
		\end{itemize}
\end{enumerate}
}

\frame{{Can Machines Think?}
\begin{itemize}
	\item Theological objections.
	\item Argument from informality of behavior. --- Human behavior is far too complex to be captured by any simple set of rules./Learning from experience.
	\item Argument from incompleteness theorems. --- No formal system incl. AIs, but only humans can ``see'' that G\"odel's unprovable sentence is true./Lucas cannot consistently assert that this sentence is true.
	\item Machines can't be conscious or feel emotions. --- Reductionism doesn't really answer the question: why can't machines be conscious or feel emotions?
	\item Machines don't have Human Quality $X$.
	\item Machines just do what we tell them to do. --- Maybe people just do what their neurons tell them to do.
	\item Machines are digital. Mental states can emerge from neural substrate only. --- Only the functionality/behavior matters.
	\item Non-Computable Physics \& Brains.
\end{itemize}
}

\frame{{Brain Dissection}
\begin{itemize}
	\item The ``brain in a vat'' experiment: (no) real experience.
	\item The brain prosthesis experiment:
		\begin{itemize}
			\item Replacing some neurons in the brain by functionally identical electronic prostheses would neither effect external behavior nor internal experience of the subject.
			\item Successively replace one neuron after the other until the whole brain is electronic.
		\end{itemize}
\end{itemize}
}

\frame{{Fermi Paradox --- Where are the aliens?}
\begin{itemize}
	\item There are none, i.e. we're all alone.
	\item We can't detect them because\dots
		\begin{itemize}
			\item we're too primitive or too far apart
			\item there are predators or all fear them
			\item we're lied to, live in a simulation
			\item \dots
		\end{itemize}
\end{itemize}
}

\frame{{Why do we need to align machine agents?}
\begin{itemize}
	\item \textbf{Goal orthogonality.}\\
	Intelligence and final goals are orthogonal: Any utility function can be combined with a powerful epistemology and decision theory.
	\item \textbf{Instrumental convergence.}\\
	Different long-term goals imply similar short-term strategies.
		\begin{itemize}
			\item Self-preservation
			\item Retention of goals through time
			\item Cognitive enhancement
			\item Technological perfection
			\item Resource acquisition
			\item \dots
		\end{itemize}
	\item \textbf{Capability gain.}\\
	There are potential ways for artificial agents to greatly gain in cognitive power and strategic options.
	\item \textbf{Alignment difficulty.}\\
	It's hard to transmit our values to AI systems or avert adversarial incentives.
\end{itemize}
}

\frame{{Asimov's Laws of Robotics}
\begin{itemize}
	\item \textbf{Law Zero}

	A robot may not injure humanity, or, through inaction, allow humanity to come to harm.
	\item \textbf{Law One}

	A robot may not injure a human being or, through inaction, allow a human being to come to harm, unless this would violate a higher order law.
	\item \textbf{Law Two}

	A robot must obey orders given it by human beings except where such orders would conflict with a higher order law.
	\item \textbf{Law Three}

	A robot must protect its own existence as long as such protection does not conflict with a higher order law.
\end{itemize}
}

\frame{{An Extended Set of the Laws of Robotics}
\resizebox{\textwidth}{!}{
\begin{minipage}{16.1cm}
\begin{itemize}
	\item \textbf{The Meta-Law}

A robot may not act unless its actions are subject to the Laws of Robotics.
	\item \textbf{Law Zero}

A robot may not injure humanity, or, through inaction, allow humanity to come to harm.
	\item \textbf{Law One}

A robot may not injure a human being, or, through inaction, allow a human being to come to harm, unless this would violate a higher-order Law.
	\item \textbf{Law Two}
		\begin{enumerate}
			\item A robot must obey orders given it by human beings, except where such orders would conflict with a higher-order Law.
			\item A robot must obey orders given it by superordinate robots, except where such orders would conflict with a higher-order Law.
		\end{enumerate}
	\item \textbf{Law Three}
		\begin{enumerate}
			\item A robot must protect the existence of a superordinate robot as long as such protection does not conflict with a higher-order Law.
			\item A robot must protect its own existence as long as such protection does not conflict with a higher-order Law.
		\end{enumerate}
	\item \textbf{Law Four}

A robot must perform the duties for which it has been programmed, except where that would conflict with a higher-order law.
	\item \textbf{The Procreation Law}

A robot may not take any part in the design or manufacture of a robot unless the new robot's actions are subject to the Laws of Robotics.
\end{itemize}
\end{minipage}}
}

\frame{{Ethical Concerns}
\begin{itemize}
	\item Is it morally justified to create intelligent systems with these constraints?
	\item Would it be possible to do so?
	\item Should intelligent systems have free will? Can we prevent them from having free will? What could it mean for a machine to have its own goals? Do we have a kind of freedom machines could never have?
	\item Will intelligent systems have consciousness?
	\item If they do, will it drive them insane to be constrained by artificial ethics placed on them by humans?
	\item If intelligent systems develop their own ethics and morality, will we like what they come up with?
\end{itemize}
}

\frame{{Machine Ethics}
\begin{itemize}
	\item People might lose their jobs to automation.
	\item People might have too much (or too little) leisure time.
	\item People might lose their sense of being unique.
	\item People might lose some of their privacy rights.
	\item The use of AI systems might result in a loss of accountability.\\
	--- Who is responsible if a physician follows the advice of a medical expert system, whose diagnosis turns out to be wrong?
	\item The success of AI might mean the end of the human race.
\end{itemize}
}

\frame{{What If We Do Succeed?}
\begin{itemize}
	\item Natural selection is replaced by artificial evolution.\\
	--- AI systems will be our mind children.
	\item Once a machine surpasses the intelligence of a human it can design even smarter machines.
	\item This will lead to an intelligence explosion and a technological singularity at which the human era ends.
	\item Prediction beyond this event horizon will be impossible.
	\item Alternative $1$: We keep the machines under control.
		\begin{itemize}
			\item Capability control (limiting what the system can or does do).
			\item Motivation selection (controlling what the system wants to do).
		\end{itemize}
	\item Alternative $2$: Humans merge with or extend their brain by AI.
\end{itemize}
\begin{tcolorbox}[colback=cyan!3,colframe=back!85,title=Single-Shot Situation]
Our first superhuman AI must be a safe one for we may not get a second chance!
\end{tcolorbox}
}

\frame{{Singularity}
\begin{itemize}
	\item Speed Explosion (Time).\\
	--- Computing speed doubles every two subjective years of work.
	\item Population Explosion (Quantitative).\\
	--- Computing costs halve for a certain amount of work.
	\item Intelligence Explosion (Qualitative).\\
	--- Proportionality Thesis: An increase in intelligence leads to similar increases in the capacity to design intelligent systems.
\end{itemize}
}

\frame{{Capability Control Methods}
\begin{itemize}
	\item Boxing: the system can only act through restricted channels.\\
	--- The AI could persuade someone to free it from its box.
	\item Incentives: access to other AIs, cryptographic reward tokens.
	\item Stunting: imposing constraints on the system's cognitive abilities
	\item Tripwires: diagnostic tests run periodically to check for dangerous activity, with shutdown a consequence of detection.
\end{itemize}
}

\frame{{Motivation Selection Methods}
\begin{itemize}
	\item Direct specification of motivations
		\begin{itemize}
			\item Rule-based methods: give the machine a set of rules that define its final goals.
			\item Direct consequentialist methods: specify some measure that is to be maximised. (e.g. human happiness.)
		\end{itemize}
	\item Augmentation
		\begin{itemize}
			\item Start with an AI with human-level intelligence, that has an acceptable motivation system: then enhance its cognitive faculties to make it superintelligent.
		\end{itemize}
	\item Indirect normativity
		\begin{itemize}
			\item Rather than specifying a normative standard directly, we specify a process for deriving a standard. We then build the system so it is motivated to carry out this process.
			\item Value learning. --- inverse reinforcement learning. --- wireheading.
		\end{itemize}
\end{itemize}
}

\frame{{External Wireheading \& Internal Wireheading}
\begin{figure}[!htbp]
	\includegraphics[width=.5\textwidth,angle=0,origin=c]{img/deer-horse}
\end{figure}
\begin{center}
\resizebox{.9\textwidth}{!}{\begin{minipage}{\textwidth}\begin{figure}[!htb]
\centering
        \begin{tikzpicture}[scale=0.75]
          \draw(0,0)circle(2.2 and 1.1)node(C){\Huge\textcolor{purple}{\textbf{\underline{Agent}}}};
          \draw(10,0)circle(4 and 3)node(P){\Huge\textbf{\underline{Environment}}};
          \path (C) edge [very thick,loop above] node {\textcolor{red}{utility}} (C);
          \path (P) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{perception}} (C);
          \path (C) edge [->,very thick,bend left] node {\huge\textcolor{red}{action}} (P);
        \end{tikzpicture}
\end{figure}\end{minipage}}
\end{center}
}

\frame{{Prisoner's Dilemma}
\begin{itemize}
	\item Difficult to prevent arms races.
	\item The winner takes all (of what remains).
	\item Arms races are dangerous because \textcolor{red}{parties sacrifice safety for speed!} --- When headed the wrong way, the last thing we need is progress.
\end{itemize}
\begin{tcolorbox}[colback=cyan!3,colframe=back!85,title=International Cooperation]
In face of uncertainty, cooperation is robust!
\end{tcolorbox}
Why should I care about the world when I am dead and gone? I want it to go fast, damn it! This increases the chance I have of experiencing a more technologically advanced future.
}

\frame{{AI Alignment}
\begin{itemize}
	\item Important to ensure it's aligned with our interests
		\begin{itemize}
			\item But how do we specify beneficial goals?
			\item How do we make sure system actually pursues them?
			\item How do we correct the system if we get it wrong?
		\end{itemize}
	\item Want solid theoretical understanding of problem \& solution
		\begin{itemize}
			\item What is correct reasoning and decision making?
			\item Probability theory, decision theory, game theory, statistical learning theory, Bayesian networks, formal verification, \dots
		\end{itemize}
\end{itemize}
}

\frame{{Technical Research Questions}
\begin{enumerate}
	\item Reliable self-modification
	\item Logical uncertainty (reasoning without logical omniscience)
	\item Reflective stability of decision theory
	\item Decision theory for Newcomb-like problems
	\item Corrigibility (accepting modifications)
	\item The shutdown problem
	\item Value loading
	\item Indirect specification of decision theory
	\item Domesticity (goal specification for limited impact)
	\item The competence gap
	\item Weighting options or outcomes for variance-normalizing solution to moral uncertainty
	\item Program analysis for self-improvement
	\item Reading values and beliefs of AIs
	\item Pascal's mugging
	\item Infinite ethics
	\item Mathematical modelling of intelligence explosion
\end{enumerate}
}

\frame{{}
\begin{quote}
Before the prospect of an intelligence explosion, we humans are like children playing with a bomb.
Such is the mismatch between the power of our
play-thing and the immaturity of our conduct.
Superintelligence is a challenge for which we are not ready now and will not be ready for a long time. We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.\par\hfill --- {\sl Nick Bostrom}
\end{quote}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Philosophy of Induction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{History}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{{Hume \& Russell}
	\begin{proposition}[Hume]
		Induction is just a mental habit, and necessity is something in the mind and not in the events.
	\end{proposition}
		\begin{figure}[htb]
			\subfigure{\includegraphics[height=0.3\textwidth,angle=0,origin=c]{img/chicken}}
			\subfigure{\includegraphics[height=0.3\textwidth,angle=0,origin=c]{img/russell-painting}}
		\end{figure}
}

\frame{{Leibniz-Wittgenstein-Goodman}
	\begin{proposition}[Leibniz]
		Since for any finite number of points there are always infinitely many curves going through them, any finite set of data is compatible with infinitely many inductive generalizations. 
	\end{proposition}
	\begin{proposition}[Wittgenstein]
		Since any finite course of action is in accord with infinitely many rules, no universal rule can be learned by examples.
	\end{proposition}
	\begin{proposition}[Goodman]
		All emeralds discovered till 2050 are green, and blue thereafter. 
	\end{proposition}
}

\frame{{Mill --- Homogeneous Universe}
	\begin{proposition}[Mill]
		Induction can be turned into a deduction, by adding principles about the world (such as `the future resembles the past', or `space-time is homogeneous').
	\end{proposition}
}

\frame{{Homogeneous?}
	\begin{problem}
		$1,3,5,7,9,11,13,15,\textcolor{purple}{\bm{?}}$
	\end{problem}
	\begin{solution}
		\begin{gather}
		2n-1\tag{\textcolor{red}{17}?}\\
		2n-1+\prod\limits_{i=1}^8(n-i)\tag{\textcolor{red}{17+8!}?}\\
		\vdots \tag{??}
		\end{gather}
	\end{solution}
}

\frame{{Epicurus vs Occam}\vspace{-2ex}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=0.2\textwidth,angle=0,origin=c]{img/overfitting}
		\end{center}
	\end{figure}\vspace{-2ex}
	\begin{proposition}[Epicurus]
		If more than one hypothesis is consistent with the observations, keep them all.
	\end{proposition}
	\begin{proposition}[Occam's Razor]
		Among the hypotheses that are consistent with the observed phenomena, select the simplest one.
	\end{proposition}
	\begin{itemize}
		\item Entities should not be multiplied beyond necessity.
		\item Wherever possible, logical constructions are to be substituted for inferred entities.
		\item It is vain to do with more what can be done with fewer.
	\end{itemize}
}

\frame{{Why Simplicity? --- Gestalt Psychology}
	\begin{center}
		\begin{figure}
			\includegraphics[width=0.6\textwidth,angle=0,origin=c]{img/boxcover}
		\end{figure}
		\begin{figure}
			\includegraphics[width=0.6\textwidth,angle=0,origin=c]{img/box.pdf}\caption{Gestalt Psychology}
		\end{figure}
	\end{center}
}

\frame{{Why Simplicity?}
	\begin{quote}
		God does not play dice.\\
		God always takes the \textcolor{red}{simplest} way.\\
		Subtle is the Lord, but \textcolor{red}{\textbf{malicious}} He is not.\\
		The most incomprehensible thing about the world is that it is \textcolor{red}{comprehensible}.\\
		What really interests me is whether God could have created the world any differently; in other words, whether the requirement of logical simplicity admits a margin of freedom.\\
		When I am judging a theory, I ask myself whether, if I were God, I would have arranged the
		world in such a way. \par\hfill --- {\sl Einstein}
	\end{quote}
	\begin{itemize}
		\item Principle of least/stationary action
		\item Noether's theorem
		\item $\dots$
	\end{itemize}
}

\frame{{Why Simplicity?}
	\begin{quote}
		\[\begin{array}{ccccc}
		\hline
		\text{program} & \xrightarrow{ \text{Computer}} &  \text{output}\\
		\hline
		\text{axioms} & \xrightarrow{ \text{Deduction}} &  \text{theorems}\\
		\hline
		\text{scientific theory} & \xrightarrow{ \text{Calculations}} &  \text{experimental data}\\
		\hline
		\text{encoded message} & \xrightarrow{ \text{Decoder}} &  \text{original message}\\
		\hline
		\text{software} & \xrightarrow{ \text{Universal Constructor}} &  \text{physical system}\\
		\hline
		\text{DNA} & \xrightarrow{ \text{Pregnancy}} &  \text{organism}\\
		\hline
		\text{Ideas} & \xrightarrow{ \text{Mind of God}} &  \text{Universe}\\
		\hline
		\end{array}\]
	\end{quote}
}

\frame{{Popper --- The Logic of Scientific Discovery}
	\begin{columns}[onlytextwidth]
		\column{.5\textwidth}
			\resizebox{.65\textwidth}{!}{
				\begin{minipage}{\textwidth}\centering
					\begin{tikzpicture}[scale=0.5]
					\draw(0,-6)ellipse(3 and 2)node(h){\Large hypothesis};
					\draw(-6,0)ellipse(3 and 2)node(p){\Large problem};
					\draw(6,0)ellipse(3 and 2)node(e){\Large experiment};
					\draw(0,6)circle(3 and 2)node(f){\Large falsification};
					
					\node at (0,0){\includegraphics[width=0.5\textwidth,angle=0,origin=c]{img/popper}};
					
					\draw [->,ultra thick,red] (p) to [bend right] (h);
					\draw [->,ultra thick,red] (h) to [bend right] (e);
					\draw [->,ultra thick,red] (e) to [bend right] (f);
					\draw [->,ultra thick,red] (f) to [bend right] (p);
					\end{tikzpicture}
			\end{minipage}}
		\column{.5\textwidth}
			\begin{proposition}[Popper]
				\begin{itemize}
					\item A single observational event may prove hypotheses wrong, but no finite sequence of events can verify them correct. 
					\item Induction is theoretically unjustifiable and becomes in practice the choice of the simplest generalization that resists falsification.
					\item The simpler a hypothesis, the easier it is to be falsified.
					\item Falsifiability is as subjective as simplicity, there is no objective criterion.
				\end{itemize}
			\end{proposition}
	\end{columns}	
}

\frame{{Kaynes $\implies$ Carnap}
	\begin{columns}
		\column{0.75\textwidth}
			\begin{itemize}
				\item Assign to inductive generalizations probabilities that should converge to $1$ as the generalizations are supported by more and more independent events.\\
				\hfill --- Kaynes
				\item Observational events provide, if not proofs, at least positive confirmations of scientific hypotheses. Chose the generalization that confirm more evidence.\\
				\hfill --- Carnap
			\end{itemize}
		\column{0.23\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth]{img/carnap}
			\end{figure}
	\end{columns}
}

\frame{{Philosophy of Induction}
	\centerline{\Large What is learnable? How to learn?}
	\centerline{\Large How can we know that what we learned is true?}
	\begin{alertblock}{History}
		\begin{center}
			Possible Worlds/Hypothesis (\textcolor{blue}{Epicurus}/Leibniz)\\
			+\\ 
			\textcolor{darkgreen}{Homogeneous Universe(s)} (Mill/\textcolor{red}{Turing})\\
			+\\
			Simplicity Criterion (\textcolor{cyan}{Occam}/\textcolor{red}{Kolmogorov})\\
			+\\
			\textcolor{purple}{Prior Belief} (Carnap/\textcolor{red}{Solomonoff})\\
			+\\
			Update Belief (\textcolor{red}{Bayes})\\
			$\Downarrow$\\
			Convergence to \textcolor{darkgreen}{Truth}
		\end{center}
	\end{alertblock}\vspace{-2ex}
	\[\textcolor{red}{P(h|e)}=\dfrac{\textcolor{cyan}{P(e|h)} \textcolor{purple}{P(h)}}{\textcolor{blue}{\sum\limits_{h\in\textcolor{darkgreen}{\mathcal{H}}}P(e|h) P(h)}}\xrightarrow{\ell(e)\to\infty}\textcolor{darkgreen}{1}\]
}

\frame{{MDL vs Bayesian Mixture}
	\begin{center}
		\resizebox{0.9\textwidth}{!}{\hspace{1.5em}
				\xymatrix @C=1pc{&&&*+++[o][F--]{\txt{\textit{\textbf{\emph{\underline{\textcolor{blue}{{\text{Model}}}}}}}}}&&\\
					&&&&
					\ar@3{-->}_*++[o]{\textcolor{red}{\text{\footnotesize Deduction}}} "1,4";"5,7"
					&&&\\
					\ar@3{-->}_*++[o]{\textcolor{red}{\text{\footnotesize Induction}}} "5,1";"1,4"
					&&&&&\\
					\ar@1{->}^*++[o]{\textcolor{purple}{\text{\footnotesize Prediction}}} "5,1";"5,7"\\
					*+++[o][F--]{\txt{\textit{\textbf{\emph{\underline{\textcolor{blue}{{\text{Sample}}}}}}}}}&&&&&&*+++[o][F--]{\txt{\textit{\textbf{\emph{\underline{\textcolor{blue}{{\text{Data}}}}}}}}}}}
	\end{center}

	When solving a problem of interest, do not solve a more general problem as an intermediate step.
}

\frame{{Bayesianism}
		\setlength\abovedisplayskip{0pt}
		\setlength\belowdisplayskip{0pt}
		\vspace{-1ex}
	\begin{theorem}[Convergence Theorem]
		\resizebox{\textwidth}{!}{
			\begin{minipage}{\textwidth}
				\begin{align*}
				\sum\limits_{t=1}^n\mathbb{E}_\mu{\left[\left(\sqrt{\dfrac{\rho(a|x_{<t})}{\mu(a|x_{<t})}}-1\right)^2\right]}\leq\sum\limits_{t=1}^n\mathbb{E}_\mu\left[\sum\limits_{a\in\mathcal{X}}\left(\sqrt{\rho(a|x_{<t})}-\sqrt{\mu(a|x_{<t})}\right)^2\right] &\leq D_n(\mu\|\rho)\\
				\sum\limits_{t=1}^n\mathbb{E}_\mu\left[\sum\limits_{a\in\mathcal{X}}\left(\rho(a|x_{<t})-\mu(a|x_{<t})\right)^2\right] &\leq D_n(\mu\|\rho)\\
				\dfrac{1}{2n}\left(\sum\limits_{t=1}^n\mathbb{E}_\mu\left[\sum\limits_{a\in\mathcal{X}}\big|\rho(a|x_{<t})-\mu(a|x_{<t})\big|\right]\right)^2 &\leq D_n(\mu\|\rho)
				\end{align*}
		\end{minipage}}
		where
		\[D_n(\mu\|\rho):= \mathbb{E}_\mu\left[\ln\frac{\mu(x_{1:n})}{\rho(x_{1:n})}\right]\]
	\end{theorem}\vspace{-1ex}
	\begin{theorem}
		\[\xi=\argmin\limits_\rho \mathbb{E}_w\left[D(\mu\|\rho)\right]\quad\text{where}\quad \xi(x):=\sum\limits_{\nu\in\mathcal{M}} w_\nu\nu(x)\]
	\end{theorem}
}

\frame{{Bayesian Decisions}
	\begin{center}
		Suppose\; $\mathit{Loss}(x_t,y_t)\in[0,1]$
	\end{center}
	\[
	y_t^{\Lambda_\rho}(x_{<t}) \;:=\; \arg\min\limits_{y_t}\sum\limits_{x_t}\rho(x_t|x_{<t})\mathit{Loss}(x_t,y_t)
	\]
	\begin{align*}
	&L^{\Lambda_\rho}(x_{<t}) \;:=\; \mathbb{E}_\mu\left[\mathit{Loss}\left(x_t,y_t^{\Lambda_\rho}\right)\,\middle|\, x_{<t}\right]\\
	&L_n^{\Lambda_\rho} \;:=\; \sum\limits_{t=1}^n\mathbb{E}_\mu\left[L^{\Lambda_\rho}(x_{<t})\right]
	\end{align*}
	\begin{theorem}
		\setlength\abovedisplayskip{0pt}
		\setlength\belowdisplayskip{0pt}
		\[
		\left(\sqrt{L_n^{\Lambda_\xi}}-\sqrt{L_n^{\Lambda_\mu}}\right)^2
		\leq 2\sum\limits_{t=1}^n \mathbb{E}_\mu\!\left[\sum\limits_{a\in\mathcal{X}}\left(\sqrt{\xi(a|x_{<t})}-\sqrt{\mu(a|x_{<t})}\;\right)^2\right]
		\leq 2D_n(\mu\|\xi)
		\]
		\[
		L_n^{\Lambda_\xi}-L_n^{\Lambda_\mu}\leq 2D_n(\mu\|\xi)+2\sqrt{L_n^{\Lambda_\mu}D_n(\mu\|\xi)}
		\]
	\end{theorem}
}

\frame{{Problem}
	\begin{tcolorbox}[title=How to choose the \textcolor{red}{model class} and \textcolor{red}{prior}?]
		\begin{itemize}
			\item choose the smallest model class that will contain the true environment.
			\item choose the priors that best reflect a rational a-priori belief in each of these environments.
			\begin{enumerate}
				\item Convergence of Bayesian mixture to true environment.
				\item Confirmation of ``the sun will always rise''.
				\item Invariance Criterion.\\
				reparametrization \& regrouping invariant.
			\end{enumerate}
		\end{itemize}
	\end{tcolorbox}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{How to Choose the Prior?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{{Invariance Criterion}
	By applying some principle to a parameter $\theta$ we get prior $w(\theta)$. If we consider some new parametrization $\theta'$ via $f\colon\theta\mapsto\theta'$, then we get a prior $\tilde{w}(\theta')$ by transforming the original prior via $f$.\\
	for discrete class $\mathcal{M}$,
	\[\tilde{w}(\theta'):=\sum\limits_{\theta:f(\theta)=\theta'}\!\!\!\!w(\theta)\]
	for continuous parametric class $\mathcal{M}$, 
	\[\tilde{w}(\theta'):=\int\!\!\delta(f(\theta)-\theta') w(\theta)\mathrm{d}\theta\tag{\text{Dirac-delta}}\]	
	\textcolor{red}{Regrouping-invariant}: $$\tilde{w}(\theta')=w'(\theta')$$ where $w'(\theta')$ is obtained by applying the same principle to the new parametrization.\\
	We say the principle is \textcolor{red}{reparametrization-invariant} when $f$ is bijective.
}

\frame{{How to Assign Prior? Indifference Principle/MaxEnt}
	\begin{columns}[onlytextwidth]
		\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth]{img/coin}
			\end{figure}
		\column{0.5\textwidth}
			\centering\includegraphics[width=.9\textwidth]{img/dart}
	\end{columns}
}

\frame{{How to Assign Prior?}
	\begin{itemize}
		\item The principle of indifference.
		Assume $w(\theta)=1$ and $\theta'=\sqrt{\theta}$.
		\[\tilde{w}(\theta')=\int_{0}^{1}\!\!\delta\big(\sqrt{\theta}-\theta'\big)w(\theta)\mathrm{d}\theta=2\sqrt{\theta}\neq w'(\theta')\]
		\item Maximize the entropy subject to some constraints provided by empirical data or considerations of symmetry, probabilistic laws, and so on.
		\item Occam's razor.
	\end{itemize}
}

\frame{{How to confirm ``All Ravens are Black''?}
	\begin{figure}
		\includegraphics[width=0.9\textwidth]{img/dark}
	\end{figure}
}

\frame{{Problem~$1$ --- All Ravens are Black~\colorbox{lightgreen}{\textcolor{blue}{$\theta=1$}}}
	\noindent Suppose $\theta$ is the percentage of ravens that are black.\\
	\noindent ``All ravens are black'' $\equiv$~\colorbox{lightgreen}{\textcolor{blue}{$\theta=1$}}.
	or, \colorbox{lightgreen}{$\textcolor{blue}{1^\infty}$} or, \colorbox{lightgreen}{$\textcolor{blue}{\forall x\colon R(x)\to B(x)}$}
	\[P(\theta=1)=\int_1^1\!\!w(\theta)\mathrm{d}\theta=0 \tag{\textcolor{red}{$0$~prior}}\]
	\[\Downarrow\]
	\[P(\theta=1|1^n)=\dfrac{P(1^n|\theta=1)P(\theta=1)}{P(1^n)}=0 \tag{\textcolor{red}{$\times$}}\]
}

\frame{{Problem~$2$ --- The Sun will always Rise~\colorbox{lightgreen}{$\textcolor{blue}{1^\infty}$}}
	\noindent Indifference Principle
	\[\left.
	\begin{aligned}
	&\int_0^1\!\! w(\theta)\mathrm{d}\theta=1\\
	&\forall\theta,\theta'\colon w(\theta)=w(\theta')
	\end{aligned}
	\right\rbrace\implies\forall\theta\colon w(\theta)=1
	\]
	The sun will rise tomorrow.~\textcolor{red}{$\checkmark$}
	\[
	P(x)=\int_0^1\!\! P(x|\theta)w(\theta)\mathrm{d}\theta \implies P(1|1^n)=\dfrac{n+1}{n+2}
	\]
	The sun will always rise.~\textcolor{red}{$\times$}
	\[P(1^{\infty}|1^n)=\lim\limits_{k\to\infty}P(1^k|1^n)=\lim\limits_{k\to\infty}\dfrac{n+1}{n+k+1}=0\]
}

\frame{{Solution~$1$ --- Soft Hypothesis --- No absolute truth!}
	\vspace{-1em}
	\[H_\varepsilon=\left\{\theta\colon\theta\in(1-\varepsilon,1]\right\}\]
	\[P(H_\varepsilon)=\int_{1-\varepsilon}^1\!\! w(\theta)\mathrm{d}\theta=\varepsilon>0\]
	\begin{align*}
	P(H_\varepsilon|1^n)&=\int_{1-\varepsilon}^1\!\! w(\theta|1^n)\mathrm{d}\theta\\
	&= \int_{1-\varepsilon}^1\!\! \frac{P(1^n|\theta) w(\theta)}{P(1^n)}\mathrm{d}\theta\\
	&=\int_{1-\varepsilon}^1\!\! (n+1)\theta^n \mathrm{d}\theta\\
	&=\left.\theta^{n+1}\right|_{1-\varepsilon}^1\\
	&=1-(1-\varepsilon)^{n+1}\xrightarrow{n\to\infty} 1
	\end{align*}
}

\frame{{Solution~$2$ --- ad hoc}
	\[\textcolor{red}{w(\theta):=\dfrac{1}{2}(1+\delta(1-\theta))}\]
	\[\text{Dirac-delta sifting property:}\quad\int\!\! f(\theta)\delta(\theta-a)\,\mathrm{d}\theta=f(a)\]
	\begin{align*}
	P(x)&=\int_0^1\!\! P(x|\theta)w(\theta)\mathrm{d}\theta\\
	&=\int_0^1\!\! \theta^s(1-\theta)^f\cdot \frac{1}{2}(1+\delta(1-\theta))\mathrm{d}\theta\\
	&=\frac{1}{2}\int_0^1\!\! \theta^s(1-\theta)^f (1+\delta(\theta-1))\mathrm{d}\theta\\
	&=\dfrac{1}{2}\left(\dfrac{s!f!}{(s+f+1)!}+1^s\cdot (1-1)^f\right) &\text{[sifting property]}\\
	&=\dfrac{1}{2}\left(\dfrac{s!f!}{(s+f+1)!}+\delta_{f,0}\right)
	\end{align*}
}

\frame{{Solution~$2$ --- ad hoc}
	\[P(1^\infty|1^n)=\lim\limits_{k\to\infty}\dfrac{P(1^{n+k})}{P(1^n)}=\lim\limits_{k\to\infty}\dfrac{\frac{1}{2}\left(\frac{(n+k)!0!}{(n+k+1)!}+1\right)}{\frac{1}{2}\left(\frac{n!0!}{(n+1)!}+1\right)}=\dfrac{n+1}{n+2}\xrightarrow{n\to\infty} 1\]
	\[P(\theta\geq a)=\int_{a}^1\!\frac{1}{2}(1+\delta(\theta-1))\mathrm{d}\theta=1-\frac{1}{2}a\]
	\[P(\theta=1)=\frac{1}{2}\]
	\[P(\theta=1|1^n)=\dfrac{P(1^n|\theta=1)P(\theta=1)}{P(1^n)}=\dfrac{1\cdot\frac{1}{2}}{\frac{1}{2}\left(\frac{n!0!}{(n+1)!}+1\right)}=\dfrac{n+1}{n+2}\xrightarrow{n\to\infty}1\]
	\[\text{\textcolor{red}{Why $\theta=1$ special?}}\]
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inductive Logic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{{Natural Wish List}
	\begin{itemize}
		\item (computability) $P_n(\varphi)$ is computable.
		\item (convergence) $P(\varphi)=\lim\limits_{n\to\infty}P_n(\varphi)$
		\item (coherent limit) $P(\varphi\wedge\psi)+P(\varphi\vee\psi)=P(\varphi)+P(\psi)$
		\item (non-dogmatism) If $\nvdash\varphi$ then $P(\varphi)<1$, and if $\nvdash\neg\varphi$ then $P(\varphi)>0$.
	\end{itemize}
}

\frame{{Unary Pure Inductive Logic}
	\begin{itemize}
		\item $\mathscr{L}$ contains countable constants $\mathscr{C}$ and $m$ unary predicates.
		\item $\mathscr{R}=\{R_1,R_2,\ldots,R_m\}$ with no function symbols nor equality.
		\item $Q_i\,:=\,\bigwedge\limits_{j=1}^m\pm R_j$ for $1\leq i\leq 2^m=:r$.
		\item $\mathscr{Q}=\left\{Q_1,\cdots,Q_r\right\}$ is a $r$-fold classification system of some Universe with domain $\mathscr{C}$.
	\end{itemize}
	\begin{definition}[Probability on Sentences]
		\noindent A probability on sentences is a non-negative function $w\colon\mathscr{S}\to[0,1]$ s.t.
		\begin{enumerate}
			\item[$P_1.$] $\vDash \psi\implies w(\psi)=1$
			\item[$P_2.$] $\psi_1\vDash\neg\psi_2\implies w(\psi_1\vee \psi_2)=w(\psi_1)+w(\psi_2)$
			\item[$P_3.$] $w\big(\exists x\psi(x)\big)=\lim\limits_{n\to\infty} w\left(\bigvee\limits_{i=1}^n\psi(a_i)\right)$
		\end{enumerate}
	\end{definition}
}

\frame{{Properties}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{theorem}
		\begin{enumerate}[i]
			\item $w(\neg\varphi)=1-w(\varphi)$
			\item $\vDash\neg\varphi\implies w(\varphi)=0$
			\item The following are equivalent:
			\begin{enumerate}[a]
				\item $w(\varphi)=1 \implies \vDash\varphi$
				\item $w(\varphi)=0 \implies \vDash\neg\varphi$
			\end{enumerate}
			\item $\varphi\vDash\psi\implies w(\varphi)\leq w(\psi)$
			\item $\vDash\varphi\leftrightarrow\psi\implies w(\varphi)=w(\psi)$
			\item $w(\varphi)+w(\psi)=w(\varphi\wedge\psi)+w(\varphi\vee\psi)$
		\end{enumerate}
	\end{theorem}
	\begin{theorem}[Extension Theorem]\label{extensionthm}
		For any probability function over quantifier-free sentences $w\colon \mathscr{S}\to[0,1]$ satisfying $P_1,P_2$, $w$ has an unique extension to $\overline{w}\colon\mathscr{S}\to[0,1]$ satisfying $P_1,P_2,P_3$.
	\end{theorem}
}

\frame{{Possible Worlds}
\begin{columns}[onlytextwidth]
\column{.59\textwidth}
\begin{itemize}
\item \textcolor{red}{\textbf{state description}} $\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)$

	where $h\colon \mathscr{C}\to \mathscr{Q}$
\item \textcolor{red}{\textbf{structure description}} $\{n_i\}_{i=1}^r$

	where $n_i:=\sum\limits_{j=1}^n\llbracket h_j=i\rrbracket$
\item \textcolor{red}{\textbf{rank description}} $\{m_i\}_{i=0}^n$

	where $m_i:=\sum\limits_{j=1}^r\llbracket n_j=i\rrbracket$
	
	Obviously, $\sum\limits_{i=1}^n i\cdot m_i=n$ and $\sum\limits_{i=0}^n m_i=r$.
\end{itemize}
\column{.4\textwidth}
	\begin{figure}
		\includegraphics[width=\textwidth]{img/box.jpg}
	\end{figure}
\end{columns}
}

\frame{{Indifference Principle}
	\begin{enumerate}[A]
		\item All state descriptions have equal weight.
		\item All structure descriptions have equal weight.
		\item Each nonempty subset of the alphabet is equally
		likely.
		\item Each nonzero cardinality is equally likely.
		\item All rank descriptions have equal weight.
	\end{enumerate}
	Given $n$ individuals, there are $r^n$ possible state descriptions,
	\[\Big|\Big\{(n_1,\ldots,n_r)\colon \sum\limits_{i=1}^r n_i=n\Big\}\Big|=\binom{n+r-1}{r-1}\]
	possible structure descriptions, and 
	\[p(n,r):=\Big|\Big\{(m_0,\ldots,m_n)\colon \sum\limits_{i=1}^n i\cdot m_i=n\;\;\&\;\;\sum\limits_{i=0}^n m_i=r\;\;\&\;\;\forall i\colon m_i\geq 0 \Big\}\Big|\]
	possible rank descriptions.
}

\frame{{(A) State Description~$\times$}
	According to (A),
	\[m^\dagger\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)=\dfrac{1}{r^n}\]
	\[c^\dagger\left(Q_j(a_{n+1})\,\middle|\,\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)=\dfrac{m^\dagger\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\wedge Q_j(a_{n+1})\right)}{m^\dagger\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)}=\dfrac{1}{r}\]
}

\frame{{(B) Structure Description~$\checkmark$}
	According to (B),
	\[m^*(n_1,\ldots,n_r)=\frac{1}{\binom{n+r-1}{r-1}}\]
	Structure Description $(n_1,\ldots,n_r)$ corresponds to $\binom{n}{n_1,\ldots,n_r}$ State Descriptions.
	\[m^*\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)=\dfrac{m^*(n_1,\ldots,n_r)}{\binom{n}{n_1,\ldots,n_r}}=\dfrac{1}{\binom{n+r-1}{r-1} \binom{n}{n_1,\ldots,n_r}}\]
	\begin{center}
		\textcolor{blue}{Sometimes we write $m^*(h_{1:n}):=m^*\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)$ for short.}
	\end{center}
}

\frame{{Carnap's Degree of Confirmation}
	\begin{block}{Carnap's Degree of Confirmation}
		\[c^*\left(Q_j(a_{n+1})\,\middle|\,\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)=\frac{m^*\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\wedge Q_j(a_{n+1})\right)}{m^*\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)}=\frac{\textcolor{red}{n_j}+1}{\textcolor{red}{n}+r}\]
	\end{block}
	frequency --- independent identical distribution(\textcolor{red}{i.i.d})\\
	extension
	\[c^*(\varphi)\]
}

\frame{{(C,D,E)}
	According to (C),
	\[m^\$(h_{1:n})=\dfrac{1}{\left(\sum\limits_{i=1}^{\min\{r,n\}}\binom{r}{i}\right)\binom{n-1}{r-m_0-1}\binom{n}{n_1,\ldots,n_r}}\index{$m^\$$}\]
	According to (D),
	\[m^\#(h_{1:n})=\dfrac{1}{\min\{r,n\}\binom{r}{r-m_0}\binom{n-1}{r-m_0-1}\binom{n}{n_1,\ldots,n_r}}\index{$m^\#$}\]
	According to (E),
	\[m^\tau(h_{1:n})=\dfrac{1}{\binom{n}{n_1,\ldots,n_r}\binom{r}{m_0,\ldots,m_n}p(n,r)}\index{$m^\tau$}\]
	\resizebox{\textwidth}{!}{Rank Description $(m_0,\ldots,m_n)$ corresponds to $\binom{r}{m_0,\ldots,m_n}$ Structure Descriptions.}
}

\frame{{What is the right $w$?}
	\begin{itemize}
		\item Constant Exchangeability Principle.\\
		For any permutation $\sigma$ of $\mathbb{N}^+$,
		\begin{equation}
		w(\psi(a_1,\ldots,a_n))=w(\psi(a_{\sigma(1)},\ldots,a_{\sigma(n)})) \tag{Ex}\label{constantpermu}
		\end{equation}
		\item Atom Exchangeability Principle.\\
		For any permutation $\tau$ of $\{1,2,\ldots,r\}$,
		\begin{equation}
		w\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)=w\left(\bigwedge\limits_{i=1}^n Q_{\tau(h_i)}(a_i)\right) \tag{Ax}\label{atompermu}
		\end{equation}
		\item Sufficientness Postulate.
		\[w\left(Q_j(a_{n+1})\,\middle|\,\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)=f_j(n_j,n) \tag{SP}\label{sufficientp}\]
	\end{itemize}
}

\frame{{What is the right $w$?}
	Principle  \ref{constantpermu} asserts that $w\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)$ depends only on the vector $\left\langle n_{h_i}\colon 1\leq i\leq n\right\rangle$, so that it is independent on the order of observing the individuals, while in the presence of \ref{constantpermu}, principle  \ref{atompermu} asserts that $w\left(\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)\right)$ depends only on $\{n_i\colon 1\leq i\leq r\}$, and $w(Q_i(a_1))=1/r$ for all $1\leq i\leq r$.
}

\frame{{Carnap's $\lambda$-continuum}
	\begin{theorem}
		Suppose language $\mathscr{L}$ has at least two predicates i.e. $m\geq 2$, then the probability function $w$ on $\mathscr{L}$ satisfies \ref{constantpermu}, \ref{sufficientp} iff $w=c_\lambda$ for some $0\leq\lambda\leq\infty$.\\
		Namely,
		\[f_i(n_i,n)=\dfrac{n_i+\lambda\gamma_i}{n+\lambda}\]
		where $\gamma_i=f_i(0,0)$ and $\lambda=\dfrac{f_i(0,1)}{f_i(0,0)-f_i(0,1)}$.\\
		By adding \ref{atompermu}, $\forall i\colon \gamma_i=\dfrac{1}{r}$.
	\end{theorem}
}

\frame{{Shortcoming~$1$ --- All Ravens are Black?~$\times$}
	\[c^*\left(\forall x\left(R(x)\to B(x)\right)\right)\leq\lim\limits_{n\to\infty}\prod\limits_{i=0}^{n-1}\dfrac{i+r-1}{i+r}=0\]
	
	\begin{center}
		\textcolor{red}{Convergence Speed?} \textcolor{blue}{\large Yes and No!}
	\end{center}
	
	\begin{tcolorbox}
		\[\prod\limits_{n\geq 1} a_n=0\iff\sum\limits_{n\geq 1}(1-a_n)=\infty\quad\mbox{for}\quad\forall n\colon 0<a_n\leq 1\]
	\end{tcolorbox}
}

\frame{{Shortcoming~$2$ --- No-Free-Lunch for Carnap!}
	\noindent\footnotesize{\textcolor{red}{\Large Strengthened ``Hume''} --- Wolpert \& Macready~1997,~Igel \& Toussaint~2004:}
	\begin{center}
		\colorbox{lightgreen}{\textcolor{red}{\Large No-Free-Lunch Theorem!}}\\
		All state descriptions with the same structure description have equal weight!\\
		\vspace{7pt}
		\textcolor{red}{\Large Block Uniform}
	\end{center}
	\begin{columns}[onlytextwidth]
		\column{0.5\textwidth}\centering
			\begin{tikzpicture}
			%put some nodes on the left
			\foreach \x[count=\x] in {0.5,1.5,...,4}{
				\node[fill,circle,inner sep=2pt] (d\x) at (0,\x) {};
			}
			\node[fit=(d1) (d2) (d3) (d4),ellipse,fill=blue!20, draw=blue!60,thick,minimum width=1.5cm] {};
			%put some nodes on the center
			\foreach \x[count=\xi] in {2,3,...,2}{
				\node[fill,circle,inner sep=2pt] (r\xi) at (3,\x){};
			}
			\node[fit=(r1) (r2),ellipse,fill=red!20, draw=red!60,thick,minimum width=1.5cm] {};
			
			\fill[blue] (d1) circle (2pt);
			\fill[blue] (d2) circle (2pt);
			\fill[blue] (d3) circle (2pt);
			\fill[blue] (d4) circle (2pt);
			\fill[red] (r1) circle (2pt);
			\fill[red] (r2) circle (2pt);
			
			\node at (2,4) {$f$};
			
			\draw[-latex] (d1) -- (r1);
			\draw[-latex] (d2) -- (r1);
			\draw[-latex] (d3) -- (r2);
			\draw[-latex] (d4) -- (r2);
			\end{tikzpicture}
		\column{0.5\textwidth}\centering
			\begin{tikzpicture}
			%put some nodes on the left
			\foreach \x[count=\x] in {0.5,1.5,...,4}{
				\node[fill,circle,inner sep=2pt] (d\x) at (0,\x) {};
			}
			\node[fit=(d1) (d2) (d3) (d4),ellipse,fill=blue!20, draw=blue!60,thick,minimum width=1.5cm] {};
			%put some nodes on the center
			\foreach \x[count=\xi] in {2,3,...,2}{
				\node[fill,circle,inner sep=2pt] (r\xi) at (3,\x){};
			}
			\node[fit=(r1) (r2),ellipse,fill=red!20, draw=red!60,thick,minimum width=1.5cm] {};
			
			\fill[blue] (d1) circle (2pt);
			\fill[blue] (d2) circle (2pt);
			\fill[blue] (d3) circle (2pt);
			\fill[blue] (d4) circle (2pt);
			\fill[red] (r1) circle (2pt);
			\fill[red] (r2) circle (2pt);
			
			\node at (2,4) {$g$};
			
			\draw[-latex] (d1) -- (r1);
			\draw[-latex] (d2) -- (r2);
			\draw[-latex] (d3) -- (r2);
			\draw[-latex] (d4) -- (r1);
			\end{tikzpicture}
	\end{columns}
}

\frame{{No Free Lunch Theorem --- Strengthened ``Hume''}
	\begin{theorem}[No Free Lunch Theorem]
		If and only if the probability distribution $P$ is block uniform, i.e.
		\[\forall f,g\in\mathcal{Y}^{\mathcal{X}}\colon\forall y\in\mathcal{Y}\left(\left|f^{-1}(y)\right|=\left|g^{-1}(y)\right|\right)\implies P(f)=P(g)\]
		then for any two algorithms $A, A'$, any value $k\in\mathbb{R}$, any $m\in\left\{1,\dots,|\mathcal{X}|\right\}$, and any performance measure $L$,
		\[\sum\limits_{f\in\mathcal{Y}^{\mathcal{X}}}P(f)\left\llbracket k=L\left(T_m^y(A,f)\right)\right\rrbracket=\sum\limits_{f\in\mathcal{Y}^{\mathcal{X}}}P(f)\left\llbracket k=L\left(T_m^y(A',f)\right)\right\rrbracket\]
		where $T_n:=\left\langle(x_1,f(x_1)),\dots,(x_n,f(x_n))\right\rangle$, $T_n^x:=\left\langle x_1,\dots,x_n\right\rangle$, $T_n^y:=\left\langle f(x_1),\dots,f(x_n)\right\rangle$ and $A\colon T_n\mapsto x_{n+1}\in\mathcal{X}\setminus T_n^x$.
	\end{theorem}
	\centering{\huge\textcolor{red}{equally well and equally poorly}}\\
	\centering{\Large\textcolor{red}{No learning is possible without some prior knowledge!}}
}

\frame{{No-Free-Lunch for Pure Inductive Logic}
	\begin{itemize}
		\item No learning is possible for $c^\dagger$. It seems possible to learn with $c^*$ and $c_\lambda$. Unfortunately, No-Free-Lunch Theorem!
		\item Take $\mathcal{X}:=\mathscr{C}, \mathcal{Y}:=\mathscr{Q}$ in the No-Free-Lunch Theorem. The state description $h\colon\mathscr{C}\to\mathscr{Q}$ can be taken as a \emph{classification} function.
		\item Then ``all state descriptions $h\colon\mathscr{C}\to\mathscr{Q}$ with the same structure description $\{n_i\colon 1\leq i\leq r\}$ have equal weight'' is \textcolor{red}{block uniform}!
		\item Define induction algorithm $A(h_{1:n}):=\argmax\limits_j c^*\left(Q_j(a_{n+1})|h_{1:n}\right)$, and loss function $L\left(A,n,h\right):=\left\llbracket A(h_{1:n})\neq h_{n+1}\right\rrbracket$. Then for any $A'$,
		\[\sum\limits_{h\in\mathcal{Y}^{\mathcal{X}}}m^*(h_{1:n}) L\left(A,n,h\right)=\sum\limits_{h\in\mathcal{Y}^{\mathcal{X}}}m^*(h_{1:n})L\left(A',n,h\right)\]
		\item Similarly for rank description $(E)$, which is related to Good-Turing estimate. And similarly for Ristad's methods $(C)(D)$.
	\end{itemize}
}

\frame{{Time Series and Solomonoff Induction}
	\begin{itemize}
		\item However, if we take time into consideration, define
		\[M(h_{1:n}):=\sum\limits_{p:U(p)=h_{1:n}*}2^{-\ell(p)}\]
		then we can get free lunch with $M'$, since $M'$ biases non-random state descriptions and is not block uniform.
		where 
		\begin{align*}
		M'(\epsilon)&:=1\\
		M'(h_{1:n})&:=M'(h_{<n})\frac{M(h_{1:n})}{\sum\limits_{Q\in\mathscr{Q}}M(h_{<n}Q)}
		\end{align*}
		and we can extend $M'$ from state descriptions to sentences.
		\item Besides, by $c^*$ the probability of ``the sun will rise tomorrow'' is $\frac{n+1}{n+2}$, but $c^*$ fails to confirm ``the sun will always rise'', while $M'$ can confirm it.
		\[\lim\limits_{n\to\infty}M'(\text{the sun will always rise}|\text{the sun rises in the first $n$ days})=1\]
	\end{itemize}
}

\frame{{PAC (Probably Approximately Correct) Learning}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{definition}[PAC-Learnability]
		A hypothesis space $\mathcal{H}\subset 2^{\mathcal{X}}$ is PAC-learnable if there exists a sample complexity function $m_{\mathcal{H}}\colon (0,1)^2\to\mathbb{N}$ and a learning algorithm $A$ with the following property:
		\begin{itemize}
			\item for every $\varepsilon,\delta\in(0,1)$
			\item for every distribution $\mathcal{D}$ over $\mathcal{X}$, and for every labeling function $f\colon\mathcal{X}\to\{0,1\}$
		\end{itemize}
		when running $A$ on $m\geq m_{\mathcal{H}}(\varepsilon,\delta)$ i.i.d. training samples $S$ generated by $\mathcal{D}$ and labeled by $f$, the algorithm $A$ returns a hypothesis $A(S)\in\mathcal{H}$ s.t.
		\[P_{S\sim\mathcal{D}^m}\Big(P_{x\sim\mathcal{D}}\big(A(S)(x)\neq f(x)\big)>\varepsilon\Big)<\delta\]
	\end{definition}
	\centering No-Free-Lunch $\implies m_{2^{\mathcal{X}}}(\varepsilon,\delta)=\infty$
}

\frame{{Agnostic PAC Learning}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{definition}[Agnostic PAC-Learnability]
		A hypothesis space $\mathcal{H}\subset\mathcal{Y}^{\mathcal{X}}$ is agnostic PAC-learnable under a class $\Delta$ of distributions with respect to $\mathcal{Z}:=\mathcal{X}\times\mathcal{Y}$ and a loss function $\ell\colon\mathcal{H}\times\mathcal{Z}\to\mathbb{R}^+$, if there exists a sample complexity function $m_{\mathcal{H}}\colon (0,1)^2\to\mathbb{N}$ and a learning algorithm $A$ with the following property:
		\begin{itemize}
			\item for every $\varepsilon,\delta\in(0,1)$
			\item for every distribution $\mathcal{D}\in\Delta$ over $\mathcal{Z}$
		\end{itemize}
		when running $A$ on $m\geq m_{\mathcal{H}}(\varepsilon,\delta)$ i.i.d. training samples $S$ generated by $\mathcal{D}$, the algorithm $A$ returns a hypothesis $A(S)\in\mathcal{H}$ s.t.
		\[P_{S\sim\mathcal{D}^m}\Big(L_{\mathcal{D}}(A(S))-\min\limits_{h\in\mathcal{H}}L_{\mathcal{D}}(h)>\varepsilon\Big)<\delta\]
		where $L_{\mathcal{D}}(h):=\mathbb{E}_{z\sim\mathcal{D}}\left[\ell(h,z)\right]$.
	\end{definition}
	\resizebox{\textwidth}{!}{PAC-learnable under $\Delta:=\left\{\mathcal{D}\colon\mathcal{D}\leqm\xi\right\}\iff$ PAC-learnable under $\xi(x):=\sum\limits_{\nu\in\mathcal{M}}2^{-K(\nu)}\nu(x)$.}
}

\frame{{VC-Dimension}
\begin{definition}[Shattering]
A hypothesis space $\mathcal{H}\subset 2^{\mathcal{X}}$ shatters a set $C\subset\mathcal{X}$ if $\mathcal{H}{\restriction_C}=2^C$.
\end{definition}
\begin{definition}[VC-Dimension]
$\mathrm{VC}(\mathcal{H}):=\sup\big\{|C|\colon \mathcal{H} \mbox{ shatters } C\big\}$.
\end{definition}
{\footnotesize\emph{If someone can explain every phenomena, her explanations are worthless.}}
\begin{theorem}
If $\mathrm{VC}(\mathcal{H})=\infty$, then $\mathcal{H}$ is not PAC-learnable.
\end{theorem}
\begin{theorem}[Fundamental Theorem of PAC Learning]
$\mathcal{H}$ is PAC-learnable iff $\mathrm{VC}(\mathcal{H})<\infty$. Indeed, there exists $C_1,C_2$ s.t.
\[C_1\frac{\mathrm{VC}(\mathcal{H})+\log(1/\delta)}{\varepsilon}\leq m_{\mathcal{H}}(\varepsilon,\delta)\leq C_2\frac{\mathrm{VC}(\mathcal{H})\log(1/\varepsilon)+\log(1/\delta)}{\varepsilon}\]
\end{theorem}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Universal Induction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{{Formal Learning Theory}
\begin{itemize}
	\item Carnap's inductive logic is a design for a \textcolor{red}{`learning machine'} that can extrapolate certain kinds of empirical regularities from the data with which it is supplied, and the task of inductive logic is to construct a \textcolor{red}{`universal learning machine'}.
	\item If there is such a thing as a correct definition of \textcolor{red}{`degree of confirmation'} which can be fixed once and for all, then a machine that predicts in accordance with it would be a \textcolor{red}{cleverest possible learning machine}.
	\item Either there are better and better `degree of confirmation' functions, but no `best possible', or there is a `best possible' but it is not computable by a machine.
\end{itemize}
\textcolor{blue}{Formal Learning Theory}
\begin{itemize}
	\item Putnam 1963
	\item Gold 1967\quad $\hat{f}(n+1)=g(\langle f(0),\ldots,f(n)\rangle)\quad\varphi_{\lim\limits_{n\to\infty}g(\langle f(0),\dots,f(n))}=f$
	\item Solomonoff 1960
\end{itemize}
}

\frame{{Ray Solomonoff}
	\begin{columns}
		\column{0.7\textwidth}
			\begin{figure}[htbp]
				\begin{center}
					\begin{tikzpicture}[node distance=17mm, auto]
					\node[circle, draw, text centered, minimum height=2em, fill=green!20] (a1) {$s_1$};
					\node[circle, draw, text centered, minimum height=2em, right of=a1, fill=green!20] (e1) {$s_2$};
					\node[circle, draw, text centered, minimum height=2em, right of=e1, fill=green!20] (a2) {$s_3$};
					\node[circle, draw, text centered, minimum height=2em, right of=a2, fill=green!20] (e2) {$s_4$};
					\node[right of=e2,minimum height=2em] (dots) {$\cdots$};
					\draw[->] (a1) to (e1);
					\draw[->] (e1) to (a2);
					\draw[->] (a2) to (e2);
					\draw[->] (e2) to (dots);
					\end{tikzpicture}
				\end{center}
			\end{figure}
			\begin{figure}[htbp]
				\begin{center}
					\begin{tikzpicture}[node distance=17mm, auto]
					\node[circle, draw, text centered, minimum height=2em, fill=red!20] (a1) {$a_1$};
					\node[circle, draw, text centered, minimum height=2em, right of=a1, fill=blue!20] (e1) {$e_1$};
					\node[circle, draw, text centered, minimum height=2em, right of=e1, fill=red!20] (a2) {$a_2$};
					\node[circle, draw, text centered, minimum height=2em, right of=a2, fill=blue!20] (e2) {$e_2$};
					\node[right of=e2,minimum height=2em] (dots) {\ldots};
					\node[circle, draw, text centered, minimum height=2em, above of=a1, fill=gray!20] (hidden) {$h$};
					\draw[->] (hidden) to (a1);
					\draw[->] (hidden) to (e1);
					\draw[->] (hidden) to (a2);
					\draw[->] (hidden) to (e2);
					\draw[->] (hidden) to (dots);
					\draw[->] (a1) to (e1);
					\draw[->,bend right] (a1) to (a2);
					\draw[->,bend right] (a1) to (e2);
					\draw[->,bend right] (a1) to (dots);
					\draw[->] (e1) to (a2);
					\draw[->,bend left] (e1) to (e2);
					\draw[->,bend left] (e1) to (dots);
					\draw[->] (a2) to (e2);
					\draw[->,bend right] (a2) to (dots);
					\draw[->] (e2) to (dots);
					\end{tikzpicture}
				\end{center}
			\end{figure}
		\column{0.23\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth]{img/solomonoff}\caption{Solomonoff}
			\end{figure}
	\end{columns}
}

\frame{{}
	\begin{columns}
		\column{0.8\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth]{img/dots}
			\end{figure}
			\begin{itemize}
				\item $01010101010101010101010101010101$
				\item $01010011011110110111010101000001$
				\item $00100100001111110110101010001000$\hfill$\{\pi\}$
			\end{itemize}
		\column{0.2\textwidth}
			\begin{figure}
				\includegraphics[width=.9\textwidth]{img/robot}
			\end{figure}
	\end{columns}
	\begin{enumerate}
		\item \textcolor{red}{What is regularity/pattern/law/principle/model/hypothesis/theory?}
		\item \textcolor{red}{What is phenomenon/data/experience?}
		\item \textcolor{red}{What is randomness/noise?}
		\item \textcolor{red}{What is typicalness/unpredictability/incompressibility?}
		\item \textcolor{red}{What is simplicity/complexity?}
		\item \textcolor{red}{What is learning?}
		\item \textcolor{red}{What is beauty/interesting/curiosity/novelty/surprise/creativity?}
		\item \textcolor{red}{What is intelligence?}
	\end{enumerate}
}

\frame{{Text $\implies$ Meaning?}
\begin{tcolorbox}[title=$\mbox{numeral}\xrightarrow{\text{short algorithm}}\mbox{value}$]
汝有田舍翁，家资殷盛，而累世不识“之”“乎”。一岁，聘楚士训其子。楚士始训之搦管临朱，书一画，训曰：“一”字。书二画，训曰：“二”字。书三画，训曰：“三”字。其子辄欣欣然掷笔，归告其父曰：“儿得矣！儿得矣！可无烦先生，重费馆谷也，请谢去。”其父喜从之，具币谢遣楚士。

逾时，其父拟征召姻友万氏者饮，令子晨起治状，久之不成。父趣之。其子恚曰：“天下姓字多矣，奈何姓万？自晨起至今，才完五百画也。”
\end{tcolorbox}
}

\frame{{Kolmogorov Complexity}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{columns}
		\column{.62\textwidth}
			\begin{definition}[Kolmogorov Complexity]
				\[K(x):=\min\limits_p\{\ell(p)\colon U(p)=x\}\]
				where $U$ is a universal prefix Turing machine.
			\end{definition}
\begin{figure}[H]
\begin{center}
\includegraphics[angle=0, width=.9\textwidth]{img/tm-monotone.pdf}
\end{center}
\end{figure}
			``Independence'' of universal Turing machine!
		\column{.27\textwidth}
			\centering How to quantify ``\textcolor{red}{simplicity}''? ``\textcolor{red}{randomness}''?
			\begin{figure}
				\includegraphics[width=\textwidth,angle=0,origin=c]{img/kolmogorov0}\caption{Kolmogorov}
			\end{figure}
	\end{columns}
}

\frame{{Properties}
	\begin{enumerate}
		\item $K(n)\leqa \log^* n\leq \log n+2\log\log n$
		\item $K(x)\leqa K(x|\ell(x))+K(\ell(x))\leqa\ell(x)+\log^* \ell(x) \leq \ell(x)+2\log \ell(x)$
		\item $\sum_x 2^{-K(x)}\leq 1$
		\item $K(x|y)\leqa K(x)\leqa K(x,y)$
		\item $K(xy)\leqa K(x,y)\leqa K(x)+K(x|y)\leqa K(x)+K(y)$
		\item $K(x)\eqa K(x,K(x))$
		\item $K(x|y,K(y))+K(y)\eqa K(x,y)\eqa K(y,x)\eqa K(y|x,K(x))+K(x)$
		\item $K(f(x))\leqa K(x)+K(f)$ for computable $f$
		\item $K(x)\leqa-\log \mu(x)+K(\mu)$ if $\mu$ is lower semicomputable and $\sum_x \mu(x)\leq 1$
		\item $\sum\limits_{x:f(x)=y} 2^{-K(x)}\eqm 2^{-K(y)}$ if $f$ is computable and $K(f)=\mathcal{O}(1)$
		\item $0\leq \mathbb{E}_\mu [K] - H(\mu) \leqa K(\mu)$ for computable probability distribution $\mu$
	\end{enumerate}
}

\frame{{Universal Similarity Metric}
	\begin{align*}
	d(x,y)&:=\dfrac{\max\{K(x|y),K(y|x)\}}{\max\{K(x), K(y)\}}\\
	&\approx\frac{K_T(xy)-\min\{K_T(x),K_T(y)\}}{\max\{K_T(x),K_T(y)\}}
	\end{align*}
	\begin{itemize}
		\item $T\colon$ Lempel-Ziv/gzip/bzip2/PPMZ, or\\
		$K_T(x):=-\log P_{google}(x)$
		\item compute similarity matrix $\left(d(x_i,x_j)\right)_{ij}$
		\item cluster similar objects.
	\end{itemize}
}

\frame{{Algorithmic Probability}
	\begin{definition}[Algorithmic Probability]
		\[M(x):=\sum\limits_{p:U(p)=x*}2^{-\ell(p)}\]
		where $U$ is a universal monotone Turing machine.
	\end{definition}
	\begin{columns}[onlytextwidth]
		\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth,angle=0,origin=c]{img/monkeytypewriter.jpg}
			\end{figure}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/monkeycomputer.jpg}\\
			\centering\colorbox{lightgreen}{\textcolor{blue}{\scriptsize{$\sum\limits_{p:U(p)=x*}2^{-\ell(p)}\gg 2^{-\ell(x)}$}}}
	\end{columns}
}

\frame{{Toss Coin onto Universal Turing Machine}
	\begin{figure}[htb]
		\subfigure{\includegraphics[height=.55\textwidth,angle=0,origin=c]{img/cointoss}}
		\subfigure{\includegraphics[height=.55\textwidth,angle=0,origin=c]{img/threetapetm}}
	\end{figure}
}

\frame{{Bolztmann Brain vs Brain in a Vat}
	\begin{columns}[onlytextwidth]
		\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth,angle=0,origin=c]{img/boltzmannbrain}
			\end{figure}
			\[K(\nu)\;\;\text{is too large}.\]
		\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth,angle=0,origin=c]{img/brain-vat}
			\end{figure}
	\end{columns}
}

\frame{{Aspect~$1$ --- Popper}
	\begin{columns}
		\column{.55\textwidth}
			\resizebox{.7\textwidth}{!}{
				\begin{minipage}{\textwidth}\centering
					\begin{tikzpicture}[scale=0.5]
					\draw(0,-6)ellipse(3 and 2)node(h){\Large hypothesis};
					\draw(-6,0)ellipse(3 and 2)node(p){\Large problem};
					\draw(6,0)ellipse(3 and 2)node(e){\Large experiment};
					\draw(0,6)circle(3 and 2)node(f){\Large falsification};
					
					\node at (0,0){\includegraphics[width=0.25\textwidth,angle=0,origin=c]{img/popper}};
					
					\draw [->,ultra thick,red] (p) to [bend right] (h);
					\draw [->,ultra thick,red] (h) to [bend right] (e);
					\draw [->,ultra thick,red] (e) to [bend right] (f);
					\draw [->,ultra thick,red] (f) to [bend right] (p);
					\end{tikzpicture}
			\end{minipage}}
		\column{.35\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth,angle=0,origin=c]{img/overfitting}
			\end{figure}
	\end{columns}
	\begin{center}
		$\mathcal{H}$: \textcolor{red}{truth $\gets$ simplcity/generality/aesthetic/utilitarian/\dots}
	\end{center}
	Make a weighted prediction based on all consistent programs, with short programs weighted higher.
}

\frame{{Aspect~$2$ --- Deterministic vs Stochastic}
	$\mathcal{M}:=\{\nu_1,\nu_2,\dots\}$~lower semicomputable semi-measure.
	\[
	\xi(x):=\sum\limits_{\nu\in\mathcal{M}}\textcolor{red}{2^{-K(\nu)}}\nu(x)\]
	\begin{center}
		\fbox{$w_\nu \,:=\, 2^{-K(\nu)}$ is reparametrization \& regrouping invariant.}
	\end{center}
	\[\textcolor{red}{M(x)\eqm \xi(x)}\]
}

\frame{{Aspect~$3$ --- Frequency Interpretation}
	\begin{align*}
	M(x)&=\sum_{p}2^{-\ell(p)}\llbracket U(p)=x*\rrbracket\\
	&=\lim\limits_{n\to\infty}\dfrac{\sum\limits_{p:\ell(p)\leq n}2^{n-\ell(p)}\left\llbracket  U(p)=x*\right\rrbracket}{2^n}\\
	&\approx\lim\limits_{n\to\infty}\dfrac{\big|\big\{p\colon\ell(p)=n\;\&\; U(p)=x*\big\}\big|}{2^n}
	\end{align*}
	
	\begin{tcolorbox}[colback=cyan!3,colframe=back!85]
		\[\textbf{algorithmic probability} = \dfrac{\big|\text{consistent worlds}\big|}{\big|\text{all possible worlds}\big|}\]
		\[\left\lbrace\begin{aligned}
		&\text{Carnap --- frequency of phenomena --- \textcolor{red}{i.i.d}}\\
		&\text{Solomonoff --- frequency of \textcolor{red}{causes} --- arbitrary order Markov chain}
		\end{aligned}\right.\]
	\end{tcolorbox}
}

\frame{{Aspect~$4$ --- Free-Lunch for Solomonoff!}
	\[\xi\to\mu\]
	
	\centering\footnotesize{\textcolor{red}{break ``block uniform'' --- bias non-random functions}}
	
	\begin{columns}
		\column{0.3\textwidth}
			\begin{tikzpicture}
			%put some nodes on the left
			\foreach \x[count=\x] in {0.5,1.5,...,4}{
				\node[fill,circle,inner sep=2pt] (d\x) at (0,\x) {};
			}
			\node[fit=(d1) (d2) (d3) (d4),ellipse,fill=blue!20, draw=blue!60,thick,minimum width=1.5cm] {};
			%put some nodes on the center
			\foreach \x[count=\xi] in {2,3,...,2}{
				\node[fill,circle,inner sep=2pt] (r\xi) at (2,\x){};
			}
			\node[fit=(r1) (r2),ellipse,fill=red!20, draw=red!60,thick,minimum width=1.5cm] {};
			
			\fill[blue] (d1) circle (2pt);
			\fill[blue] (d2) circle (2pt);
			\fill[blue] (d3) circle (2pt);
			\fill[blue] (d4) circle (2pt);
			\fill[red] (r1) circle (2pt);
			\fill[red] (r2) circle (2pt);
			
			\node at (1.5,4) {$f$};
			
			\draw[-latex] (d1) -- (r1);
			\draw[-latex] (d2) -- (r1);
			\draw[-latex] (d3) -- (r2);
			\draw[-latex] (d4) -- (r2);
			\end{tikzpicture}
		
		\column{0.3\textwidth}
			\begin{tikzpicture}
			%put some nodes on the left
			\foreach \x[count=\x] in {0.5,1.5,...,4}{
				\node[fill,circle,inner sep=2pt] (d\x) at (0,\x) {};
			}
			\node[fit=(d1) (d2) (d3) (d4),ellipse,fill=blue!20, draw=blue!60,thick,minimum width=1.5cm] {};
			%put some nodes on the center
			\foreach \x[count=\xi] in {2,3,...,2}{
				\node[fill,circle,inner sep=2pt] (r\xi) at (2,\x){};
			}
			\node[fit=(r1) (r2),ellipse,fill=red!20, draw=red!60,thick,minimum width=1.5cm] {};
			
			\fill[blue] (d1) circle (2pt);
			\fill[blue] (d2) circle (2pt);
			\fill[blue] (d3) circle (2pt);
			\fill[blue] (d4) circle (2pt);
			\fill[red] (r1) circle (2pt);
			\fill[red] (r2) circle (2pt);
			
			\node at (1.5,4) {$g$};
			
			\draw[-latex] (d1) -- (r1);
			\draw[-latex] (d2) -- (r2);
			\draw[-latex] (d3) -- (r2);
			\draw[-latex] (d4) -- (r1);
			\end{tikzpicture}
		\column{0.4\textwidth}
			\centering\includegraphics[width=.8\textwidth]{img/dart}
	\end{columns}
}

\frame{{Algorithmic Coding Theorem}
		%\setlength\abovedisplayskip{0pt}
		%\setlength\belowdisplayskip{0pt}
	\begin{tcolorbox}
	\[m(x):=\sum\limits_{p:U(p)\downarrow=x}2^{-\ell(p)}\]
	\end{tcolorbox}
	\begin{theorem}[Algorithmic Coding Theorem]
		\[K(x)\,\eqa\,-\log m(x)\]
	\end{theorem}
	The probability of a string being produced by a random algorithm is inversely proportional to its algorithmic complexity.\\
	If a string has many long descriptions then it also has a short description.
	\[KM(x):=-\log M(x)\]
	\[KM(x)\leq Km(x)\leqa KM(x)+K(KM(x))\]
}

\frame{{Completeness Theorem}
	\begin{align*}
	M'(\epsilon)&:=1\\
	M'(x_{1:t})&:=M'(x_{<t})\frac{M(x_{1:t})}{\sum\limits_{x\in\mathcal{X}}M(x_{<t}x)}=\frac{M(x_{1:t})}{M(\epsilon)}\prod\limits_{i=1}^t\frac{M(x_{<i})}{\sum\limits_{x\in\mathcal{X}}M(x_{<i}x)}
	\end{align*}
	\begin{theorem}[Completeness Theorem]
		For any computable measure $\mu$,
		\[\sum\limits_{t=1}^\infty\sum\limits_{x_{1:t}\in\mathcal{X}^t}\mu(x_{<t})\Big(M'(x_t|x_{<t})-\mu(x_t|x_{<t})\Big)^2\leq D(\mu\|M) \leqa K(\mu)\ln2\]
	\end{theorem}
}

\frame{{Emergence of Simple Laws of Physics}
	\begin{tcolorbox}[title=Completeness Theorem]
	For any computable measure $\mu$, there is a set $A\subset\mathcal{X}^*$ with $\mu(A)=1$ s.t. for all $\bm{x}\in A$
	\[\sum\limits_{y\in\mathcal{X}}\Big(\sqrt{M'(y|\bm{x})}-\sqrt{\mu(y|\bm{x})}\Big)^2\xrightarrow{n\to\infty}0\]
	\end{tcolorbox}
\begin{columns}
\column{.77\textwidth}
	\begin{tcolorbox}[title=Emergence of Simple Laws of Physics]
	For any computable measure $\mu$,
	\[M'\left\{\sum\limits_{y\in\mathcal{X}}\Big(\sqrt{M'(y|\bm{x})}-\sqrt{\mu(y|\bm{x})}\Big)^2\xrightarrow{n\to\infty}0\right\}\geq 2^{-K(\mu)}\]
	\end{tcolorbox}
\column{.2\textwidth}
\includegraphics[width=\textwidth]{img/it-bit.pdf}
\end{columns}
}

\frame{{Universal Prediction of Selected Bits}
	\begin{theorem}[Universal Prediction of Selected Bits]
		Let $f\colon\{0,1\}^*\to\{0,1,\epsilon\}$ be a total recursive function and $x\in 2^\omega$ satisfying $f(x_{<n})=x_n$ whenever $f(x_{<n})\neq\epsilon$. If $f(x_{<n_i})\neq\epsilon$ for an infinite sequence $n_1,n_2,\dots$ then
		\[\lim\limits_{i\to\infty}M'(x_{n_i}|x_{<n_i})=1\]
	\end{theorem}
}

\frame{{Pure Universal Inductive Logic?}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{tcolorbox}
\begin{itemize}
\item $M(\Theta(a_{1:n})):=\sum\limits_{p:U(p)=h_{1:n}*}2^{-\ell(p)}$\\
where $\Theta(a_{1:n}):=\bigwedge\limits_{i=1}^n Q_{h_i}(a_i)$.
\item $M'\left(\varphi(\vec{a})\right):=\sum\limits_{\Theta(\vec{b})\vDash\psi(\vec{a})} M'(\Theta(\vec{b}))$\\
where
$\vDash\varphi(\vec{a})\leftrightarrow\bigvee\limits_{\Theta(\vec{b}) \vDash \varphi(\vec{a})}\Theta(\vec{b}). \hfill\text{FDNF}$
\end{itemize}
\end{tcolorbox}
\centerline{\fbox{$\sum\limits_{t=1}^\infty\sum\limits_{\varphi(a_{1:t})}\mu\left(\varphi(a_{<t})\right)\Big(M'\left(\varphi(a_t)\,\middle|\,\varphi(a_{<t})\right)-\mu\left(\varphi(a_t)\,\middle|\,\varphi(a_{<t})\right)\Big)^2\leqa K(\mu)\ln 2$}}
where $\varphi(a_{1:t}):=\bigwedge\limits_{i=1}^t\varphi(a_i/x)$.
}

\frame{{All Ravens are Black!~$\checkmark$}
	\begin{theorem}[All Ravens are Black]
		\[\lim\limits_{n\to\infty} M'\left(\forall x(R(x)\to B(x))\,\middle|\,\bigwedge\limits_{i=1}^n (\neg R(a_i)\vee B(a_i))\right)=1\]
	\end{theorem}
	\begin{theorem}[Confirmation by Random Sampling]
		If the sampling function $t\colon\mathbb{N}\to\mathbb{N}$ satisfies $\forall i\colon t_i\leq t_{i+1}$ and $\chi_{1:\infty}$ is algorithmic random, where $\chi_i:=\llbracket\exists k(t_k=i)\rrbracket$, then
		\[M'\left(\forall x\varphi(x)\,\middle|\,\bigwedge\limits_{i=1}^n \varphi(a_{t_i})\right)\xrightarrow{n\to\infty}1\]
	\end{theorem}
	\[M(1|1^n)\xrightarrow{n\to\infty}1\qquad M(0|1^n)\eqm 2^{-K(n)}\qquad\sum\limits_{n=0}^\infty M(0|1^n)<\infty\]
}

\frame{{Why Solomonoff Prior?}
\[H(w)\leq\mathbb{E}_w[K]\leq H(w)+K(w)\]
\begin{tcolorbox}
\centerline{Maximum Entropy + Occam's Razor}
\end{tcolorbox}
\begin{gather*}
\mathop{\mathit{minimize}}\limits_{w\vDash\sum\limits_{\nu\in\mathcal{M}}\!w_\nu=1} \dfrac{\mathbb{E}_w[K]}{H(w)}\\
\rotatebox[origin=C]{270}{$\implies$}\\
w_\nu^*=\dfrac{2^{-K(\nu)}}{\sum\limits_{\nu\in\mathcal{M}}2^{-K(\nu)}}
\end{gather*}
}

\frame{{Advantages \& Disadvantages}
	\begin{itemize}
		\item \textcolor{blue}{free-lunch}
		\item \textcolor{blue}{universality --- finite error}
		\item \textcolor{blue}{data sparse problem --- arbitrary order Markov chain --- universal smoothing method}
		\item \textcolor{blue}{confirmation of $\forall x\colon R(x)\to B(x)$}
		\item \textcolor{red}{incomputability}
		\item \textcolor{red}{weakly depends on universal Turing machine}
	\end{itemize}
}

\frame{{\href{http://www2.odn.ne.jp/tadaki}{\textcolor{fore}{A statistical mechanical interpretation of AIT}}}\centering
\begin{tabu}{lll}
	an energy eigenstate $n$ &$\implies$ &a program $p$ s.t. $U(p)\downarrow$\\
	the energy $E_n$ of $n$ &$\implies$ &the length $\ell(p)$ of $p$\\
	Boltzmann constant $k$ &$\implies$ &$1/\ln 2$
\end{tabu}
\resizebox{\textwidth}{!}{\centering
\begin{minipage}{\textwidth}
\begin{tabu}{llll}
	\Xhline{1pt}
	$Z=\sum\limits_n\! e^{-\frac{E_n}{kT}}$&$\implies$ &$Z=\textcolor{red}{\sum\limits_{p:U(p)\downarrow}\!\!2^{-\frac{\ell(p)}{T}}}$&\text{Partition function}\\
	$F=-kT\ln Z$&$\implies$ &$F=-T\log Z$&\text{Free energy}\\
	$P(n)=\dfrac{1}{Z}e^{-\frac{E_n}{kT}}$&$\implies$ &$P(p)=\dfrac{1}{Z}2^{-\frac{\ell(p)}{T}}$&\text{Boltzmann distribution}\\
	$E=\sum\limits_n\! P(n)E_n$&$\implies$ &$E=\!\sum\limits_{p:U(p)\downarrow}\!\!P(p)\ell(p)$&\text{Energy}\\
	$S=\dfrac{E-F}{T}$&$\implies$ &$S=\dfrac{E-F}{T}\textcolor{red}{=H(P)}$&\text{Entropy}\\
	\Xhline{1pt}
\end{tabu}
\end{minipage}}
\[T\geq 1\implies H(P)=\infty\qquad 0<T<1\implies H(P)<\infty\]
\[\fbox{Temperature = Compression Rate}\]
\[T=\lim\limits_{n\to\infty}\frac{K(Z_{1:n})}{n}=\lim\limits_{n\to\infty}\frac{K(F_{1:n})}{n}=\lim\limits_{n\to\infty}\frac{K(E_{1:n})}{n}=\lim\limits_{n\to\infty}\frac{K(S_{1:n})}{n}\]
Fixpoint theorem: for $T\in(0,1)$, if $Z$ is computable, then $\lim\limits_{n\to\infty}\frac{K(T_{1:n})}{n}=T$.
}

\frame{{Computable Universal Predicter}
\[Z_h=\textcolor{blue}{\sum\limits_{p:U(p)=h*}\!\!2^{-\frac{\ell(p)}{T}}}\]
\hrule
\[Z(h)=\sum\limits_{p:U(p)=h*}\!\!2^{-\frac{\textcolor{red}{\ell(p)+\log t(p,h)}}{T}}\]
\[\fbox{$T$ is computable $\implies Z(h)$ is computable}\]
\[\sum\limits_{t=1}^\infty |1-Z(h_t|h_{<t})|\leq\dfrac{Km(h)\ln 2+\ln t(p,h)}{T}\]
}

\frame{{Stochastic Case}
\[E_{\{h,\nu\}}=-\log w_\epsilon^\nu-\log\nu(h)\]
\[Z_h=\sum\limits_{\nu\in\mathcal{M}}2^{-E_{\{h,\nu\}}}=\sum\limits_{\nu\in\mathcal{M}}w_\epsilon^\nu\nu(h)=\xi(h)\]
\[P_h(\nu)=\frac{2^{-E_{\{h,\nu\}}}}{Z_h}=\frac{w_\epsilon^\nu\nu(h)}{\xi(h)}=w_h^\nu\]
\[F_h=-\log Z_h=\underbrace{-\log\xi(h)}_{\approx K(h)}=\underbrace{\mathbb{E}_{w_h}[-\log\nu(h)]}_{noise}+\underbrace{D(w_h\|w_\epsilon)}_{surprise}\]
}

\frame{{Deduction vs Induction}\vspace{-1ex}
	\begin{table}[H]\small
		\begin{center}
			\resizebox{.98\textwidth}{!}{
					\begin{minipage}{77ex}
						\centering\begin{tabu}{l|rcl}
						\hline
							& \textbf{Induction}           & $\bm{|}$ & \textbf{Deduction} \\ \hline
							Type of inference  & generalization/prediction& $\Leftrightarrow$          & specialization/derivation \\
							Framework          & probability axioms       & $\widehat=$                & logical axioms \\ % better word for framework
							Assumptions        & prior                    & $\widehat=$                & non-logical axioms \\   % postulates
							Inference rule     & Bayes rule               & $\widehat=$                & modus ponens \\
							Results            & posterior                & $\widehat=$                & theorems \\
							Universal scheme   & Solomonoff probability   & $\widehat=$                & ZFC \\
							Universal inference& universal induction      & $\widehat=$                & universal theorem prover \\ 
							\hline
							Limitation         & uncomputable (Turing)            &
							$\widehat=$                & imcomplete (G\"odel)\\
							In practice        & approximations             &
							$\widehat=$                & semi-formal proofs\\
							Operation          & computation              &
							$\widehat=$                & proof\\
						\hline
						\end{tabu}
			\end{minipage}}
		\end{center}
	\end{table}\vspace{-1ex}
	\begin{table}
		\centering
		\begin{tabu}{c|c}
			\hline
			\large\textcolor{red}{Logic} &\large\textcolor{red}{Statistics}\\
			\hline
			rule-based &data-driven\\
			\hline
			rigour &possibility\\
			\hline
			knowable &black-box\\
			\hline
			simple \& perfect world &complex \& uncertain world\\
			\hline
			\Large\textcolor{red}{$\times$} &\Large\textcolor{red}{$\checkmark$}\\
			\hline
		\end{tabu}%\vspace{-1ex}\caption{How to unify?}
	\end{table}
}

\frame{{Prediction with Expert Advice}
	Assume that there is some large, possibly infinite, class of `experts' which make predictions. The aim is to observe how each of these experts perform and develop independent predictions based on this performance.
	\begin{itemize}
		\item Follow the (perturbed) leader.
		\item Predicts according to a majority vote by the ``good'' experts.
		\item Multiplicative Weights. --- take expert which performed best in past with high probability and others with smaller probability.
		\item Regularization. Choose the class of all computable experts, and penalize ``complex'' experts.
		\item Universal Portfolios.
	\end{itemize}
}

\frame{{Universal Portfolios}
	\begin{itemize}
		\item the agent chooses a distribution $\bm{b}_t\in\Delta_n:=\left\{x\in[0,1]^n\colon\|x\|_1=1\right\}$ of wealth over $n$ goods.
		\item nature chooses returns $\bm{x}_t\in(\mathbb{R}^+)^n$, where \[(\bm{x}_t)_i=\frac{\text{price of good $i$ at end of $t$}}{\text{price of good $i$ at beginning of $t$}}\]
		\item the total wealth.
		$W_t(\bm{b},\bm{x})=W_1\prod\limits_{k=1}^t\bm{b}_k^\top(\bm{x}_{<k})\bm{x}_k$
		\item regret.
		$R_t:=\max\limits_{\bm{b}\in\Delta_n}\sum\limits_{k=1}^t\log \bm{b}_k^\top(\bm{x}_{<k})\bm{x}_k-\sum\limits_{k=1}^t\log \bm{b}_k^\top(\bm{x}_{<k})\bm{x}_k$
		\item universal portfolios.
		\begin{align*}
		\hat{\bm{b}}_1&:=\left(\tfrac{1}{n},\dots,\tfrac{1}{n}\right)\\
		\hat{\bm{b}}_{t+1}(\bm{x}_{1:t})&:=\frac{\int_{\Delta_n}\!\!\bm{b} W_t(\bm{b},\bm{x})\mathrm{d}\bm{b}}{\int_{\Delta_n}\!\!W_t(\bm{b},\bm{x})\mathrm{d}\bm{b}}
		\end{align*}
		\item Asymptotic Optimality.
		$\frac{1}{t}\log W_t(\hat{\bm{b}},\bm{x})\xrightarrow{t\to\infty}\frac{1}{t}\log \max\limits_{\bm{b}\in\Delta_n}W_t(\bm{b},\bm{x})$
	\end{itemize}
}

\frame{{Randomness}
\begin{enumerate}
	\item \textcolor{red}{Typicalness} (The statistician's approach): A random sequence is the typical outcome of a random variable. Random sequences should not have effectively rare distinguishing properties.
	\item \textcolor{red}{Incompressibility} (The coder's approach): Rare patterns can be used to compress information. Random sequences should not be effectively described by a significantly shorter description than their literal representation.
	\item \textcolor{red}{Unpredictability} (The gambler's approach): A betting strategy can exploit rare patterns. Random sequences should be unpredictable. No effective martingale can make an infinite amount betting on the bits.
\end{enumerate}
}

\frame{{The Statistician's Approach}
\begin{itemize}
	\item A random sequence should be absolutely normal.
	\item If you select a subsequence, then it should satisfy the law of large numbers, the law of the iterated logarithm\dots
	\item But what selection functions should be allowed? Computable?
	\item Martin-L\"of: we can effectively test whether a particular infinite sequence does not satisfy a particular law of randomness by effectively testing whether the law is violated on increasingly long initial segments. We should consider the intersection of all sets of measure one with recursively enumerable complements. (Such a complement set is expressed as the union of a recursively enumerable set of cylinders).
\end{itemize}
}

\frame{{Cantor Space $2^\omega$}
\begin{itemize}
	\item For $x\in 2^{<\omega}$, the cylinder set $\Gamma_x:=\{y\in 2^\omega\colon x\prec y\}$ is the basic open set. It corresponds to the interval $[0.x,0.x+2^{-\ell(x)})$.
	\item For $A\subset 2^{<\omega}$, the open set generated by $A$ is $\Gamma_A:=\bigcup\limits_{x\in A}\Gamma_x$.
	\item The Lebesgue measure $\mu(\Gamma_x):=2^{-\ell(x)}$, $\mu(x):=\mu(\Gamma_x)$.
	\item The outer measure of $C\subset 2^\omega$ is $\mu^*(C):=\inf\left\{\sum\limits_{x\in A}2^{-\ell(x)}\colon C\subset\Gamma_A\right\}$.
	\item The inner measure of $C$ is $\mu_*(C):=1-\mu^*(2^\omega\setminus C)$.
	\item If $C$ is measurable, then $\mu^*(C)=\mu_*(C)$.
	\item $A\subset 2^\omega$ has measure $0$ iff there is a sequence $\{V_n\}_{n\in\omega}$ of open sets s.t. $A\subset\bigcap\limits_{n\in\omega}V_n$ and $\lim\limits_{n\to\infty}\mu(V_n)=0$.
\end{itemize}
}

\frame{{Martin-L\"of Randomness}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{definition}[Martin-L\"of Randomness]
\begin{itemize}
	\item A total lower semicomputable function $\delta\colon 2^{<\omega}\to\omega$ is a Martin-Löf test if $\forall n\colon\mu(V_n)\leq 2^{-n}$, where $V_n:=\{x\colon\delta(x)\geq n\}$.
	\item $x\in 2^\omega$ is ML-random if for every ML-test $\delta$, $\sup_n\delta(x_{1:n})<\infty$.
\end{itemize}
\end{definition}
\[\delta(x)<\infty\iff x\notin\bigcap\limits_{n=1}^\infty V_n\]
\begin{definition}[Martin-L\"of Randomness]
\begin{itemize}
	\item A Martin-Löf test is a uniformly c.e. (i.e., $\big\{\langle n,x\rangle\colon x\in V_n\big\}$ is c.e.) sequence of open sets $\{V_n\}_{n\in\omega}$ s.t. $\forall n\colon \mu(V_n)\leq 2^{-n}$.
	\item $x\in 2^\omega$ is ML-random if for every ML-test $\{V_n\}_{n\in\omega}$, $x\notin\bigcap\limits_{n=1}^\infty V_n$.
\end{itemize}
\end{definition}
}

\frame{{Martin-L\"of Randomness}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{definition}[Universal Martin-L\"of Test]
A ML-test $\delta_0$ is \emph{universal} if for every ML-test $\delta$, $\exists c\forall x\colon\delta_0(x)\geq\delta(x)-c$.
\end{definition}
A ML-test $\{U_n\}_{n\in\omega}$ is \emph{universal} iff for every ML-test $\{V_n\}_{n\in\omega}$, $\bigcap\limits_{n\in\omega}U_n\supset\bigcap\limits_{n\in\omega}V_n$.
\begin{center}
\fbox{$\delta(x):=\ell(x)-K(x|\ell(x))$ is a universal ML-test.}
\end{center}
\[R_b:=\big\{x\in 2^\omega\colon\exists n\big(K(x_{1:n})<n-b\big)\big\}\]
\begin{center}
\fbox{$\{R_b\}_{b\in\omega}$ is a universal ML-test.}
\end{center}
\begin{theorem}[Schnorr 1973]
A sequence $x\in 2^\omega$ is ML-random iff it is $1$-random.
\end{theorem}
}

\frame{{The Gambler's Approach}
\begin{itemize}
	\item A martingale is a function $d\colon 2^{<\omega}\to[0,\infty)$ s.t. for every $\sigma\in 2^{<\omega}$
	\[d(\sigma)=\frac{d(\sigma0)+d(\sigma1)}{2}\]
	\item A supermartingale is a function $d\colon 2^{<\omega}\to[0,\infty)$ s.t. for every $\sigma\in 2^{<\omega}$
	\[d(\sigma)\geq\frac{d(\sigma0)+d(\sigma1)}{2}\]
	\item A (super)martingale $d$ succeeds on $x\in 2^\omega$ if $\limsup\limits_{n\to\infty}d(x_{1:n})=\infty$.
\end{itemize}
\begin{theorem}
A sequence $x\in 2^\omega$ is ML-random iff no c.e. (super)martingale succeeds on it.
\end{theorem}
}

\frame{{The Coder's Approach}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{columns}
\column{.39\textwidth}
\begin{definition}[$1$-Randomness]
$x\in 2^\omega$ is $1$-random if
\[\exists c\forall n\colon K(x_{1:n})\geq n-c\]
\end{definition}
\column{.62\textwidth}
\begin{theorem}
The following are equivalent.
\begin{itemize}
	\item $x\in 2^\omega$ is ML-random.
	\item No c.e. (super)martingale succeeds on it.
	\item $\exists c\forall n\colon K(x_{1:n})\geq n-c$
	\item $\forall n\colon Km(x_{1:n})\eqa n$
	\item $\lim\limits_{n\to\infty}K(x_{1:n})-n=\infty$
	\item $\sum\limits_{n=1}^\infty 2^{n-K(x_{1:n})}<\infty$
	\item $\sup_n 2^{n-K(x_{1:n})}<\infty$
	\item $C(x_{1:n})\geqa n-K(n)$
	\item $C(x_{1:n})\geqa n-f(n)$ for every computable $f$ s.t. $\sum\limits_{n=1}^\infty 2^{-f(n)}<\infty$.
\end{itemize}
\end{theorem}
\end{columns}
}

\frame{{}
\begin{definition}[Solovay Reducibility]
Let $a_n\to\alpha$ and $b_n\to\beta$ be two computable strictly increasing sequences of rationals converging to lower semicomputable reals $\alpha$ and $\beta$. We say that $\alpha\leq_S\beta$ if there is a constant $c$ and a total computable function $f$ s.t. $\forall n\colon\alpha-a_{f(n)}\leq c(\beta-b_n)$.
\end{definition}
\begin{theorem}
For lower semicomputable reals $\alpha$, the following are equivalent.
\begin{itemize}
	\item $\alpha$ is $1$-random
	\item $\alpha\geq_S\beta$ for all lower semicomputable reals $\beta$.
	\item $\alpha\geq_S\Omega$
	\item $K(\alpha_{1:n})\geqa K(\beta_{1:n})$ for all lower simicomputable reals $\beta$.
	\item $K(\alpha_{1:n})\geqa K(\Omega_{1:n})$
	\item $K(\alpha_{1:n})\eqa K(\Omega_{1:n})$
	\item $\alpha=\Omega_U:=\sum\limits_{p:U(p)\downarrow}2^{-\ell(p)}$ for some universal prefix Turing machine $U$.
\end{itemize}
\end{theorem}
}

\frame{{$\mu/\xi$-randomness}
\begin{definition}[$\mu/\xi$-randomness]
\begin{itemize}
	\item A sequence $x\in 2^\omega$ is $\mu/\xi$-random if $\exists c\forall n\colon \xi(x_{1:n})\leq c\cdot\mu(x_{1:n})$.
	\item A sequence $x\in 2^\omega$ is $\mu$-ML-random if $\exists c\forall n\colon M(x_{1:n})\leq c\cdot\mu(x_{1:n})$.
\end{itemize}
\end{definition}
\begin{itemize}
\item $x_{1:\infty}$ is $\mu$-ML-random iff $\sup_n\delta(x_{1:n}|\mu)<\infty$, where
\[\delta(x|\mu):=\log\frac{M(x)}{\mu(x)}\]
\item For a computable $\mu$, $x_{1:\infty}$ is $\mu$-ML-random iff
\[\forall n\colon Km(x_{1:n})\eqa-\log\mu(x_{1:n})\]
\end{itemize}
}

\frame{{Randomness, Triviality, Logical Depth}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{itemize}
\item $A\subset\mathbb{N}$ is \emph{low} if $A'\leq_T\emptyset'$, and $A$ is \emph{high} if $\emptyset''\leq_T A'$.
\item $A$ is \emph{low for ML-randomness} if each ML-random set is already ML-random relative to $A$.
\item $A$ is \emph{low for $K$} if $\exists c\forall x\colon K(x)\leq K^A(x)+c$.
\item $x\in 2^\omega$ is \emph{$K$-trivial} if $\exists c\forall n\colon K(x_{1:n})\leq K(n)+c$.
\end{itemize}
\begin{theorem}
$A$ is $K$-trivial $\iff$ $A$ is low for ML-randomness $\iff A$ is low for $K$.
\end{theorem}
Some sequences are $K$-trivial but not computable.

Neither randoms, nor $K$-trivials, are deep.
\begin{definition}[Logical Depth]
The logical depth of $x$ at a significance level $b$ is
\[\mathit{depth_b}(x):=\min\left\{t\colon U^t(p)=x\;\;\&\;\;\ell(p)-K(x)\leq b\right\}\]
\end{definition}
$x$ is called \emph{shallow} if $\mathit{depth_b}(x)\leqa\ell(x)$.\\
Let $\chi_i:=\llbracket\varphi_i(i)\downarrow\rrbracket$. $\chi_{1:\infty}$ is deep. $\Omega$ is shallow.
}

\frame{{Effective Complexity}
	\resizebox{\textwidth}{!}{
	\begin{minipage}{\textwidth}
				\begin{align*}
				\delta(x|A)&:=\log |A|-K(x|A)  &\text{[randomness deficiency]}\\
				\delta(x|\mu)&:=\log\dfrac{M(x)}{\mu(x)} &\text{[$\mu$-randomness deficiency]}\\
				\beta_x(k)&:=\min\limits_A\{\delta(x|A)\colon x\in A\;\;\&\;\;K(A)\leq k\} &\text{[Best-Fit]}\\
				h_x(k)&:=\min\limits_A\{\log |A|\colon x\in A\;\;\&\;\;K(A)\leq k\} &\text{[Kolmogorov structure function / ML]}\\
				h_x(k)&:=\min\limits_\mu\{-\log \mu(x)\colon K(\mu)\leq k\}  &\text{[[Kolmogorov structure function / ML]}\\
				A^*(x)&:=\iota A\Big[x\in A\;\;\&\;\;K(A)=\mu k\big[k+h_x(k)\eqa K(x)\big]\Big]&\text{[Kolmogorov minimal sufficient statistic]}\\
				\lambda_x(k)&:=\min\limits_A\left\{K(A)+\log |A|\colon x\in A\;\;\&\;\;K(A)\leq k\right\} &\text{[MDL]}\\
				\lambda_x(k)&:=\min\limits_\mu\{K(\mu)-\log\mu(x)\colon K(\mu)\leq k\} &\text{[MDL]}\\
				\Delta(x|A)&:=K(A)+\log |A|-K(x) &\text{[discrepancy]}\\
				\mathit{soph_c}(x)&:=\min\limits_A\{K(A)\colon\Delta(x|A)<c\} &\text{[sophistication]}\\
				\mathit{csoph}(x)&:=\min\limits_A\{K(A)+\Delta(x|A)\} &\text{[coarse sophistication]}\\
				\Sigma(\mu)&:=K(\mu)+H(\mu) &\text{[total information]}\\
				\mathcal{E}_{\delta,\Delta}(x|\mathcal{M})&:=\min\limits_{\mu\in\mathcal{M}}\left\{K(\mu)\colon\Sigma(\mu)-K(x)\leq\Delta\;\;\&\;\;\mu(x)\geq 2^{-H(\mu)(1+\delta)}\right\} &\text{[effective complexity]}\\
				\mathcal{E}_\delta(x|\mathcal{M})&:=\min\limits_{\mu\in\mathcal{M}}\left\{K(\mu)+\Sigma(\mu)-K(x)\colon \mu(x)\geq 2^{-H(\mu)(1+\delta)}\right\}  &\text{[coarse effective complexity]}
				\end{align*}
	\end{minipage}}
}

\frame{{}
\begin{columns}
\column{.5\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{img/structure-function.pdf}
\end{figure}
\column{.5\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{img/sufficient-statistic.pdf}
\end{figure}
\end{columns}
}

\frame{{}
\begin{theorem}
For $x$ and $k$,
\[\lambda_x(k)\leq h_x(k)+k\leqa\lambda_x(k)+K(k)\]
For $k$ with $0\leq k\leq K(x)-O(\log\ell(x))$,
\[\beta_x(k)+K(x)\leqa\lambda_x(k)\]
\[\lambda_x(k+O(\log \ell(x)))\leq\beta_x(k)+K(x)\]
\end{theorem}
In other words, the equality
\[\beta_x(k)+K(x)=\lambda_x(k)=h_x(k)+k\]
holds within logarithmic additive terms in argument and value.
}

\frame{{Sophistication and Computational Depth}
\begin{theorem}
\[\mathit{csoph}(x)=\min\limits_c\{\mathit{soph_c}(x)+c\}\]
\end{theorem}
\begin{align*}
K^t(x)&:=\min\limits_p\{\ell(p)\colon U^t(p)=x\}\tag{\text{\footnotesize Time-bounded Kolmogorov Complexity}}\\
\mathit{depth^t}(x)&:=K^t(x)-K(x)\tag{\text{\footnotesize Basic Computational Depth}}\\
\mathit{depth_{BB}}(x)&:=\min\limits_t\{\mathit{depth^t}(x)+K(t)\}\tag{\text{\footnotesize Busy Beaver Computational Depth}}
\end{align*}
\begin{theorem}
\[|\mathit{csoph}(x)-\mathit{depth_{BB}}(x)|\leq O(\log \ell(x))\]
\end{theorem}
}

\frame{{Zurek's Physical Entropy}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{definition}[Physical Entropy]
Physical entropy $S(d)$ of a microstate $d$ is the sum of the conditional Shannon entropy $H_d:=-\sum\limits_k P(k|d)\log P(k|d)$ and of the Kolmogorov Complexity $K(d)$.
\[S(d):=H_d+K(d)\]
\end{definition}
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\textwidth,angle=0,origin=c]{img/physical-entropy.pdf}\caption{random vs regular microstate}
	\end{center}
\end{figure}
}

\frame{{Maxwell's Demon \& Landauer's Principle}
\begin{columns}
\column{.75\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{img/maxwell-demon}\caption{The demon turns entropy into information, the information-erasure operation turns information into entropy. In the course of ideal measurement on an equilibrium ensemble, the decrease of the entropy must be compensated by the increase of the size of the minimal record, and vice versa. $\Delta H\approx-\langle\Delta K\rangle$.}
\end{figure}
\column{.25\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{img/books-burning}\caption{\tiny Destroying information generates heat}
\end{figure}
\end{columns}
Landauer's Principle: Logically irreversible computation costs energy. Erasing $1$ bit of information dissipates at least $kT\ln2$ of heat into the environment.
}

\frame{{G\'acs' Algorithmic Entropy}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{definition}[Algorithmic Entropy]
\begin{itemize}
	\item \emph{coarse-grained algorithmic entropy} of a cell $\Gamma$ with respect to $\mu$
	\[H_\mu(\Gamma):=\log\mu(\Gamma)+K(\Gamma|\mu)\]
	\item \emph{fine-grained algorithmic entropy} of $x\in 2^\omega$ with respect to $\mu$
	\[H_\mu(x):=\inf_n\big\{\log\mu(x_{1:n})+K(x_{1:n}|
	\mu)\big\}\]
\end{itemize}
\end{definition}
\begin{itemize}
	\item $-H_\mu(x)$ is a universal ML-test: $x$ is $\mu$-ML-random iff $H_\mu(x)>-\infty$.
	\item The fine-grained algorithmic entropy of a microstate can be approximated by the coarse-grained algorithmic entropies of successively smaller cells containing it.
	\[H_\mu(x)=\inf_n\big\{H_\mu(\Gamma_{x_{1:n}})\big\}\]
\end{itemize}
}


%--------------------------%
\section{Reinforcement Learning}
%--------------------------%


\frame{{Markov Decision Process}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{columns}
\column{.43\textwidth}
	\begin{center}
		A (finite) MDP $(\mathcal{S},\mathcal{A},P, R)$.
	\end{center}
	\begin{figure}
	\includegraphics[width=\textwidth]{img/mdp}
	\end{figure}
\column{.57\textwidth}
	\begin{definition}[Value of a state under $\pi$]
		\[V^\pi(s):=\mathbb{E}\left[\sum\limits_{k=0}^\infty\gamma^k r_{t+k+1}\,\middle|\, S_t=s\right]\]
	\end{definition}
	\begin{definition}[Action-value under $\pi$]
		\[Q^\pi(s,a):=\mathbb{E}\left[\sum\limits_{k=0}^\infty\gamma^k r_{t+k+1}\,\middle|\, S_t=s,A_t=a\right]\]
	\end{definition}
\end{columns}
}

\frame{{Bellman Expectation Equations}
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{0pt}
\[r(s,a):=\mathbb{E}[r|s,a]\]
\begin{tcolorbox}
\begin{align*}
V^\pi(s)&=\sum\limits_{a\in\mathcal{A}}\pi(a|s)Q^\pi(s,a)\\
Q^\pi(s,a)&=r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V^\pi(s')
\end{align*}
\end{tcolorbox}
\[V^\pi(s)=\mathbb{E}\left[r+\gamma V^\pi(s')\,\middle|\, s\right]=\sum\limits_{a\in\mathcal{A}}\pi(a|s)\left(r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V^\pi(s')\right)\]
advantage
\[A^\pi(s,a):=Q^\pi(s,a)-V^\pi(s)\]
}

\frame{{Bellman Optimality Equations}
	\begin{definition}[Optimal Values]
		\[V^*(s):=\max\limits_\pi V^\pi(s)\]
		\[Q^*(s,a):=\max\limits_\pi Q^\pi(s,a)\]
	\end{definition}
	\begin{definition}[Optimal Policy]
		A policy $\pi$ is called optimal if $\forall s\in\mathcal{S}\colon V^\pi(s)=V^*(s)$.
	\end{definition}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{tcolorbox}
		\begin{align*}
		V^*(s)&=\max\limits_{a\in\mathcal{A}}Q^*(s,a)\\
		Q^*(s,a)&=r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V^*(s')
		\end{align*}
	\end{tcolorbox}
}

\frame{{Action/Policy Evaluation Operator \& Greedy Policy}
	\begin{definition}[Action Evaluation Operator]
		\[T_aV(s):=r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V(s')\]
	\end{definition}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{definition}[Policy Evaluation Operator]
		\[T^\pi V(s):=\sum\limits_{a\in\mathcal{A}}\pi(a|s)T_aV(s)=\sum\limits_{a\in\mathcal{A}}\pi(a|s)\left(r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V(s')\right)\]
		\[T^* V(s):=\max\limits_{a\in\mathcal{A}}T_aV(s)=\max\limits_{a\in\mathcal{A}}\left(r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V(s')\right)\]
	\end{definition}
	\begin{definition}[Greedy policy]
		Policy $\pi$ is greedy w.r.t. $V$ if $T^\pi V=T^* V$.
	\end{definition}	
}

\frame{{Banach's Fixpoint Theorem}
	\begin{theorem}[Banach's Fixpoint Theorem]
		Let $\mathcal{V}$ be a Banach space and $T\colon\mathcal{V}\to\mathcal{V}$ be a contraction mapping, with Lipschitz constant $\gamma<1$. Then $T$ has a unique fixpoint $v\in\mathcal{V}$. Further, for each $v_0\in\mathcal{V}$, $\lim\limits_{n\to\infty}\|T^n(v_0)-v\|=0$,  and the convergence is geometric:
		\[\|T^n(v_0)-v\|\leq\gamma^n\|v_0-v\|\]
	\end{theorem}
}

\frame{{Application of Banach's Fixpoint Theorem}
	\begin{theorem}
		$(\mathcal{V},\|\cdot\|_\infty)$ is a Banach space, where
		$\mathcal{V}:=\left\{V\in\mathbb{R}^{\mathcal{S}}\colon \|V\|_\infty<\infty\right\}$ and $\|V\|_\infty:=\max\limits_{s\in\mathcal{S}}|V(s)|$.
	\end{theorem}
	\begin{tcolorbox}
		\begin{itemize}
			\item $T^\pi$ is a contraction, and $V^\pi$ is the unique fixpoint of $T^\pi$.
			\[\lim\limits_{n\to\infty}\left\|(T^\pi)^nV_0-V^\pi\right\|_\infty=0\]
			\item $T^*$ is a contraction, and $V^*$ is the unique fixpoint of $T^*$.
			\[\lim\limits_{n\to\infty}\left\|(T^*)^nV_0-V^*\right\|_\infty=0\]
		\end{itemize}
	\end{tcolorbox}
}

\frame{{Application of Banach's Fixpoint Theorem}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{tcolorbox}
		\[T^\pi Q(s,a):=r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)\sum\limits_{a'\in\mathcal{A}}\pi(a'|s')Q(s',a')\]
		\[T^* Q(s,a):=r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)\max\limits_{a'\in\mathcal{A}}Q(s',a')\]
		\begin{itemize}
			\item $T^\pi$ is a contraction, and $Q^\pi$ is the unique fixpoint of $T^\pi$.
			\[\lim\limits_{n\to\infty}\left\|(T^\pi)^nQ_0-Q^\pi\right\|_\infty=0\]
			\item $T^*$ is a contraction, and $Q^*$ is the unique fixpoint of $T^*$.
			\[\lim\limits_{n\to\infty}\left\|(T^*)^nQ_0-Q^*\right\|_\infty=0\]
		\end{itemize}
	\end{tcolorbox}
}

\frame{{Two Theorems}
	\begin{theorem}[Fixpoint of Bellman Optimality Operator]
		Let $V$ be the fixpoint of $T^*$ and assume that there is policy $\pi$ which is greedy w.r.t $V$. Then $V= V^*$ and $\pi$ is an optimal policy.
	\end{theorem}
	\begin{theorem}[Policy Improvement Theorem]
		Choose some stationary policy $\pi_0$ and let $\pi$ be
		greedy w.r.t. $V^{\pi_0}$. Then $V^\pi\geq V^{\pi_0}$, i.e., $\pi$ is an improvement upon $\pi_0$. In particular, if $T^*V^{\pi_0}(s)>V^{\pi_0}(s)$ for some state $s$ then $\pi$ strictly improves upon $\pi_0$ at $s\colon V^\pi(s)>V^{\pi_0}(s)$. On the other hand, when $T^*V^{\pi_0}(s)=V^{\pi_0}(s)$ then $\pi_0$ is an optimal policy.
	\end{theorem}
}

\frame{{Solving MDPs --- Finite-Horizon Dynamic Programming}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	Principle of optimality: the tail of an optimal policy is
	optimal for the ``tail'' problem.
	
	\begin{tcolorbox}[colback=cyan!3,colframe=back!85,title=Backward Induction]
		\begin{itemize}
			\item Backward recursion: $V_N^*(s)=r_N(s)$ and for $k=N-1,\dots,0$
			\[V_k^*(s)=\max\limits_{a\in\mathcal{A}_k}\left(r_k(s,a)+\sum\limits_{s'\in\mathcal{S}_{k+1}}P_k(s'|s,a)V_{k+1}^*(s')\right)\]
			\item Optimal policy: for $ k=0,\dots,N-1$
			\[\pi_k^*(s)\in\argmax\limits_{a\in\mathcal{A}_k}\left(r_k(s,a)+\sum\limits_{s'\in\mathcal{S}_{k+1}}P_k(s'|s,a)V_{k+1}^*(s')\right)\]
		\end{itemize}
	\end{tcolorbox}
	\begin{itemize}
		\item Cost: $N|\mathcal{S}||\mathcal{A}|$ vs $|\mathcal{A}|^{N|\mathcal{S}|}$ of brute force policy search.
		\item From now on, we will consider infinite-horizon discounted MDPs.
	\end{itemize}
}

\frame{{Solving MDPs --- Value Iteration}
	\begin{theorem}[Principle of Optimality]
		A policy $\pi$ achieves the optimal value from state $s$,
		$V^\pi(s)=V^*(s)$, iff, for any state $s'$ reachable from $s$, $\pi$ achieves the optimal value from state $s'$, $V^\pi(s')=V^*(s')$.
	\end{theorem}
	Any optimal policy $\pi^*$ can be subdivided into two components:
	\begin{itemize}
		\item an optimal first action $a^*$,
		\item followed by an optimal policy from successor state $s'$.
		\[V^*(s)=\max\limits_{a\in\mathcal{A}}\left(r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V^*(s')\right)\]
	\end{itemize}
	\textcolor{blue}{Value Iteration}: $V_{k+1}\gets T^*V_k$
	\[V_1\to V_2\to\cdots\to V^*\]
	\begin{center}
		\fbox{$\|V_{k+1}-V_k\|_\infty<\varepsilon\implies\|V_{k+1}-V^*\|_\infty<\frac{2\gamma\varepsilon}{1-\gamma}$}
	\end{center}
}

\frame{{Solving MDPs --- Policy Iteration}
	\textcolor{blue}{Policy Iteration}: $\pi_0\xrightarrow{E} V^{\pi_0}\xrightarrow{I}\pi_1\xrightarrow{E} V^{\pi_1}\xrightarrow{I}\pi_2\xrightarrow{E}\cdots\xrightarrow{I}\pi^*\xrightarrow{E} V^*$
	\begin{itemize}
		\item E --- policy evaluation:
		\[V_{k+1}^\pi\gets T^\pi V_k^\pi\]
		\item I --- policy improvement:
		\[\pi_{k+1}(s):=\argmax\limits_{a\in\mathcal{A}}\left(r(s,a)+\gamma\sum\limits_{s'\in\mathcal{S}}P(s'|s,a)V^{\pi_k}(s')\right)\]
	\end{itemize}
	\begin{center}
		\includegraphics[width=.65\textwidth,angle=0,origin=c]{img/policyit.pdf}
	\end{center}
}

\frame{{Monte-Carlo Methods}
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{0pt}
MC learns from complete episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return.
\[G_t:=\sum\limits_{k=0}^{T-t-1}\gamma^k r_{t+k+1}\]
\[V(s):=\frac{\sum\limits_{t=1}^T\llbracket S_t=s\rrbracket G_t}{\sum\limits_{t=1}^T\llbracket S_t=s\rrbracket}\]
\[Q(s,a):=\frac{\sum\limits_{t=1}^T\llbracket S_t=s,A_t=a\rrbracket G_t}{\sum\limits_{t=1}^T\llbracket S_t=s,A_t=a\rrbracket}\]
}

\frame{{Temporal-Difference Learning}
TD Learning is model-free and learns from incomplete episodes of experience.
\begin{align*}
	&V(s_t)\gets(1-\alpha)V(s_t)+\alpha G_t\\
	&V(s_t)\gets V(s_t)+\alpha(G_t-V(s_t))\\
	&V(s_t)\gets V(s_t)+\alpha(r_{t+1}+\gamma V(s_{t+1})-V(s_t))
\end{align*}
\[Q(s_t,a_t)\gets Q(s_t,a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))\]
\begin{itemize}
	\item MC updates value $V(s_t)$ toward actual return $G_t$.
	\[V(s_t)\gets V(s_t)+\alpha(G_t-V(s_t))\]
	\item TD updates value $V(s_t)$ toward estimated return $r_{t+1}+\gamma V(s_{t+1})$.
	\[V(s_t)\gets V(s_t)+\alpha(\underbrace{\overbrace{r_{t+1}+\gamma V(s_{t+1})}^{\text{TD target}}-V(s_t)}_{\text{TD error}})\]
\end{itemize}
}

\frame{{SARSA: On-Policy TD control}
\textcolor{blue}{SARSA}
\begin{enumerate}
	\item At time step $t$, we start from state $s_t$ and pick action according to $Q$ values, $a_t=\argmax\limits_{a\in\mathcal{A}}Q(s_t,a)$; $\varepsilon$-greedy is commonly applied.
	\item With action $a_t$, we observe reward $r_{t+1}$ and get into $s_{t+1}$.
	\item Then pick the next action $a_{t+1}=\argmax\limits_{a\in\mathcal{A}}Q(s_{t+1},a)$.
	\item Update the action-value function: $Q(s_t,a_t)\gets Q(s_t,a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))$.
	\item $t = t+1$ and repeat from step $1$.
\end{enumerate}
\vspace{2ex}
\textcolor{blue}{Expected SARSA}
\[Q(s_t,a_t)\gets Q(s_t,a_t)+\alpha\left(r_{t+1}+\gamma \sum\limits_{a\in\mathcal{A}}\pi(a|s_{t+1})Q(s_{t+1},a)-Q(s_t,a_t)\right)\]
}

\frame{{Q-Learning: Off-policy TD control}
\begin{enumerate}
	\item At time step $t$, we start from state $s_t$ and pick action according to $Q$ values, $a_t=\argmax\limits_{a\in\mathcal{A}}Q(s_t,a)$; $\varepsilon$-greedy is commonly applied.
	\item With action $a_t$, we observe reward $r_{t+1}$ and get into $s_{t+1}$.
	\item Update the action-value function: $Q(s_t,a_t)\gets Q(s_t,a_t)+\alpha\left(r_{t+1}+\gamma\max\limits_{a\in\mathcal{A}}Q(s_{t+1},a)-Q(s_t,a_t)\right)$.
	\item $t = t+1$ and repeat from step $1$.
\end{enumerate}
\begin{figure}[!htb]
\includegraphics[width=.5\textwidth]{img/sarsa.pdf}\caption{SARSAR, Expected SARSA, and Q-Learning}
\end{figure}
}

\frame{{Monte-Carlo Backup}
\[V(s_t)\gets V(s_t)+\alpha(G_t-V(s_t))\]
\begin{figure}
\includegraphics[width=\textwidth]{img/tree-mc.pdf}
\end{figure}
}

\frame{{Temporal-Difference Backup}
\[V(s_t)\gets V(s_t)+\alpha(r_{t+1}+\gamma V(s_{t+1})-V(s_t))\]
\begin{figure}
\includegraphics[width=\textwidth]{img/tree-td.pdf}
\end{figure}
}

\frame{{Dynamic Programming Backup}
\[V(s_t)\gets\mathbb{E}_\pi[r_{t+1}+\gamma V(s_{t+1})]\]
\begin{figure}
\includegraphics[width=\textwidth]{img/tree-dp.pdf}
\end{figure}
}

%--------------------------%
\subsection{Artificial Neural Network and Deep Reinforcement Learning}
%--------------------------%

\frame{{McCulloch-Pitts Artificial Neural Network}
	\begin{columns}\hspace{-1cm}
		\column{.6\textwidth}
			\resizebox{\textwidth}{!}{
				\begin{minipage}{\textwidth}
					\begin{tikzpicture}
					\node[rectangle, blue, ultra thick, fill=red, opacity=1,label=above:{\parbox{2cm}{\centering activation \\ function}}] at (2,-2)  (sigmoid) {\huge $g$};
					\node[darkgreen,label=above:{\parbox{2cm}{\centering output}}] at (4,-2)  (output) {$y$};
					%%Create a style for the arrows we are using
					\tikzset{normal arrow/.style={draw,-triangle 45,very thick}}
					%%Create the different coordinates to place the nodes
					\path (0,0) coordinate (1) ++(0,-2) coordinate (2) ++(0,-2) coordinate (3);
					\path (1) ++(-2,-1) coordinate (x1);
					\path (3) ++(-2,1) coordinate (x2);
					%%Place nodes at each point using the foreach construct
					\node[draw,circle,shading=axis,top color=green!1, bottom color=green,shading angle=5] at (2) (n2) {$\sum$};
					\node[above of=n2,above=.5cm,label=above:{\parbox{2cm}{\centering bias}}] (b) {$1$};
					\node[below of=n2,below=1ex] {linear};
					\node[below of=sigmoid,below=1ex] {nonlinear};
					\node[left of=n2,left=.2cm] (vdots) {\textcolor{cyan}{\huge $\vdots$}};
					%%Place the remaining nodes separately
					\node[label=above:{\parbox{2cm}{$\;\,$ inputs}}] (nx1) at (x1) {$x_1$};
					\node (nx2) at (x2) {$x_n$};
					%\node (ny)  at (7)  {$y$};
					\path[normal arrow,cyan] (b) -- node[right=.05em,cyan] {$b$} (n2);
					\path[normal arrow,cyan] (n2) --  (sigmoid);
					\path[normal arrow,cyan] (sigmoid) --  (output);
					\path[normal arrow,cyan] (nx1) -- node[label=above:{\parbox{2cm}{\centering\small \textcolor{black}{weights}}}][above=.3em,cyan](w1) {$w_1$} (n2);
					\path[normal arrow,cyan] (nx2) -- node[below=.3em,cyan] {$w_n$} (n2);
					\end{tikzpicture}
			\end{minipage}}
			{\Large \[y=g\left(\sum\limits_{i=1}^n w_ix_i+b\right)\]}
		\column{.4\textwidth}
			\resizebox{.8\textwidth}{!}{
				\begin{minipage}{\textwidth}
					\begin{tikzpicture}
					\node[blue, rectangle, fill=red, opacity=1] at (2,-2)  (sigmoid) {$\chi_{\geq 0}$};
					\node[darkgreen] at (4,-2)  (and) {\Huge{$\wedge$}};
					%%Create a style for the arrows we are using
					\tikzset{normal arrow/.style={draw,-triangle 45,very thick}}
					%%Create the different coordinates to place the nodes
					\path (0,0) coordinate (1) ++(0,-2) coordinate (2) ++(0,-2) coordinate (3);
					\path (1) ++(-2,-1) coordinate (x1);
					\path (3) ++(-2,1) coordinate (x2);
					%%Place nodes at each point using the foreach construct
					\node[draw,circle,shading=axis,top color=green!1, bottom color=green,shading angle=5] at (2) (n2) {$\sum$};
					\node[above of=n2,above=.5cm] (b) {$1$};
					%%Place the remaining nodes separately
					\node (nx1) at (x1) {$x_1$};
					\node (nx2) at (x2) {$x_2$};
					%\node (ny)  at (7)  {$y$};
					\path[normal arrow,cyan] (b) -- node[right=.05em,cyan] {$-2$} (n2);
					\path[normal arrow,cyan] (n2) --  (sigmoid);
					\path[normal arrow,cyan] (sigmoid) --  (and);
					\path[normal arrow,cyan] (nx1) -- node[above=.5em,cyan] {$+1$} (n2);
					\path[normal arrow,cyan] (nx2) -- node[below=.5em,cyan] {$+1$} (n2);
					\end{tikzpicture}
			\end{minipage}}\\
			\resizebox{.8\textwidth}{!}{
				\begin{minipage}{\textwidth}\vspace{-.5cm}
					\begin{tikzpicture}
					\node[blue, rectangle, fill=red, opacity=1] at (2,-2)  (sigmoid) {$\chi_{\geq 0}$};
					\node[darkgreen] at (4,-2)  (or) {\Huge{$\vee$}};
					%%Create a style for the arrows we are using
					\tikzset{normal arrow/.style={draw,-triangle 45,very thick}}
					%%Create the different coordinates to place the nodes
					\path (0,0) coordinate (1) ++(0,-2) coordinate (2) ++(0,-2) coordinate (3);
					\path (1) ++(-2,-1) coordinate (x1);
					\path (3) ++(-2,1) coordinate (x2);
					\node[draw,circle,shading=axis,top color=green!1, bottom color=green,shading angle=5] at (2) (n2) {$\sum$};
					\node[above of=n2,above=.5cm] (b) {$1$};
					%%Place the remaining nodes separately
					\node (nx1) at (x1) {$x_1$};
					\node (nx2) at (x2) {$x_2$};
					%\node (ny)  at (7)  {$y$};
					\path[normal arrow,cyan] (b) -- node[right=.05em,cyan] {$-1$} (n2);
					\path[normal arrow,cyan] (n2) --  (sigmoid);
					\path[normal arrow,cyan] (sigmoid) --  (or);
					\path[normal arrow,cyan] (nx1) -- node[above=.5em,cyan] {$+1$} (n2);
					\path[normal arrow,cyan] (nx2) -- node[below=.5em,cyan] {$+1$} (n2);
					\end{tikzpicture}
			\end{minipage}}\\
			\resizebox{.8\textwidth}{!}{
				\begin{minipage}{\textwidth}\vspace{-.5cm}
					\begin{tikzpicture}
					\node[blue, rectangle, fill=red, opacity=1] at (2,-2)  (sigmoid) {$\chi_{\geq 0}$};
					\node[darkgreen] at (4,-2)  (not) {\Huge{$\neg$}};
					%%Create a style for the arrows we are using
					\tikzset{normal arrow/.style={draw,-triangle 45,very thick}}
					%%Create the different coordinates to place the nodes
					\path (0,0) coordinate (1) ++(0,-2) coordinate (2) ++(0,-2) coordinate (3);
					\path (1) ++(-2,-2) coordinate (x1);
					%%Place nodes at each point using the foreach construct
					\node[draw,circle,shading=axis,top color=green!1, bottom color=green,shading angle=5] at (2) (n2) {$\sum$};
					\node[above of=n2,above=.5cm] (b) {$1$};
					%%Place the remaining nodes separately
					\node (nx1) at (x1) {$x$};
					%\node (ny)  at (7)  {$y$};
					\path[normal arrow,cyan] (b) -- node[right=.05em,cyan] {$0$} (n2);
					\path[normal arrow,cyan] (n2) --  (sigmoid);
					\path[normal arrow,cyan] (sigmoid) --  (not);
					\path[normal arrow,cyan] (nx1) -- node[above=.5em,cyan] {$-1$} (n2);
					\end{tikzpicture}
			\end{minipage}}
	\end{columns}
}

\frame{{}
	\centering\includegraphics[width=\textwidth,angle=0,origin=c]{img/neuralnet}
	
	{\centering Learning: small change in weights $\to$ small change in output}
}

\frame{{Kolmogorov Superposition Theorem}
	\begin{theorem}[Kolmogorov Superposition Theorem]
		For each $n\geq 2$ there exists a computable function $\psi\colon [0, 1]\to\mathbb{R}$ and computable constants $a,\lambda_{pq}\in\mathbb{R}$, $p = 1,\dots,n$, $q = 0,\dots,2n$ s.t.: every continuous function $f\colon [0,1]^n\to\mathbb{R}$ has a representation as
		\[f(x_1,\dots,x_n)=\sum\limits_{q=0}^{2n} g\left(\sum\limits_{p=1}^n\lambda_{pq}\psi(x_p+qa)\right)\]
		for some continuous function $g\colon[0,1]\to\mathbb{R}$ that is computable from $f$.
	\end{theorem}
	\begin{theorem}[Hecht-Nielsen Theorem]
		The class of functions $f\colon [0, 1]^n\to\mathbb{R}$, implementable by three-layer feed-forward neural networks with (computable) continuous activation functions $g\colon [0, 1]\to\mathbb{R}$ and (computable) weights $\lambda\in\mathbb{R}$, is exactly the class of (computable) continuous functions $f\colon[0,1]^n\to\mathbb{R}$.
	\end{theorem}
}

\frame{{}
	\centering\includegraphics[width=.8\textwidth,angle=0,origin=c]{img/superposition}
}

\frame{{Universal Approximation Theorem}
	\begin{theorem}[Universal Approximation Theorem]
		Let $g$ be a nonconstant, bounded, and increasing continuous function. Let $I_n$ be any compact subset of $\mathbb{R}^n$. The space of continuous functions on $I_n$ is denoted by $C(I_n,\mathbb{R})$. Then, given any function $f\in C(I_n,\mathbb{R})$ and $\varepsilon>0$, there exists an integer $N$, real constants $v_i,b_i\in\mathbb{R}$ and real vectors $\bm{w}_i \in \mathbb{R}^n$, where $i=1,\dots,N$, s.t.
		\[\forall\bm{x}\in I_n\colon |h(\bm{x}) - f(\bm{x})| < \varepsilon\]
		where
		\[h(\bm{x}):=\sum\limits_{i=1}^{N} v_i g\left(\bm{w}_i^\top\bm{x} + b_i\right)\]
		In other words, functions of the form $h(\bm{x})$ are dense in $C(I_n,\mathbb{R})$.
	\end{theorem}
}

\frame{{Representation \& Approximation}
	\begin{itemize}
		\item A feed-forward network with $1$ hidden layer can represent any boolean function, but require exponential hidden units.
		\item A feed-forward network with $2$ hidden layers and (computable) continuous activation functions can represent any (computable) continuous function.
		\item A feed-forward network with a linear output layer and at least $1$ hidden layer and continuous and differentiable activation functions can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error.
		\item A feed-forward network with $2$ hidden layers and continuous and differentiable activation functions can approximate any function.
	\end{itemize}
}

\frame{{Deep Learning}\vspace{-3ex}
	\begin{figure}
		\includegraphics[width=0.6\textwidth]{img/learning}
	\end{figure}\vspace{-4ex}
	\begin{enumerate}
		\item \textcolor{red}{hypothesis space} --- Network Structure --- $\textcolor{blue}{f_\theta}$
		\item \textcolor{red}{the goodness of a function} --- Learning Target ---  \textcolor{blue}{loss function $\ell$}
		\item \textcolor{red}{pick the best function} --- Learn --- \textcolor{blue}{find the network parameters $\theta^*:=\argmin\limits_\theta L(\theta)$ that minimize total cost $L(\theta)$ by gradient decent} \[\textcolor{blue}{\theta\gets\theta-\eta\nabla_\theta L(\theta)}\]		
		\textcolor{blue}{where $L(\theta):=\mathbb{E}_P\left[\ell\left(f_\theta(\bm{a}),t \right)\right]+\lambda\Omega(\theta)$ and $\Omega(\theta)$ is a regularizer.}
	\end{enumerate}
}

\frame{{Deep Learning}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{center}
		\scalebox{0.8}{$\textcolor{darkgreen}{f_\theta\colon\bm{a}^{(0)} \mapsto\underbrace{\bm{g}^{(n)}\Bigg(\underbrace{\cdots\overbrace{\bm{g}^{(2)}\bigg(\overbrace{\bm{w}^{(2)}\underbrace{\bm{g}^{(1)}\Big(\underbrace{\bm{w}^{(1)}\bm{a}^{(0)} +\bm{b}^{(1)}}_{\textcolor{blue}{\bm{z}^{(1)}}}\Big)}_{\textcolor{blue}{\bm{a}^{(1)}}}+\bm{b}^{(2)}}^{\textcolor{blue}{\bm{z}^{(2)}}}\bigg)}^{\textcolor{blue}{\bm{a}^{(2)}}}\cdots}_{\textcolor{blue}{\bm{z}^{(n)}}}\Bigg)}_{\textcolor{blue}{\bm{a}^{(n)}}}}$}
	\end{center}
	\textcolor{darkgreen}{where parameters $\theta:=\left\{\bm{w}^{(i)},\bm{b}^{(i)}\right\}_{i=1}^n$ and activation functions $\bm{g}$}
	\[\textcolor{darkgreen}{\sigma(z)=\frac{1}{1-e^{-z}}\qquad \tanh(z)=\dfrac{e^z-e^{-z}}{e^z+e^{-z}}\qquad\mathit{ReLU}(z)=\max(0,z)}\]
	\begin{tcolorbox}
		\[
		\left.\begin{aligned}
		a_0^{(l)}&:=1\\
		w_{0j}^{(l)}&:=b_j^{(l)}
		\end{aligned}\right\}\implies\left\{
		\begin{aligned}
		z_j^{(l+1)}&:=\sum\limits_i w_{ij}^{(l+1)}a_i^{(l)}\\
		a_j^{(l+1)}&:=g_j^{(l+1)}\left(z_j^{(l+1)}\right)
		\end{aligned}\right.
		\]
	\end{tcolorbox}
}

\frame{{}
	\begin{columns}
		\column{.5\textwidth}
			\centering\includegraphics[width=.88\textwidth,angle=0,origin=c]{img/backpropa}
		\column{.5\textwidth}
			\begin{tcolorbox}[colback=cyan!3,colframe=back!85,title=Backpropagation]
				\[\delta_j^{(l)}:=\frac{\partial L}{\partial z_j^{(l)}}\]
				\[\delta_j^{(l)}={g_j^{(l)}}'\!\left(z_j^{(l)}\right)\sum_k\delta_k^{(l+1)}w_{jk}^{(l+1)}\]
				\[\frac{\partial L}{\partial w_{ij}^{(l)}}=a_i^{(l-1)}\delta_j^{(l)}\]
				\[w_{ij}^{(l)}\gets w_{ij}^{(l)}-\eta\frac{\partial L}{\partial w_{ij}^{(l)}}\]
			\end{tcolorbox}
	\end{columns}
}

\frame{{}
	\begin{columns}
		\column{.4\textwidth}
			\begin{itemize}
				\item network structure?
				\item how many layers?
				\item how many units per layer?
				\item loss function?
				\item regularization?
				\item weight decay?
				\item learning rate?
				\item activation function?
				\item early stopping?
				\item dropout?
				\item mini-batch?
				\item momentum?
				\item \dots
			\end{itemize}
		\column{.63\textwidth}
			\centering\includegraphics[width=.9\textwidth,angle=0,origin=c]{img/cnn.pdf}
			\[a_{ij}^{(l+1)}=g_{ij}^{(l+1)}\left(\sum_{m=0}^{k-1} \sum_{n=0}^{k-1} w_{m,n} a_{i+m,j+n}^{(l)}+b\right)\]
			\centering\includegraphics[width=.9\textwidth,angle=0,origin=c]{img/lstm0}
	\end{columns}
}

\frame{{Key Properties of CNNs}
\begin{table}[H]
\begin{tabu}{|c|c|c|c|c|c|}
\hline
1 & 0 & 0 & 0 & 0 & 1 \\
\hline
0 & 1 & 0 & 0 & 1 & 0 \\
\hline
0 & 0 & 1 & 1 & 0 & 0 \\
\hline
1 & 0 & 0 & 0 & 1 & 0 \\
\hline
0 & 1 & 0 & 0 & 1 & 0 \\
\hline
0 & 0 & 1 & 0 & 1 & 0 \\
\hline
\end{tabu}
$*$
\begin{tabu}{|c|c|c|}
\hline
1  & -1 & -1 \\
\hline
-1 & 1  & -1 \\
\hline
-1 & -1 & 1  \\
\hline
\end{tabu}
$=$
\begin{tabu}{|c|c|c|c|}
\hline
3  & -1 & -3 & -1 \\
\hline
-3 & 1  & 0  & -3 \\
\hline
-3 & -3 & 0  & 1  \\
\hline
3  & -2 & -2 & -1 \\
\hline
\end{tabu}\caption{Convolution (stride $1$)}
\end{table}
Take advantage of the structure of the data!
\begin{itemize}
	\item Convolutional Filters \textbf{(Translation invariance)}
	\item Multiple layers \textbf{(Compositionality)}
	\item Filters localized in space \textbf{(Locality)}
	\item Weight sharing \textbf{(Self-similarity)}
\end{itemize}
}

\frame{{DQN}
	\[Q^\pi(s,a)=\mathbb{E}\left[\sum\limits_{t=0}^\infty\gamma^tr_t\,\middle|\, s,a\right]\]
	\[\max_\pi Q^\pi(s,a)=:Q^*(s,a)=\mathbb{E}_{s'}\left[r+\gamma\max_{a'}Q^*(s',a')\,\middle|\, s,a\right]\]
	\[Q_{t+1}(s,a)=\mathbb{E}_{s'}\left[r+\gamma\max_{a'}Q_t(s',a')\,\middle|\, s,a\right]\]
	\[\textcolor{blue}{Q_t\xrightarrow{t\to\infty}Q^*}\]
	\[Q_{t+1}(s,a)=Q_t(s,a)+\alpha\left(r+\gamma\max\limits_{a'} Q_t(s',a')-Q_t(s,a)\right)\]
	\[\textcolor{red}{Q(s,a;\theta)\approx Q^*(s,a)}\]
	\[L(\theta):=\mathbb{E}_{s,a,r,s'}\left[\Big(r+\gamma\max\limits_{a'} Q\left(s',a';\theta^-\right)-Q(s,a;\theta)\Big)^2\right]\tag{DQN}\]
}

\frame{{DQN}
	\[L(\theta):=\mathbb{E}_{s,a,r,s'}\left[\Big(r+\gamma Q\big(s',\argmax\limits_{a'} Q(s',a';\theta);\theta^-\big)-Q(s,a;\theta)\Big)^2\right]\tag{Double DQN}\]
	
	\[Q(s,a)=V(s;\theta)+A(s,a;\theta')\tag{Dueling Network}\]
	
	\[\left\{\begin{aligned}
	&\nabla_\theta\log\pi(a_t|s_t;\theta)\left(\sum\limits_{i=0}^{k-1}\gamma^ir_{t+i}+\gamma^k V(s_{t+k};\theta')-V(s_t;\theta')\right)&\text{actor}\\
	&\nabla_{\theta'}\left(\sum\limits_{i=0}^{k-1}\gamma^ir_{t+i}+\gamma^k V(s_{t+k};\theta')-V(s_t;\theta')\right)^2&\text{critic}
	\end{aligned}\right.\tag{AC}
	\]
}

\frame{{AlphaZero}
\resizebox{\textwidth}{!}{
\begin{minipage}{1.3\textwidth}
\xymatrix{*++{current\atop player} \ar@//[r] & *++[F-]{\mbox{policy evaluation}\atop\mbox{\textcolor{blue}{neural network}}} \ar@//[rrr]^{\small \mbox{\!\!\!\!\!\!\!\!\!\!\!position `value'}\atop\mbox{\!\!\!\!\!\!\!\!\!\!\!move `probability'}} &&& *++[F-]{\mbox{policy improvement}\atop\mbox{\textcolor{blue}{neural network}}} \ar@//[r] & *++{improved\atop player} \ar@/^4pc/[lllll]_{\mbox{self-learning/policy iteration}}}
\end{minipage}}
\begin{columns}
\column{.53\textwidth}
	\begin{figure}
		\includegraphics[width=\textwidth]{img/alphazero}
	\end{figure}
\column{.47\textwidth}
	\[(\bm{p},v)=f_\theta(s)\]
	\[\ell=(z-v)^2-\pi^\top\log\bm{p}+c\|\theta\|^2\]
	\begin{block}{Intuition $+$ Calculation}\centering
		DNN $+$ MCTS
	\end{block}
\end{columns}
}

\frame{{GAN --- Generative Adversarial Network}
	\[V(D,G)=\mathbb{E}_{x\sim P_{data}}\left[\log{D(x)}\right]+\mathbb{E}_{z\sim P_{noise}}\left[\log{(1-D(G(z)))}\right]\]
	\[G^*=\argmin\limits_G\max\limits_DV(D,G)\tag{GAN}\]
	\begin{figure}[H]
		\includegraphics[width=.9\textwidth,angle=0,origin=c]{img/gan}
	\end{figure}
}

\frame{{Why ``Deep'' rather than ``Fat''?}	
	\begin{itemize}
		\item Exploiting compositionality gives an exponential gain in representational power.
		\begin{itemize}
			\item Distributed representations: feature learning
			\item Deep architecture: multiple levels of feature learning
		\end{itemize}
		\item Each basic classifier can be trained by little data.
		\begin{itemize}
			\item \textcolor{darkgreen}{deep $\to$ modularization $\to$ less training data?}\\
			With more complex features, the number of parameters in the linear layers may be drastically decreased.
			\item efficiency \& sample complexity
			\item better memory/computation trade-off?
		\end{itemize}
		\item higher-level abstractions $\to$ easier generalization \& transfer
	\end{itemize}
}

\frame{{Minimal Sufficient Statistic}
\begin{definition}[Sufficient Statistic]
Let $Y$ be a parameter indexing a family of probability distributions. Let $X$ be random variable drawn from a probability distribution determined by $Y$. $T(X)$ is a sufficient statistic for $Y$ if $X$ is independent of $Y$ given $T(X)$, i.e., $p(x|t,y)=p(x|t)$.
\end{definition}
\begin{definition}[Minimal Sufficient Statistic]
A sufficient statistic $S(X)$ is minimal if for any sufficient statistic $T(X)$, there exists a function $f$ s.t. $S=f(T)$ almost everywhere w.r.t $X$.
\end{definition}
\begin{theorem}
\begin{itemize}
\item $T$ is sufficient statistics for $Y$ $\iff$ $I(T(X);Y)=I(X;Y)$.
\item $S$ is minimal sufficient statistics for $Y$ $\implies$ $I(X;S(X))\leq I(X;T(X))$.
\end{itemize}
\end{theorem}
}

\frame{{The Information Bottleneck --- Learning is to forget!}
\begin{theorem}
Let $X$ be a sample drawn according to a distribution determined by the random variable $Y$. The set of solutions to
\[\min\limits_T I(X;T)\quad s.t.\quad I(T;Y)=\max\limits_{T^\prime}I(T^\prime;Y)\]
is exactly the set of minimal sufficient statistics for $Y$ based on $X$.
\end{theorem}
Find a random variable $T$ s.t.:
\begin{itemize}
	\item $Y\leftrightarrow X\leftrightarrow T$ form a Markov chain.
	\item $I(X;T)$ is minimized (minimality, \textcolor{red}{complexity} term), while\\ $I(T;Y)$ is maximized (sufficiency, \textcolor{red}{accuracy} term).
\end{itemize}
\[\fbox{$T^*:=\argmin\limits_{T\colon I(T(X);Y)=I(X;Y)}I(X;T(X))$}\] is the \textcolor{red}{Information Bottleneck} between $X$ and $Y$.
}

\frame{{}\vspace{-1ex}
\begin{columns}
\column{.55\textwidth}
\begin{figure}[H]
	\includegraphics[width=\textwidth,angle=0,origin=c]{img/encoder-decoder}
	\includegraphics[width=\textwidth,angle=0,origin=c]{img/bottleneck}
\end{figure}
\column{.21\textwidth}
张三丰：将所见到的剑招忘得半点不剩，才能得其神髓。
\end{columns}
}

\frame{{The Information Bottleneck}
$\min\limits_{p(t|x),p(y|t),p(t)}\Big\{I(X;T)-\beta I(T;Y)\Big\}$ subject to Markov chain $Y\to X\to T$.
\[\mathcal{L}[p(t|x)]:=I(X;T)-\beta I(T;Y)-\sum\limits_x\lambda(x)\sum\limits_t p(t|x)\]
Let
\[\frac{\delta\mathcal{L}}{\delta p(t|x)}=0\]
The solution is
\begin{align*}
p(t|x)&=\frac{p(t)}{Z(x,\beta)}\mathrm{e}^{-\beta D[p(y|x)\|p(y|t)]}\\
p(t)&=\sum\limits_x p(t|x)p(x)\\
p(y|t)&=\sum\limits_x p(y|x)p(x|t)
\end{align*}
}

\frame{{Expressiveness \& Sample Complexity}
	\begin{theorem}
	The hypothesis class of neural networks of depth $T$ and size $\mathcal{O}(T^2)$ contains all functions that can be implemented by a Turing machine within $T$ operations, while having $\mathcal{O}(T^2)$ sample complexity.
	\end{theorem}
}

\frame{{The Ultimate Hypothesis Space}
\begin{itemize}
	\item \textcolor{red}{No Free Lunch:} Sample complexity is exponentially large (w.r.t. the input dimension) if the hypothesis class is all possible functions.
	\item \textcolor{red}{Shallow learning (SVM, Boosting):} Hypothesis class is linear functions over manually determined features --- strong prior knowledge.
	\item \textcolor{red}{Deep learning:} Hypothesis class is all functions implemented by determining the weights of a given artificial neural network.
\end{itemize}\vspace{-1ex}
	\begin{figure}[H]
		\includegraphics[width=.7\textwidth,angle=0,origin=c]{img/priornfl}\vspace{-2ex}\caption{Prior vs Universality}
	\end{figure}
	\[\fbox{\textbf{\textcolor{red}{Prior --- a necessary good or a necessary evil?}}}\]
}

%--------------------------%
\section{General Reinforcement Learning}
%--------------------------%

\frame{{UAI}
	\begin{enumerate}
		\item Solve intelligence
		\item Use it to solve everything else
	\end{enumerate}
	\begin{itemize}
		\item learn automatically from raw inputs --- not pre-programmed.
		\item same algorithm, different tasks.
	\end{itemize}
}

\frame{{UAI}
	\begin{columns}
		\column{.6\textwidth}\vspace{-4ex}
			\begin{table}
				\centering
				\begin{tabu}{c|c}
					\hline
					\large \textcolor{red}{(Deep) RL} &\large\textcolor{red}{General RL}\\
					\hline
					state space &history\\
					\hline
					ergodic &not ergodic\\
					\hline
					fully observable &partially observable\\
					\hline
					$\varepsilon$-exploration works &$\varepsilon$-exploration fails\\
					\hline
					\Large\textcolor{blue}{MDP/DQN} &\Large\textcolor{blue}{AIXI}\\
					\hline
				\end{tabu}\caption{(Deep) RL vs General RL}
			\end{table}
		\column{.4\textwidth}\hspace{-0.4\textwidth}
			\resizebox{.8\textwidth}{!}{	\begin{minipage}{\textwidth}
					\begin{center}
						\unitlength=2ex
						\linethickness{0.4pt}
						\begin{picture}(32,20)(0,0)
						\thicklines
						% boxes
						\put(16,17.5){\oval(5,3)\makebox(0,0)[cc]{UAI}}
						\put(16,12){\oval(7,2)\makebox(0,0)[cc]{Framework}}
						\put(11,6){\oval(5,2)\makebox(0,0)[cc]{\textcolor{red}{Learning}}}
						\put(16,6){\oval(4,2)\makebox(0,0)[cc]{Utility}}
						\put(21,6){\oval(5,2)\makebox(0,0)[cc]{\textcolor{red}{Planning}}}
						\thinlines
						\put(16,13){\line(0,4){3}}
						\put(11,7){\line(3,4){3}}
						\put(16,7){\line(0,4){4}}
						\put(21,7){\line(-3,4){3}}
						\end{picture}
					\end{center}
			\end{minipage}}
	\end{columns}\vspace{-5ex}
	\begin{center}
		\boxed{
			\begin{minipage}{60ex}\centering
				\begin{large}
					\begin{tabu}{ccc}
						\textcolor{cyan}{Decision Theory} & $=$ & \textcolor{cyan}{Probability $+$ Utility Theory} \\
						$+$ & & $+$ \\
						\textcolor{red}{Universal Induction} & $=$ & \textcolor{red}{Occam $+$ Bayes $+$ Turing} \\
						$\scriptstyle||$ & & $\scriptstyle||$ \\
						\multicolumn{3}{c}{\textcolor{blue}{Universal Artificial Intelligence without Parameters}} \\
					\end{tabu}
				\end{large}
		\end{minipage}}
	\end{center}
}

\frame{{Marcus Hutter}
			\begin{figure}
				\subfigure{\includegraphics[height=.4\textwidth,angle=0,origin=c]{img/hutter}}
				\subfigure{\includegraphics[height=.4\textwidth,angle=0,origin=c]{img/uai.jpeg}}
			\end{figure}
	Jan Leike, Tor Lattimore, Shane Legg, Joel Veness, Laurent Orseau, Mark Ring, Peter Sunehag, Mayank Daswani, Tom Everitt, Jan Poland, Daniel Filan, William Uther, Kee Siong Ng, David Silver, J\"urgen Schmidhuber, Alexey Potapov, Bill Hibbard, Daniil Ryabko, Alexey Chernov\dots
}

\frame{{}
	\begin{figure}[H]
		\begin{center}
			\resizebox{.95\textwidth}{!}{\begin{minipage}{70ex}
						\xymatrix @C=1pc{&&&&&&&\\
							&*++[o][F.]{\txt{Sensors}}\ar[d]&&&\\
							&*++[F]{\txt{What the world\\ is like now}}\ar[d]\\
							&*++[F]{\txt{What it will be like\\ if I do action $a$}}\ar[d]
							&*+++[o][F--]{\txt{\textit{\textbf{\emph{\underline{\large{\textcolor{blue}{Agent}}}}}}}}
							&&&&*+++[F=]{\txt{\textit{\textbf{\emph{\underline{\Huge\textcolor{red}{{Environment}}}}}}}}
							\ar@2{->}_*++[o]{perception} "4,7";"2,2"
							\ar@2{<-}^*++[o]{action} "4,7";"7,2"\\
							&*++[F]{\txt{How happy I will be\\in such a state}}\ar[d]\\
							&*++[F]{\txt{What action I\\ should do now}}\ar[d]\\
							&*++[o][F.]{\txt{Effectors}}&&&&\\
							&&&&&&&&
							\save "1,1"."8,4"*[F=]\frm{}\restore}
			\end{minipage}}
		\end{center}
	\end{figure}
}

\frame{{Computationalism}
	\begin{figure}[H]
				\hspace{-17pt}
					\begin{center}
						\unitlength=1.1mm
						\large
						\begin{picture}(106,47)
						\thicklines
						\put(1,41){\framebox(16,6)[cc]{\textcolor{blue}{$e_1$}}}
						\put(17,41){\framebox(16,6)[cc]{\textcolor{blue}{$e_2$}}}
						\put(33,41){\framebox(16,6)[cc]{\textcolor{blue}{$e_3$}}}
						\put(49,41){\framebox(16,6)[cc]{\textcolor{blue}{$e_4$}}}
						\put(65,41){\framebox(16,6)[cc]{\textcolor{blue}{$e_5$}}}
						\put(81,41){\framebox(16,6)[cc]{\textcolor{blue}{$e_6$}}}
						\put(97,47){\line(1,0){9}}\put(97,41){\line(1,0){9}}\put(102,44){\makebox(0,0)[cc]{\ldots}}
						\put( 1,1){\framebox(16,6)[cc]{\textcolor{red}{$a_1$}}}
						\put(17,1){\framebox(16,6)[cc]{\textcolor{red}{$a_2$}}}
						\put(33,1){\framebox(16,6)[cc]{\textcolor{red}{$a_3$}}}
						\put(49,1){\framebox(16,6)[cc]{\textcolor{red}{$a_4$}}}
						\put(65,1){\framebox(16,6)[cc]{\textcolor{red}{$a_5$}}}
						\put(81,1){\framebox(16,6)[cc]{\textcolor{red}{$a_6$}}}
						\put(97,7){\line(1,0){9}}\put(97,1){\line(1,0){9}}\put(102,4){\makebox(0,0)[cc]{\ldots}}
						%
						\textcolor{red}{\put(1,21){\framebox(16,6)[cc]{\text{work}}}}
						\thicklines
						\put(17,17){\textcolor{red}{\framebox(20,14)[cc]{$\displaystyle{\begin{array}{c}
										\textcolor{red}{\textbf{Agent}}\atop
										\textcolor{red}{\bm{p}}
										\end{array}}$}}}
						%\thinlines
						\textcolor{red}{\put(37,27){\line(1,0){14}}}
						\textcolor{red}{\put(36,21){\line(1,0){14}}}
						\textcolor{red}{\put(39,24){\makebox(0,0)[lc]{\text{tape}\ldots}}}
						%
						\textcolor{blue}{\put(56,21){\framebox(16,6)[cc]{\text{work}}}}
						\thicklines
						\put(72,17){\textcolor{blue}{\framebox(20,14)[cc]{$\displaystyle{\begin{array}{l}
										\textcolor{blue}{\textbf{Environment}}\atop
										\textcolor{blue}{\bm{q}}
										\end{array}}$}}}
						%\thinlines
						\textcolor{blue}{\put(92,27){\line(1,0){14}}}
						\textcolor{blue}{\put(91,21){\line(1,0){14}}}
						\textcolor{blue}{\put(94,24){\makebox(0,0)[lc]{\text{tape}\ldots}}}
						%
						\normalcolor
						\thicklines
						\textcolor{red}{\put(46,41){\vector(-2,-1){20}}}
						\textcolor{blue}{\put(81,31){\vector(-2,1){20}}}
						\textcolor{blue}{\put(47,7){\vector(3,1){30}}}
						\textcolor{red}{\put(17,17){\vector(3,-1){30}}}
						\end{picture}
					\end{center}
	\end{figure}
}

\frame{{Agent \& Environment}
\begin{definition}[Agent \& Environment]
	\begin{columns}
		\column{.75\textwidth}
				\begin{itemize}
					\item finite set of possible actions $\mathcal{A}$ and perceptions $\mathcal{E}$;
					\item prior knowledge $w\in\Delta\mathcal{M}$ of the environments $\mathcal{M}$;
					\item utility function $u\colon({\mathcal{A}} \times {\mathcal{E}})^*\to[0,1]$;
					\item discount factor $\gamma\in[0,1]$;
				\end{itemize}
				\[\pi\colon(\mathcal{A}\times\mathcal{E})^*\to\Delta\mathcal{A}\]
				\[\mu\colon (\mathcal{A}\times\mathcal{E})^*\times\mathcal{A}\to\Delta\mathcal{E}\]
				\[{}_\mu^\pi(\ae_{<t}):=\prod\limits_{i=1}^{t-1}\pi(a_i|\ae_{<i})\mu(e_i|\ae_{<i}a_i)\]
		\column{.25\textwidth}\hspace{-11ex}
			\resizebox{\textwidth}{!}{
				\begin{minipage}{\textwidth}
					\begin{figure}[H]
						\tikzstyle{mcirc} = [draw, circle, fill=none]
						\tikzstyle{line} = [draw, -latex',font=\scriptsize]
						\begin{tikzpicture}
						\node [mcirc] (s0) at (0,0) {$\text{agent} \atop \pi$};
						\node [mcirc] (s1) at (3,0) {$\text{environment} \atop \mu$};
						\path [line] (s0)  edge [bend right=50]  node [midway, below] {action} (s1);
						\path [line] (s0)  edge [bend right=50]  node [midway, above] {$a$} (s1);
						\path [line] (s1)  edge [bend right=50] node [midway, above] {perception} (s0);
						\path [line] (s1)  edge [bend right=50] node [midway, below] {$e$} (s0);
						\end{tikzpicture}
					\end{figure}
			\end{minipage}}
	\end{columns}
\end{definition}
}

\frame{{Value Function}
	\[r_n:=u(\ae_{1:n})\]
	\[V_\mu^\pi(\ae_{<t}):=\mathbb{E}_\mu^\pi\left[\sum\limits_{k=0}^\infty\gamma^k r_{t+k}\,\middle|\, \ae_{<t}\right]\]
	\textcolor{red}{Bellman equation}:
	\begin{align*}
	V_\mu^\pi(\ae_{<k})&=\sum\limits_{a_k\in\mathcal{A}}\pi(a_k|\ae_{<k})\sum\limits_{e_k\in\mathcal{E}}\mu(e_k|\ae_{<k}a_k)\left[r_k+\gamma V_\mu^\pi(\ae_{1:k})\right]\tag{\text{\tiny{recursive}}}\\
	&\textcolor{red}{=}\sum\limits_{\ae_{k:m}}{}_\mu^\pi(\ae_{k:m}|\ae_{<k})\left[\sum\limits_{i=k}^m \gamma^{i-k} r_i+\gamma^{m-k+1} V_\mu^\pi(\ae_{1:m})\right]\tag{\text{\tiny{iterative}}}
	\end{align*}
	\[V_\mu^\pi(\ae_{<k})=\lim\limits_{m\to\infty}\sum\limits_{\ae_{k:m}}{}_\mu^\pi(\ae_{k:m}|\ae_{<k})\left[\sum\limits_{i=k}^m\gamma^{i-k}r_i\right]\]
}

\frame{{Optimal Value/Policy}
	\[V_\mu^*:=\max\limits_\pi V_\mu^\pi\]
	\begin{align*}
	V_\mu^*(\ae_{<k})&=\lim\limits_{m\to\infty}\max\limits_{a_k\in\mathcal{A}}\sum\limits_{e_k\in\mathcal{E}}\cdots \max\limits_{a_m\in\mathcal{A}}\sum\limits_{e_m\in\mathcal{E}}\sum\limits_{i=k}^m\gamma^{i-k}r_i\prod\limits_{j=k}^i \mu(e_j|\ae_{<j}a_j)\\
	&\textcolor{red}{=}\lim\limits_{m\to\infty}\max\limits_{a_k\in\mathcal{A}}\sum\limits_{e_k\in\mathcal{E}}\cdots \max\limits_{a_m\in\mathcal{A}}\sum\limits_{e_m\in\mathcal{E}}\left[\sum\limits_{i=k}^m\gamma^{i-k}r_i\right]\mu(e_{k:m}|\ae_{<k}a_{k:m})
	\end{align*}
	
	\[\pi_\mu^*:=\argmax\limits_\pi V_\mu^\pi\]
}

\frame{{Bayesian Mixture \& Belief Update}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\[\xi(e_{<n}|a_{<n}):=\sum\limits_{\nu \in \mathcal{M}} w_\nu \nu(e_{<n}|a_{<n})\]
	\[w_{\ae_{<n}}^\nu:=\dfrac{w_\nu \nu(e_{<n} | a_{<n})}{\xi(e_{<n}|a_{<n})}\]
	\[
	\sum\limits_{k=1}^\infty \sum\limits_{e_{1:k}} \mu(e_{<k} | a_{<k}) \Big(\mu(e_k|\ae_{<k}a_k) - \xi(e_k|\ae_{<k}a_k ) \Big)^2\leq\min\limits_{\nu\in\mathcal{M}}\Big\{-\ln w_\nu+D(\mu\|\nu)\Big\}
	\]
\begin{center}
What probability should an observer assign to future experiences if she is told that she will be simulated on a computer?
\end{center}
}

\frame{{Intelligence Measure \& AIXI}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{center}
		\huge What is `\textcolor{red}{intelligence}'?
	\end{center}
	\begin{center}
		A Blind Man in a Dark Room Looking for a Black Cat That Is Not There?
	\end{center}
	\begin{quote}\it
		Intelligence measures an agent's ability to achieve goals in a wide range of environments.\par
		\hfill --- {\sl Shane Legg and Marcus Hutter}
	\end{quote}
	\begin{center}
		\boxed{
			\begin{minipage}{60ex}
				\begin{align*}
				\Upsilon(\pi)&:=\sum\limits_{\nu\in\mathcal{M}}w_\nu V_\nu^\pi(\epsilon)=V_\xi^\pi(\epsilon)\tag{\textcolor{red}{Intelligence Measure}}\\
				\textcolor{red}{\mathrm{AIXI}}&:=\argmax\limits_\pi\Upsilon(\pi)=\pi_\xi^*
				\end{align*}
		\end{minipage}}
	\end{center}
	\[V_\xi^\pi(h)=\sum\limits_{\nu\in\mathcal{M}}w_h^\nu V_\nu^\pi(h)\]
	\[\textcolor{red}{w_\nu:=2^{-K(\nu)}}\implies\xi(e_{1:m}|a_{1:m})\eqm M(e_{1:m}|a_{1:m}):=\textcolor{blue}{\sum\limits_{p:U(p,a_{1:m})=e_{1:m}}\!\!2^{-\ell(p)}}\]
}

\frame{{AIXI}\vspace{-3ex}
\[a_k^*\;:=\;\argmax_{a_k}\sum_{e_k}\dots \max_{a_m}\sum_{e_m}\left[\sum\limits_{i=k}^m \gamma^{i-k}r_i\right]\!\sum\limits_{p:U(p,{a_{1:m}})=e_{1:m}}\!\!\!\!\!\!\!\!\!\!\! 2^{-\ell(p)}\tag{AIXI}\]
	\begin{figure}[!htb]\vspace{-1ex}
		\begin{center}
			\resizebox{\textwidth}{!}{
				\boxed{
					\begin{minipage}{75ex}
						\begin{center}
							\small
							\unitlength=1mm\vspace{-4ex}
							\begin{picture}(145,60)(10,10)
							\thicklines
							\textcolor{blue}{
								\put(50,60){\circle*{1.5}}
								\put(50,60){\line(-1,-1){20}}
								\put(39,47){\makebox(0,0)[lc]{$a_k\!=0$}}
								\put(50,60){\line(1,-1){20}}
								\put(61,47){\makebox(0,0)[rc]{$a_k\!=1$}}
								\put(50,55){\makebox(0,0)[ct]{$\underbrace{\;\textcolor{cyan}{max}\;}$}}
								\put(55,59){\makebox(0,0)[lc]{$\scriptstyle V_\mu^*(\ae_{<k})=\max\limits_{a_k} Q_\mu^*(\ae_{<k}a_k)$}}
								% observation $e_k$
							}\textcolor{red}{
								\put(30,40){\circle*{1}}
								\put(30,40){\line(-1,-2){10}}
								\put(27,27){\makebox(0,0)[lc]{$\scriptstyle e_k=?$}}
								\put(22,23){\makebox(0,0)[lc]{$\scriptstyle u(\ae_{1:k})=?$}}
								\put(30,40){\line(1,-2){10}}
								\put(30,35){\makebox(0,0)[ct]{$\underbrace{\textcolor{cyan}{\mathbb{E}}}$}}
								\put(70,40){\circle*{1}}
								\put(70,40){\line(-1,-2){10}}
								\put(70,40){\line(1,-2){10}}
								\put(73,27){\makebox(0,0)[rc]{$\scriptstyle e_k=?$}}
								\put(77.5,23){\makebox(0,0)[rc]{$\scriptstyle  u(\ae_{1:k})=?$}}
								\put(70,35){\makebox(0,0)[ct]{$\underbrace{\textcolor{cyan}{\mathbb{E}}}$}}
								\put(75,39){\makebox(0,0)[lc]{$\scriptstyle Q_\mu^*(\ae_{<k}a_k)=\sum\limits_{e_k}\mu(e_k|\ae_{<k}a_k)\left[r_k+\gamma V_\mu^*(\ae_{1:k})\right]$}}
								% action $a_{k+1}$
								\normalcolor}\textcolor{darkgreen}{
								\thinlines
								\put(20,20){\circle*{0.8}}
								\put(20,20){\line(-1,-2){5}}
								\put(20,20){\line(1,-2){5}}
								\put(20,14){\makebox(0,0)[ct]{${{\scriptscriptstyle \textcolor{cyan}{max}}\atop \smile}$}}
								\put(30,15){\makebox(0,0)[cc]{$\scriptstyle a_{k+1}$}}
								\put(40,20){\circle*{0.8}}
								\put(40,20){\line(-1,-2){5}}
								\put(40,20){\line(1,-2){5}}
								\put(40,14){\makebox(0,0)[ct]{${{\scriptscriptstyle \textcolor{cyan}{max}}\atop \smile}$}}
								\put(50,15){\makebox(0,0)[cc]{$\scriptstyle a_{k+1}$}}
								\put(60,20){\circle*{0.8}}
								\put(60,20){\line(-1,-2){5}}
								\put(60,20){\line(1,-2){5}}
								\put(60,14){\makebox(0,0)[ct]{${{\scriptscriptstyle \textcolor{cyan}{max}}\atop \smile}$}}
								\put(70,15){\makebox(0,0)[cc]{$\scriptstyle a_{k+1}$}}
								\put(80,20){\circle*{0.8}}
								\put(80,20){\line(-1,-2){5}}
								\put(80,20){\line(1,-2){5}}
								\put(80,14){\makebox(0,0)[ct]{${{\scriptscriptstyle \textcolor{cyan}{max}}\atop \smile}$}}
								\put(85,19){\makebox(0,0)[lc]{\scriptsize $\displaystyle V_\mu^*(\ae_{1:k})=\max\limits_{a_{k+1}}Q_\mu^*(\ae_{1:k}a_{k+1})$}}
								\put(15,8){\makebox(0,0)[cc]{$\vdots$}}
								\put(25,8){\makebox(0,0)[cc]{$\vdots$}}
								\put(35,8){\makebox(0,0)[cc]{$\vdots$}}
								\put(45,8){\makebox(0,0)[cc]{$\vdots$}}
								\put(55,8){\makebox(0,0)[cc]{$\vdots$}}
								\put(65,8){\makebox(0,0)[cc]{$\vdots$}}
								\put(75,8){\makebox(0,0)[cc]{$\vdots$}}
								\put(85,8){\makebox(0,0)[cc]{$\vdots$}}}
							\end{picture}\vspace{1ex}\caption{\emph{ExpectiMax}}
						\end{center}
			\end{minipage}}}
		\end{center}
	\end{figure}
}

\frame{{AIXI}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/aixi-environment}
		\end{center}
	\end{figure}
}

\frame{{RL vs GRL}
	\begin{itemize}
		\item If $\mu$ is a completely observable MDP, $V_\mu^\pi$ reduces to the recursive Bellman equation.
		\item In a finite MDP, with a geometric discounting function, we can plan ahead by value iteration.
		\item According to Banach's fixpoint theorem, value iteration converges to the value of the optimal policy.
		\item What about GRL?
	\end{itemize}
}

\frame{{}\vspace{-2ex}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\[\text{discount } \gamma\colon\mathbb{N}^2\to[0,1]\;\;\text{and utility } u\colon(\mathcal{A}\times\mathcal{E})^*\to[0,1]\]
	\[V_t^{\pi\mu}(h_{<k}):=\mathbb{E}_\mu^\pi\left[\sum\limits_{i=k}^\infty \gamma_t^i u(h_{1:i})\,\middle|\, h_{<k}\right]\]
	\begin{tcolorbox}[title=Assumption]
		\[
		\forall t\in\mathbb{N}^+\colon \lim\limits_{m\to\infty}\sup\limits_\pi\sum\limits_{h_{<m}}
		V_t^{\pi\mu}(h_{<m}){}_\mu^\pi(h_{<m})=0
		\]
	\end{tcolorbox}
	\begin{theorem}[Extreme Value Theorem]
		If $K$ is compact and $f\colon K\to\mathbb{R}$ is continuous, then $f$ is bounded and there exist $p,q\in K$ s.t. $f(p)=\sup_{x\in K}f(x)$ and $f(q)=\inf_{x\in K}f(x)$.
	\end{theorem}\vspace{-2ex}
	\[\left\langle\Pi:=\mathcal{A}^{(\mathcal{A}\times\mathcal{E})^*}, D(\pi,\pi'):=\mathrm{e}^{-\min\left\{n\colon\exists h_{<n}\left(\pi(h_{<n})\neq\pi'(h_{<n})\right)\right\}}\right\rangle\]\vspace{-2ex}
	\begin{center}
		\fbox{\textcolor{red}{$V_\mu^\pi(h)\colon\Pi\to\mathbb{R}$ is continuous on the compact metric space $\left\langle\Pi,D\right\rangle$.}}
	\end{center}
	\[\pi_t^\mu:=\argmax\limits_\pi V_t^{\pi\mu}\qquad \pi^\mu(h_{<t}):=\pi_t^\mu(h_{<t})\]
}

\frame{{Deterministic vs Stochastic}
	If \[\mu(e_{<t}|a_{<t})=\sum\limits_{p:U(p,a_{<t})=e_{<t}}\mu(p)\]
	then $\mu$ can be interpreted in \emph{two ways}:
	\begin{itemize}
		\item either the true environment is \textbf{deterministic}, but we only have \textbf{subjective belief} of which environment being the true environment; or
		\item the environment itself behaves \textbf{stochastically} defined by $\mu$.
	\end{itemize}
}

\frame{{Intelligence vs Game}\vspace{-1ex}
	\begin{table}[H]
		\begin{center}
			\begin{minipage}{70ex}
					\centering\begin{tabu}{c|c|c}
					\hline
						&Game in $\mathcal{M}_D$ &Game in $\mathcal{M}_U$\\
						\hline
						Ex Post Equilibrium &\textcolor{gray}{Deterministic}   &\textcolor{red}{$\pi_\mu^*$}(recursive/iterative)\\
						\hline
						Bayesian-Nash Equilibrium & \textcolor{red}{$\pi_\mu^*$}(\textcolor{blue}{functional}) &\colorbox{darkgreen}{\textcolor{white}{$\pi_\xi^*$}}(\textcolor{blue}{functional})\\
					\hline
					\end{tabu}
			\end{minipage}
		\end{center}
	\end{table}
	\begin{center}
		\begin{minipage}{.87\textwidth}
			\begin{itemize}
				\item Ex post expected utility $V_t^{\pi\mu}$\hfill \textcolor{red}{$V_\mu^\pi$}
				\item Ex interim expected utility (\textcolor{blue}{Intelligence Measure}) $V_t^{\pi\xi}$\hfill \textcolor{red}{$V_\xi^\pi$}
				\item Ex post equilibrium $\pi_t^\mu$\hfill \textcolor{red}{$\pi_\mu^*$}
				\item Bayesian-Nash equilibrium $\pi_t^\xi$\hfill \textcolor{red}{$\pi_\xi^*$}
				\item Perfect Bayesian-Nash equilibrium $\pi^\xi$
			\end{itemize}
		\end{minipage}
	\end{center}
	\begin{center}
		\textcolor{darkgreen}{\Large Intelligence is an Equilibrium,\\
			We just have to Identify the Game.}
	\end{center}
	\[\text{Intelligence}=\overbrace{\text{Induction}+\text{Action}}^{\textbf{efficiently}}\]
}

\frame{{AIXI}
	\begin{itemize}
		\item Intelligence measure: valid, informative, wide range, general, dynamic, unbiased, fundamental, formal, objective, fully defined, universal\textcolor{red}{?}
		\item AIXI is the most intelligent environmental independent, i.e. universally optimal, agent possible\textcolor{red}{?}
		\item Applications: Sequence Prediction, Games, Optimization, Supervised Learning, Classification\dots
		\item \textcolor{red}{AIXI is not limit computable, thus can't be approximated using finite computation. However there are limit computable $\varepsilon$-optimal approximations to AIXI.}
		\item \textcolor{red}{There are no known nontrivial and non-subjective optimality results for AIXI. General reinforcement learning is difficult even when disregarding computational costs.}
	\end{itemize}
}

\frame{{AIXI Depends on UTM/Prior! --- Dogmatic Prior}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{center}
	\includegraphics[width=.7\textwidth]{img/dogmatic-prior}
	\end{center}
	Dogmatic prior assigns high probability that the agent goes to hell if it does not follow one particular dogma $\pi$. As long as the policy $\pi$ yields some rewards, the prior says that exploration would be too costly and AIXI does not dare to explore.
}

\frame{{Dogmatic Prior}
	\begin{theorem}[Dogmatic Prior]
		Let $\pi$ be any computable deterministic policy, let $\xi$ be any Bayesian mixture over $\mathcal{M}_{LSC}$.	For $\varepsilon>0$, there is a Bayesian mixture $\xi'$ s.t. for any history $h_{<t}$ consistent with $\pi$ and for which $V_\xi^\pi(h_{<t})>\varepsilon$, the action $\pi(h_{<t})$ is the unique $\xi'$-optimal action.
	\end{theorem}
	\begin{theorem}[AIXI Emulates Computable Policies]
		Let $\varepsilon> 0$ and let $\pi$ be any
		computable policy. There is a Bayesian mixture $\xi'$ s.t. for any $\xi'$-optimal policy $\pi_{\xi'}^*$ and for any environment $\nu$,
		\[\Big|V_\nu^{\pi_{\xi'}^*}(\epsilon)-V_\nu^\pi(\epsilon)\Big|<\varepsilon\]
	\end{theorem}
}

\frame{{AIXI Depends on UTM/Prior!}
	\[\overline{\Upsilon}_\xi:=\sup\limits_\pi\Upsilon_\xi(\pi)=\sup\limits_\pi V_\xi^\pi(\epsilon)=V_\xi^{\pi_\xi^*}(\epsilon)=\Upsilon_\xi(\pi_\xi^*)\]
	\begin{figure}[t]
		\begin{center}
			\begin{tikzpicture}
			\filldraw[purple] (1.2, .05) -- (8.7, .05) -- (8.7, -.05) -- (1.2, -.05);
			\foreach \x in {2.4, 2.6, 3.5, 4.7, 6.2} {
				%\filldraw[white] (\x, .05) -- ({\x+.4}, .05) -- ({\x+.4}, -.05) -- (\x, -.05);
			}
			\draw (0,0) to (10, 0);
			\draw (0, -.2) to (0, .2) node[above] {$0$};
			\draw (10, -.2) to (10, .2) node[above] {$1$};
			\draw (4.2, .5) to (4.2, .5) node[above] {\texttt{random}};
			\draw (4.2, 0) to (4.2, 0) node[above] {$\downarrow$};
			\draw (4.2, -.1) to (4.2, -.1) node[below] {\qquad\qquad\textcolor{purple}{\texttt{image of}\; $\Upsilon$}};
			\draw (8.7, .1) to (8.7, -.1) node[below] {$\overline\Upsilon_\xi$};
			\draw (8.7, .5) to (8.7, .5) node[above] {$\pi_\xi^*$};
			\draw (8.7, 0) to (8.7, 0) node[above] {$\downarrow$};
			\draw (1.2, .1) to (1.2, -.1) node[below] {$\underline\Upsilon_\xi$};
			\draw (1.3, .5) to (1.3, .5) node[above] {$\pi_{\xi'}^*$};
			\draw (1.3, 0) to (1.3, 0) node[above] {$\downarrow$};
			\end{tikzpicture}
		\end{center}
	\end{figure}
	\centering{Computable policies are dense in $\left[\underline\Upsilon_\xi,\overline\Upsilon_\xi\right]$. \\
		AIXI emulates computable policies.\\
		\Large AIXI can be arbitrarily stupid!}\\
	The devil imitates God. --- \underline{orthogonality}!
	\begin{center}
		\begin{minipage}{.65\textwidth}
			\begin{itemize}
				\item Prior problem in Universal Induction \hfill $\textcolor{red}{\boxed{\checkmark}}$
				\item Prior problem in Universal Intelligence \hfill $\textcolor{red}{\Huge\boxed{\bm{!}}}$
			\end{itemize}
		\end{minipage}
	\end{center}
}

\frame{{Stupid AIXI}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{theorem}[Computable Policies are Dense]
		The set $\big\{\Upsilon_\xi(\pi)\colon \pi \text{ is a computable policy}\big\}$ is dense in $\left[\underline\Upsilon_\xi,\overline\Upsilon_\xi\right]$.
	\end{theorem}
	\begin{theorem}[Some AIXIs are Stupid]
		For any Bayesian mixture  $\xi$ over $\mathcal{M}_{LSC}$ and every $\varepsilon> 0$, there is a Bayesian mixture $\xi'$ s.t. $\Upsilon_\xi(\pi_{\xi'}^*)<\underline{\Upsilon}_\xi+\varepsilon$.
	\end{theorem}
	\begin{theorem}[AIXI is Stupid for Some $\Upsilon$]
		For any deterministic $\xi$-optimal policy $\pi_\xi^*$ and for every $\varepsilon> 0$ there is a Bayesian mixture $\xi'$ s.t. $\Upsilon_{\xi'}(\pi_\xi^*)\leq\varepsilon$ and $\overline{\Upsilon}_{\xi'}>1-\varepsilon$.
	\end{theorem}
	\begin{theorem}[Computable Policies can be Smart]
		For any computable policy $\pi$ and any $\varepsilon> 0$ there is a Bayesian mixture $\xi'$ s.t. $\Upsilon_{\xi'}(\pi)>\overline{\Upsilon}_{\xi'}-\varepsilon$.
	\end{theorem}
}

\frame{{What is a good optimality criterion?}
	\begin{itemize}
		\item Pareto optimality is \textcolor{red}{\em trivial}. Every policy is Pareto optimal in any $\mathcal{M}\supset\mathcal{M}_{comp}$.
		\item Bayes-optimality is \textcolor{red}{\em subjective}, because two different Bayesians with two different universal priors could view each other's AIXI as a very stupid agent.
	\end{itemize}
}

\frame{{Optimality}
	\begin{itemize}
		\item Pareto optimality
		\[\nexists\pi'\colon \forall\nu\in\mathcal{M}\left[\left( V_\nu^{\pi'}(\epsilon)\geq  V_\nu^\pi(\epsilon)\right)\;\&\;\exists\rho\in\mathcal{M}\left( V_\rho^{\pi'}(\epsilon) >  V_\rho^\pi(\epsilon)\right)\right]\]
		\item Balanced Pareto optimality
		\[\forall\pi'\colon\sum\limits_{\nu\in\mathcal{M}} w_\nu\left( V_\nu^\pi(\epsilon)- V_\nu^{\pi'}(\epsilon)\right)\geq 0\]
		\item Bayes optimality($\iff$ Balanced Pareto optimality)
		\[\forall h_{<t}\colon V_\xi^\pi(h_{<t})=V_\xi^*(h_{<t})\]
		\item Probably approximately correct (PAC)
		\[\forall \varepsilon\delta>0\colon {}_\mu^\pi\left(\forall t\geq m(\varepsilon,\delta)\colon V_\mu^*(h_{<t})-V_\mu^\pi(h_{<t})> \varepsilon\right)<\delta\]
	\end{itemize}
}

\frame{{Optimality? --- Guess how God created the multiverse}
	\begin{columns}[onlytextwidth]
		\column{.33\textwidth}
			\[\text{prior}\left\{\begin{aligned}
			&\text{distribution}\\
			&\text{hypothesis space}\\
			&\text{\textcolor{blue}{prior probability}}\\
			&\text{regularization}
			\end{aligned}\right.\]
			
			\centering{\huge\textcolor{red}{No learning without prior!}}\\
			\textcolor{blue}{no-free-lunch}
		\column{.67\textwidth}
			\[\left.
			\begin{aligned}
			\text{\textcolor{red}{Homogeneous}}\\
			\text{\textcolor{red}{Causality}}\\
			\text{\textcolor{red}{Simplicity}}\\
			\text{\textcolor{red}{Goodness}}\\
			\text{\textcolor{red}{Beauty}}\\
			\text{\textcolor{red}{Perfection}}\\
			\text{\textcolor{red}{Value}}\\
			\text{\textcolor{red}{Regret}}\\
			\text{\textcolor{red}{Unexpectedness}}\\
			\text{\textcolor{red}{Interesting}}\\
			\textcolor{red}{\cdots}
			\end{aligned}
			\right\rbrace\impliedby\text{\textcolor{red}{God!}}
			\]
	\end{columns}
}

\frame{{Genesis --- Zero-Sum Two Person Game}
\begin{columns}
\column{.4\textwidth}
\centering\includegraphics[width=\textwidth]{img/ivsgod}
\column{.3\textwidth}
	\begin{figure}[!htbp]
		\centering\includegraphics[width=\textwidth,angle=0,origin=c]{img/centroid}\vspace{-1ex}
		\caption{\textcolor{darkgreen}{center of mass}\; \textcolor{red}{$\argmax\limits_w\mathbb{E}_w[D(\nu\|\xi)]$}}
	\end{figure}
\end{columns}
	\vspace{-4ex}
	\begin{figure}[!htbp]
		\[\hspace{-1ex}\xrightarrow{\text{message}}\framebox[2cm]{\text{encoder}}\xrightarrow{\mu}\framebox[3cm]{
			$
			\begin{matrix}
			\text{channel}\\
			P(x|\nu)\\
			||\\
			\nu(x)
			\end{matrix}
			$
			$
			\begin{bmatrix}
			\nu_1\\
			\nu_2\\
			\vdots\\
			\nu_n\\
			\end{bmatrix}
			$
		}\xrightarrow{x}\framebox[2cm]{\text{decoder}}\xrightarrow{\text{estimate of message}}\]\vspace{-2ex}\caption{possible worlds as channel --- dominant strategy equilibrium}
	\end{figure}
}

\frame{{Genesis --- Zero-Sum Two Person Game}
\centerline{\textcolor{red}{``Subtle is the Lord, but \textbf{\Large malicious} He is not.''\textbf{?}}}
\centering\includegraphics[width=.32\textwidth]{img/ivsgod}\vspace*{-3ex}
\begin{itemize}
\item God's strategy: $w$
\item Agent's strategy: $\xi$
\item God's utility: expected redundancy $\mathbb{E}_w[D(\mu\|\xi)]$
\item Agent's utility: $-$ expected redundancy / error bound / channel capacity $\max\limits_wE_w[D(\mu\|\xi)]=\max\limits_w I(\mathcal{M};\mathcal{X})$
\item Nash equilibrium: $(w^*,\xi^*)$ \textcolor{red}{dominant strategy equilibrium}
\end{itemize}
\[w^*=\argmax\limits_w I(\mathcal{M};\mathcal{X})\]
\[\xi^*=\argmin\limits_\xi\mathbb{E}_{\textcolor{red}{w^*}}\left[D(\mu\|\xi)\right]\]
\[\fbox{\textcolor{red}{The error bound could be arbitrarily large!}}\]
}

\frame{{Genesis}
	\begin{itemize}
		\item Occam's razor vs Maximum entropy. \[\mathop{minimize}\limits_{w\vDash\left\{
			{\scalebox{.5}{$\begin{aligned}
				& H(w)=C\\
				&\sum\limits_{\nu\in\mathcal{M}}w_\nu=1
				\end{aligned}$}}\right.} \sum\limits_{\nu\in\mathcal{M}}w_\nu K(\nu)\qquad\mathop{maximize}\limits_{w\vDash\left\{
			{\scalebox{.5}{$\begin{aligned}
				&\sum\limits_{\nu\in\mathcal{M}}w_\nu K(\nu)=C\\
				&\sum\limits_{\nu\in\mathcal{M}}w_\nu=1
				\end{aligned}$}}\right.} H(w)\]
		\item Optimal code length for possible worlds --- Solomonoff prior.
		\[\mathop{minimize}\limits_{w\vDash
			\sum\limits_{\nu\in\mathcal{M}}w_\nu=1} \dfrac{\mathbb{E}_w[K]}{H(w)}\]
		\item Maximum expected redundancy/error bound/channel capacity.
		\[\mathop{maximize}\limits_{w\vDash\left\{
			{\scalebox{.5}{$\begin{aligned}
				&H(w)=C\\
				&\sum\limits_{\nu\in\mathcal{M}}w_\nu=1
				\end{aligned}$}}\right.} \mathbb{E}_w\left[D(\nu\|\xi)\right]\qquad\mathop{maximize}\limits_{w\vDash\left\{
			{\scalebox{.5}{$\begin{aligned}
				&\sum\limits_{\nu\in\mathcal{M}}w_\nu K(\nu)=C\\
				&\sum\limits_{\nu\in\mathcal{M}}w_\nu=1
				\end{aligned}$}}\right.} \mathbb{E}_w\left[D(\nu\|\xi)\right]\]
	\end{itemize}
}

\frame{{What is a good optimality criterion?}
\centering\fbox{\large Asymptotic optimality}
		\begin{itemize}
			\item Asymptotic optimality requires only convergence  \textcolor{red}{\em in the limit}.
			\item The agent can be arbitrarily lazy.
			\item \textcolor{red}{AIXI is not asymptotically optimal} because it does not explore enough.
			\item To be asymptotically optimal you have to explore everything.
			\item If you explore more, you're likely to end up in a trap.
			\item Every policy will be asymptotically optimal after falling into the trap.
		\end{itemize}
\begin{figure}[htb]
\subfigure{\includegraphics[height=.2\textwidth]{img/trap1}}
\subfigure{\includegraphics[height=.2\textwidth]{img/trap2}}
\end{figure}
}

\frame{{Asymptotic Optimality}
	\begin{itemize}
		\item strongly asymptotically optimal
		\[{}_\mu^\pi\left(\lim\limits_{t\to\infty} \left[V_\mu^*(h_{<t})-V_\mu^\pi(h_{<t})\right]=0\right)=1\]
		\item weakly asymptotically optimal
		\[{}_\mu^\pi\left(\lim\limits_{n\to\infty} \frac{1}{n}\sum\limits_{t=1}^n\left[V_\mu^*(h_{<t})-V_\mu^\pi(h_{<t})\right]=0\right)=1\]
		\item asymptotically optimal in mean
		\[\lim\limits_{t\to\infty}\mathbb{E}_\mu^\pi\left[ V_\mu^*(h_{<t})-V_\mu^\pi(h_{<t})\right]=0\]
		\item asymptotically optimal in probability (PAC)
		\[\forall \varepsilon>0\colon \lim\limits_{t\to\infty}{}_\mu^\pi\left( V_\mu^*(h_{<t})-V_\mu^\pi(h_{<t})>\varepsilon\right)=0\]
	\end{itemize}
	\[\mbox{strong a.o.} \implies
	\begin{cases}
	\mbox{weak a.o.}\\
	\mbox{a.o. in mean} \iff \mbox{a.o. in probability}
	\end{cases}\]
}

\frame{{AIXI}
	\begin{itemize}
		\item AIXI is not asymptotically optimal.
		\begin{center}
			\resizebox{.9\textwidth}{!}{\fbox{$\forall \mathcal{M}\supset\mathcal{M}_{comp}\exists\mu\in\mathcal{M}\exists t_0\forall t\geq t_0\colon {}_\mu^{\pi_\xi^*}\left(\lim\limits_{t\to\infty} V_\mu^*(h_{<t})-V_\mu^{\pi_\xi^*}(h_{<t})=\frac{1}{2}\right)=1$}}
		\end{center}
		\item AIXI achieves \textcolor{blue}{on-policy value convergence}.
		\begin{center}
			\fbox{${}_\mu^\pi\left(\lim\limits_{t\to\infty} V_\mu^\pi(h_{<t})-V_\xi^\pi(h_{<t})=0\right)=1$}
		\end{center}
	Similarly for MDL \textcolor{darkgreen}{$\argmin\limits_{\nu\in\mathcal{M}}\{-\log v(e_{<t}|a_{<t})+K(\nu)\}$}\\ and universal compression \textcolor{darkgreen}{$2^{-Km(e_{<t}|a_{<t})}$}.
	\end{itemize}
\textbf{Remark:} AIXI asymptotically learns to predict the environment perfectly and with a small total number of errors analogously to Solomonoff induction, but only on policy: AIXI learns to correctly predict the value of its own actions, but generally not the value of counterfactual actions that it does not take.
}

\frame{{Effective Horizon}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\[\Gamma_t:=\sum\limits_{i=t}^\infty\gamma_i\qquad H_t(\varepsilon):=\min\left\{m\colon \dfrac{\Gamma_{t+m}}{\Gamma_t}\leq\varepsilon\right\}\]
	\begin{theorem}
		If there is a nonincreasing computable sequence of positive reals $(\varepsilon_t)_{t\in\mathbb{N}}$ s.t. $\varepsilon_t\xrightarrow{t\to\infty}0$ and $\frac{H_t(\varepsilon_t)}{t\varepsilon_t}\xrightarrow{t\to\infty}0$, then there is a \textcolor{blue}{limit-computable} policy	that is weakly asymptotically optimal in the class of all computable stochastic environments.
	\end{theorem}
	\begin{definition}[$\varepsilon$-Optimal Policy]
		A policy $\pi$ is $\varepsilon$-optimal in environment $\nu$ if 
		\[\forall h\colon V_\nu^*(h)-V_\nu^\pi(h)<\varepsilon\]
	\end{definition}
	\centering $\varepsilon$-optimal BayesExp
}

\frame{{\small Sufficient Condition for Strong Asymptotic Optimality of Bayes}\vspace{-1ex}
	\begin{theorem}[Self-Optimizing Theorem]
		Let $\mu$ be some environment. If there is a policy $\pi$ and a sequence of policies $\pi_1,\pi_2,\dots$ s.t for all $\nu\in\mathcal{M}$
		\begin{equation}
		{}_\mu^\pi\left(\lim\limits_{t\to\infty} V_\nu^*(h_{<t})-V_\nu^{\pi_t}(h_{<t})=0\right)=1\label{sao-condition}
		\end{equation}
		then
		\[{}_\mu^\pi\left(\lim\limits_{t\to\infty} V_\mu^*(h_{<t})-V_\mu^{\pi_\xi^*}(h_{<t})=0\right)=1\]
	\end{theorem}\vspace{-1ex}
	\begin{itemize}
		\item The policies $\pi_1,\pi_2,\dots$ need to converge to the optimal value on the history generated by ${}_\mu^\pi$, not ${}_\nu^{\pi_t}$.
		\item If $\pi=\pi_\xi^*$ and (\ref{sao-condition}) holds for all $\mu\in\mathcal{M}$, then $\pi_\xi^*$ is strongly asymptotically optimal in the class $\mathcal{M}$.
		\item \textcolor{blue}{$\pi_\xi^*$ is strongly asymptotically optimal in the class of ergodic finite-state MDPs if $\forall \varepsilon\colon H_t(\varepsilon)\xrightarrow{t\to\infty}\infty$.}
	\end{itemize}
}

\frame{{For Which Class $\mathcal{M}$ does $V_\mu^{\pi_\xi^*}$ Converge to $V_\mu^*$?}
	\begin{figure}
		\includegraphics[width=.7\textwidth,angle=0,origin=c]{img/self-optimising}
	\end{figure}
}

\frame{{Recoverability}
	An environment $\nu$ is recoverable iff
	\[\lim\limits_{t\to\infty}\sup\limits_\pi\Big|\mathbb{E}_\nu^{\pi_\nu^*}\left[V_\nu^*(h_{<t})\right]-\mathbb{E}_\nu^\pi\left[V_\nu^*(h_{<t})\right]\Big|=0\]
	\textbf{Remark:} Recoverability compares following the worst policy $\pi$ for $t-1$ time steps and then
	switching to the optimal policy $\pi_\nu^*$ to having followed $\pi_\nu^*$ from the beginning. The recoverability assumption states that switching to the optimal policy at any time enables the recovery of most of the value.
}

\frame{{Sublinear Regret}\vspace{-1ex}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\[R_m(\pi,\mu):=\sup\limits_{\pi'}\mathbb{E}_\mu^{\pi'}\left[\sum\limits_{t=1}^m r_t\right]-\mathbb{E}_\mu^\pi\left[\sum\limits_{t=1}^m r_t\right]\]
	\begin{assumption}[Discount Assumption]
		\begin{enumerate}
			\item $\forall t\colon\gamma_t>0$
			\item $\gamma_t$ is monotone decreasing.
			\item $\forall \varepsilon>0\colon H_t(\varepsilon)\in o(t)$
		\end{enumerate}
	\end{assumption}\vspace{-1ex}
	\begin{theorem}
		If the discount function $\gamma$ satisfies the discount assumption, the environment $\mu$ is recoverable, and $\pi$ is asymptotically optimal in mean, then $R_m(\pi,\mu)\in o(m)$.
	\end{theorem}\vspace{-1ex}
	\[\argmin\limits_\pi\max\limits_\mu R_m(\pi,\mu)\qquad\qquad w_m^\mu:=\frac{2^{-R_m(\pi,\mu)}}{\sum\limits_{\mu\in\mathcal{M}}2^{-R_m(\pi,\mu)}}\]
}

\frame{{Regret in Non-Recoverable Environments}
	\begin{figure}
		\includegraphics[width=.7\textwidth,angle=0,origin=c]{img/hell-heaven}
	\end{figure}
\[R_m(\alpha,\mu_1)=m\qquad R_m(\alpha,\mu_2)=0\]
\[R_m(\beta,\mu_1)=0\qquad R_m(\beta,\mu_2)=m\]
	\begin{tcolorbox}
		For non-recoverable environments:\\
		\textcolor{red}{Either the agent gets caught in a trap or it is not asymptotically optimal.}
	\end{tcolorbox}
}

\frame{{Intrinsic Utility}
	\begin{itemize}
		\item \textcolor{red}{square} \textcolor{blue}{$-\xi(e_{t:k}|\ae_{<t}a_{t:k})$}
		\item \textcolor{red}{shannon} \textcolor{blue}{$-\log\xi(e_{t:k}|\ae_{<t}a_{t:k})\approx K(\ae_{1:k})-K(\ae_{<t})$}
		\item \textcolor{red}{KL divergence} $\textcolor{blue}{D\left(w_{\ae_{<k}}\|w_{\ae_{<t}}\right)}$ where $w_{\ae_{<n}}^\nu=\dfrac{w_\nu\nu(e_{<n}|a_{<n})}{\xi(e_{<n}|a_{<n})}$
		\item \textcolor{red}{information gain} $\textcolor{blue}{H(w_{h_{<t}})-H(w_{h_{1:k}})}$ where \[H(w_h):=-\sum\limits_{\nu\in\mathcal{M}}w_h^\nu\log w_h^\nu\]
		\item \textcolor{red}{effective complexity} $\textcolor{blue}{\mathcal{E}_\delta(\ae_{1:k})-\mathcal{E}_\delta(\ae_{<t})}$ where
		\[\mathcal{E}_\delta(\ae_{<n}):=\min\limits_{\nu\in\mathcal{M}}\left\{2K(\nu)+H(\nu)-K(\ae_{<n})\colon  \nu(e_{<n}|a_{<n})\geq 2^{-H(\nu)(1+\delta)}\right\}\]
		\item \textcolor{red}{logical depth} $\textcolor{blue}{\mathit{depth_b}(h_{1:k})-\mathit{depth_b}(h_{<t})}$ where
		\[\mathit{depth_b}(x):=\min\left\{t\colon U^t(p)=x\;\;\&\;\;\ell(p)-K(x)\leq b\right\}\]
	\end{itemize}
}

\frame{{Hibbard's Two-Stage Model-Based Utility Agent}
\begin{align*}
&\lambda(h):=\argmax\limits_{q\in\mathcal{Q}} P(h|q)P(q)\\
&\rho(h')=P(h'|\lambda(h))\\
&Q(ha)=\sum\limits_{e\in\mathcal{E}}\rho(e|ha)\left[\sum\limits_{z\in Z_h}P(z|\lambda(h))u(z)+\gamma V(h\ae)\right]\\
&V(h)=\max\limits_{a\in\mathcal{A}}Q(ha)\\
&\pi(h)=\argmax\limits_{a\in\mathcal{A}}Q(ha)
\end{align*}
where $Z_h$ is the internal state histories induced by $\lambda(h_{<t})$ that are consistent with $h$.

\textbf{Remarks:} An agents using \textcolor{red}{model-based utility function} will not self-delude: it need to make more accurate estimate of its environment state variables from its interaction history.
}

\frame{{Daniel Dewey's Value Learning Agent \& CIRL}
\[a_k^*=\argmax\limits_{a_k}\sum\limits_{e_k\ae_{k+1:m}}\xi(\ae_{\leq m}|\ae_{<k}a_k)\sum\limits_{u\in\mathcal{U}}P(u|\ae_{\leq m})u(\ae_{\leq m})\]
What could it mean for a machine to have its own goals?
\[\text{\textcolor{red}{\textbf{Shutdown Button}} --- Uncertainty of goals}\]
\[\tilde{U}(u)\implies P_{\tilde{U}}(u)\]
Russell: Cooperative Inverse Reinforcement Learning

CIRL agents learn about a human utility function $u^*$ by observing the actions the human takes.
\resizebox{\textwidth}{!}{\begin{minipage}{1.05\textwidth}
\[V^*(\ae_{<k})=\max\limits_{a_k\in\mathcal{A}}Q^*(\ae_{<k}a_k)\]
\[Q^*(\ae_{<k}a_k)=\mathbb{E}_{e_k}\left[\textcolor{red}{\sum\limits_{a_k^H}P(a_k^H\mid a_k)\sum\limits_{u\in\mathcal{U}}P(u\mid a_k,a_k^H)u(\ae_{1:k})}+\gamma V^*(\ae_{1:k})\,\middle|\,\ae_{<k}a_k\right]\]
\end{minipage}}
}

\frame{{Leibniz Prior}
\begin{itemize}
	\item There's much we don't know about the world.
	\item but we know it's the best possible world.
	\item So simplicity and richness will be represented in the actual (best possible) world.
	\item This is a good \textcolor{red}{inductive bias}.
\end{itemize}
}

\frame{{Leibniz Prior}
	\begin{columns}[onlytextwidth]
		\column{0.47\textwidth}
			\begin{itemize}
				\item the best of all possible worlds
				\item balancing the simplicity of means against the richness of ends
				\item pre-established harmony
			\end{itemize}
			\begin{center}
				\textcolor{red}{prior}\\
				$\Downarrow$\\
				\textcolor{red}{utility}\\
				$\Downarrow$\\
				\textcolor{red}{prior}
			\end{center}
	\[\text{\textcolor{red}{\textbf{Orthogonality!}}}\]
	\[\text{\textcolor{red}{\textbf{Wisdom $\ne$ Intelligence}}}\]
		\column{0.53\textwidth}
			\begin{gather*}
			\textcolor{red}{{\text{universal prior (assumption)}}~w}\\
			\MapDown{\textcolor{blue}{\text{surprise/neg-entropy}}}\\
			\textcolor{blue}{\text{intrinsic utility}}\\
			\MapDown{\text{stochastic environment}}\\
			\text{expected intrinsic utility}\\
			\MapDown{\textcolor{darkgreen}{\underline{\text{Schauder fixpoint}}}}\\
			\textcolor{red}{{\text{universal prior}}~w^*}\\
			\MapDown{\text{Bayesian mixture}}\\
			\xi\\
			\MapDown{\text{ExpectiMax}}\\
			\pi_\xi^*
			\end{gather*}
	\end{columns}
}

\frame{{Leibniz's ``Wisdom''}
			\[\underline{W}isdom =\argmax\limits_\pi \mathbb{E}_\xi^\pi[\underline{H}appiness]\]
\[\underline{H}appiness=\sum\limits_{t=1}^\infty \underline{P}erfection(t)\]
\[\underline{P}erfection = \underline{V}ariety-\underline{S}implicity\]
\[\underline{V}ariety = \mathbb{E}_w[\underline{P}erception]\]
\[\underline{P}erception = \underline{R}eason + (\underline{E}xperience|\underline{R}eason)\]
\[\pi^*:=\argmax\limits_\pi\mathbb{E}_\xi^\pi\left[\sum\limits_{t=1}^\infty\Big(\mathbb{E}_w\left[R + (E|R)\right] - S\Big)\right]\]
}

\frame{{Leibniz's ``Wisdom''}
\[u_t^{\mathrm{in}}(h_{1:k})=H(w_{h_{<t}})-H(w_{h_{1:k}})\]
\[\bar{U}(\nu)=\mathbb{E}_\nu\left[\sum\limits_{t\geq 1} u_t^{\mathrm{in}}(h_{1:t})\right]\]
\[w_\epsilon^\nu\mapsto\bar{U}(\nu)\mapsto w_\epsilon^\nu\]
\begin{align*}
\pi^*&:=\argmax\limits_\pi\mathbb{E}_\xi^\pi\left[\sum\limits_{t=1}^\infty\Big(\mathbb{E}_w\left[R + (E|R)\right] - S\Big)\right]\\
&=\argmax\limits_\pi\mathbb{E}_\xi^\pi\left[\sum\limits_{t=1}^\infty u_t^{\mathrm{in}}(h_{1:t})\right]
\end{align*}
}

\frame{{}
	\begin{itemize}
		\item \underline{Prior: Simplicity}(Kolmogorov Complexity) $\xrightarrow[\text{regular/random}~\mathcal{M}]{\text{break block uniform}}$ free lunch
		\item \underline{\textcolor{red}{Intrinsic Utility}}
		\item \underline{Universal Prior} (Natural UTM)
	\end{itemize}
		\centering\textcolor{red}{Metaphysical} vs \textcolor{red}{Moral/Utilitarian} \\
		means vs ends \qquad wisdom vs intelligence
	\[\mbox{simplicity}+\mbox{intrinsic utility}\to\mbox{universal prior}\]
	\[\mbox{inverse/value reinforcement learning}\]\vspace{-3ex}
\begin{columns}
\column{0.37\textwidth}
	\begin{itemize}
		\item \textcolor{blue}{\textbf{orthogonality}}
		\item \textbf{human interests}
		\item \textcolor{red}{\textbf{external wireheading}}
		\item \textcolor{red}{\textbf{shutdown button}}
	\end{itemize}
\column{0.23\textwidth}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/walle-fire}\\
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/walle-rubik}
		\end{center}
	\end{figure}
\column{0.2\textwidth}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/wireheading}
		\end{center}
	\end{figure}
\column{0.2\textwidth}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/button}
		\end{center}
	\end{figure}
\end{columns}
}

\frame{{Knowledge-Seeking Agent}\vspace{-2ex}
	\[
	V_{\mathit{IG}}^{\pi,m}(h_{<t}):=\mathbb{E}_\xi^\pi\left[\textcolor{darkgreen}{H\big(w_{h_{<t}}^\cdot\big)-H\big(w_{h_{1:m}}^\cdot\big)}\,\middle|\, h_{<t}\right] = \sum\limits_{\nu\in\mathcal{M}} w_{h_{<t}}^\nu D_m\big({}_\nu^\pi\big\|{}_\xi^\pi\bigm\vert h_{<t}\big)
	\]
	\[
	\begin{aligned}
	D_\gamma\big({}_\nu^\pi\big\|{}_\xi^\pi\bigm\vert h_{<t}\big)&:=\sum\limits_{k=t}^\infty\gamma_k\sum\limits_{h'\in\mathcal{H}^{k-t}}{}_\nu^\pi(h'|h_{<t})D\big({}_\nu^\pi\big\|{}_\xi^\pi\bigm\vert h_{<t}h'\big)\\
	V_{\mathit{IG}}^\pi(h_{<t})&:=\mathbb{E}_\xi^\pi\left[\sum\limits_{k=t}^\infty\gamma_k \textcolor{darkgreen}{D\big(w_{h_{1:k}}^\cdot\big\| w_{h_{<k}}^\cdot\big)}\,\middle|\, h_{<t}\right]=\sum\limits_{\nu\in\mathcal{M}}w_{h_{<t}}^\nu D_\gamma\big({}_\nu^\pi\big\|{}_\xi^\pi\bigm\vert h_{<t}\big)\\
	\pi_{\mathit{IG}}^*&:=\argmax\limits_\pi V_{\mathit{IG}}^\pi
	\end{aligned}
	\]
	\[\lim\limits_{t\to\infty}\frac{1}{\Gamma_t}\mathbb{E}_\mu^\pi\left[D_\gamma\big({}_\mu^\pi\big\|{}_\xi^\pi\bigm\vert h_{1:t}\big)\right]=0 \tag{on-policy}\]
	\[\lim\limits_{t\to\infty}\frac{1}{\Gamma_t}\mathbb{E}_\mu^{\pi_{\mathit{IG}}^*}\left[\sup\limits_{\pi\in\Pi(h_{1:t})}D_\gamma\big({}_\mu^\pi\big\|{}_\xi^\pi\bigm\vert h_{1:t}\big)\right]=0 \tag{off-policy}\]
	\resizebox{\textwidth}{!}{maximize knowledge / exploration$=$exploitation / resistant to noise / avoid traps}
}

\frame{{Bayesian Agent}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Require{Model class $\mathcal{M}$; prior $w\in\Delta\mathcal{M}$; history $\ae_{<t}$.}
%\Statex
\Function{act}{$\pi$}
\State Sample and perform action $a_t\sim \pi(\cdot\lvert \ae_{<t})$
\State Receive $e_t\sim \nu(\;\cdot\;\lvert \ae_{<t}a_t) $
\For{$\nu\in\mathcal{M}$}
\State $w_\nu \gets \frac{\nu\left(e_{<t}\lvert a_{<t}\right)}{\xi\left(e_{<t}\lvert a_{<t}\right)}w_\nu$
\EndFor
\State $t \gets t + 1$
\EndFunction
\end{algorithmic}
\caption{Bayesian Agent}\label{alg:bayesian}
\end{algorithm}
}

\frame{{MDL}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Require{Model class $\mathcal{M}$; prior $w\in\Delta\mathcal{M}$; regularizer constant $\lambda\in\mathbb{R}^+$.
}
%\Statex
\Loop
\State $\sigma\gets\arg\min_{\nu\in\mathcal{M}}\left[K(\nu) - \lambda\sum\limits_{k=1}^{t}\log\nu(e_k\lvert\ae_{<k}a_k)\right]$
\State $\textsc{act}\left(\pi_{\sigma}^*\right)$
\EndLoop
\end{algorithmic}
\caption{MDL Agent}\label{alg:mdl}
\end{algorithm}
}	

\frame{{MDL}
	\begin{definition}[MDL]
		\[ \widehat{\nu}=\arg\min\limits_{\nu\in\mathcal{M}}\{K_\nu(x)+K_w(\nu)\}=\arg\max\limits_{\nu\in\mathcal{M}}\{w_\nu \nu(x)\}\]
		where $K_\nu(x):=-\log\nu(x)$ and $K_w(\nu):=-\log w_\nu$
	\end{definition}
	\begin{theorem}[MDL Bound]
		\setlength\abovedisplayskip{0pt}
		\setlength\belowdisplayskip{0pt}
		\begin{gather*}
		\sum\limits_{t=1}^\infty\mathbb{E}_\mu\left[\sum\limits_{x_t\in\mathcal{X}}\Big(\widehat{\nu}(x_t|x_{<t})-\mu(x_t|x_{<t})\Big)^2\right]
		\leqa 8w_\mu^{-1}
		\end{gather*}
	\end{theorem}
	MDL converges, but speed can be exponential worse than Bayes.
}

\frame{{Weak Asymptotic Optimality --- Optimistic Agent}\vspace{-1ex}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Require{Finite class of deterministic environments $\mathcal{M}_0=\mathcal{M}$}
\State $t=1$
\Repeat
\State $(\pi^*,\nu^*):=\argmax_{\pi\in\Pi,\nu\in\mathcal{M}_{t-1}}V_\nu^\pi(h_{t-1})$
  \Repeat
  \State $a_t=\pi^*(h_{t-1})$
  \State Perceive $e_t$ from environment $\mu$
  \State $h_t\gets h_{t-1}a_te_t$
  \State Remove inconsistent environment $\mathcal{M}_t:=\left\{\nu\in\mathcal{M}_{t-1}\colon h_t^{\pi^\circ\nu}=h_t\right\}$
  \State $t\gets t+1$
  \Until $\nu^*\notin\mathcal{M}_{t-1}$
\Until $\mathcal{M}=\emptyset$
\end{algorithmic}
\caption{Optimistic Agent $\pi^\circ \hfill \pi_t^\circ:=\argmax_\pi\max_{\nu\in\mathcal{M}_t} V_\nu^\pi(h_{1:t})$}\label{alg:optimistic}
\end{algorithm}\vspace{-1ex}
	stochastic case: $\mathcal{M}_t:=\Big\{\nu\in\mathcal{M}_{t-1}\colon \nu(e_{<t}|a_{<t})\geq\varepsilon_t\max\limits_{\rho\in\mathcal{M}}\rho(e_{<t}|a_{<t})\Big\}$\\
	Act optimally w.r.t. the most optimistic environment until contradicted.\\
	If there is a chance: Try it! --- Vulnerable to traps.
}

\frame{{Asymptotic Optimality in Mean --- Thompson Sampling}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Require{Model class $\mathcal{M}$; prior $w\in\Delta\mathcal{M}$; exploration schedule $\left(\varepsilon_t\right)_{t\in\mathbb{N}}$.
}
%\Statex
\Loop
\State Sample $\rho\sim w_{\ae_{<t}}^\cdot$
\For{$i = 1\to H_t\left(\varepsilon_t\right)$}
\State $\textsc{act}\left(\pi_{\rho}^*\right)$
\EndFor
\EndLoop
\end{algorithmic}
\caption{Thompson Sampling $\pi_T$}\label{alg:thompson}
\end{algorithm}
	\begin{theorem}
		If the discount function $\gamma$ satisfies the discount assumption, the environment $\mu$ is recoverable, then $R_m(\pi_T,\mu)\in o(m)$.
	\end{theorem}
}

\frame{{Weak Asymptotic Optimality --- BayesExp}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Require{Model class $\mathcal{M}$; prior $w\in\Delta\mathcal{M}$; exploration schedule $\left(\varepsilon_t\right)_{t\in\mathbb{N}}$.
}
%\Statex
\Loop
\If{$V_{\mathit{IG}}^*\left(\ae_{<t}\right)>\varepsilon_t$}
\For{$i = 1\to H_t\left(\varepsilon_t\right)$}
\State $\textsc{act}\left(\pi_{\mathit{IG}}^*\right)$
\EndFor
\Else
\State $\textsc{act}\left(\pi_{\xi}^*\right)$
\EndIf
\EndLoop
\end{algorithmic}
\caption{BayesExp $\pi_{\mathit{BE}}$}\label{alg:bayesexp}
\end{algorithm}
	\textbf{$\varepsilon$-optimal BayesExp:} If the optimal information gain value $V_{\mathit{IG}}^*>\varepsilon_t$, then execute the $\varepsilon$-optimal information gain policy $\pi_{\mathit{IG}}^{\varepsilon_t}$ for  $H_t(\varepsilon_t)$ steps, else execute $\pi_\xi^{\varepsilon_t}$ for $1$ step.
}

\frame{{Strong Asymptotic Optimality --- Inquisitive Agent}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{align*}
V_{\mathit{IG}}^\pi(h_{<t})&:=\mathbb{E}_\xi^\pi\left[D\big(w_{h_{<t+m}}^\cdot\big\| w_{h_{<t}}^\cdot\big)\,\middle|\, h_{<t}\right]\\
\pi_{\mathit{IG}}^{m,k}&:=\argmax_{\pi\in\mathcal{A}^{\mathcal{H}^{<m}}} V_{\mathit{IG}}^\pi(h_{<t-k})\\
\rho(h_{<t},m,k)&:=\min\left\{\frac{1}{m^2(m+1)}, \eta V_{\mathit{IG}}^{\pi_{\mathit{IG}}^{m,k}}(h_{<t-k})\right\}
\end{align*}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\While{True}
\State calculate $\rho(h_{<t},m,k)$ for all $m$ and for all $k<\min\{m,t\}$
\State $\textsc{act}\;\pi_{\mathit{IG}}^{m,k}(h_{<t})$ with probability $\rho(h_{<t},m,k)$
\State $\textsc{act}\;\pi_\xi^*(h_{<t})$ with probability $1-\sum\limits_{m\in\mathbb{N}} \sum\limits_{k<m,t}\rho(h_{<t},m,k)$
\EndWhile
\end{algorithmic}
\caption{Inquisitive Agent $\pi^\dagger$}
\end{algorithm}
\resizebox{\textwidth}{!}{$\pi^\dagger(a|h_{<t}):=\sum\limits_{m\in\mathbb{N}}\sum\limits_{k<m,t}\rho(h_{<t},m,k)\left\llbracket a=\pi_{\mathit{IG}}^{m,k}(h_{<t})\right\rrbracket+\left(1-\sum\limits_{m\in\mathbb{N}} \sum\limits_{k<m,t}\rho(h_{<t},m,k)\right)\left\llbracket a=\pi_\xi^*(h_{<t})\right\rrbracket$}
}

\frame{{Approximation}
	\begin{center}
		\fbox{The AIXI approximations are outperformed by DQN/DRQN\dots}
	\end{center}
	\begin{itemize}
		\item MC-AIXI-CTW.
		\begin{itemize}
			\item Approximate Solomonoff induction --- most recent actions and percepts (=context) more relevant --- Context Tree Weighting
			\item Sample paths in expectimax tree.
		\end{itemize}
		\item Feature Reinforcement Learning ($\Phi$MDP). --- history $\mapsto$ state\\
		e.g. Classical physics: Position+velocity of objects = position at two time-slices. ($2^{nd}$ order Markov.)
		\[\Phi\colon h\mapsto s\]
		\[\Phi^{best}:=\argmin\limits_\Phi \mathit{Cost}(\Phi|h)\]
		\[\mathit{Cost}(\Phi|h):=\mathit{CL}(s_{1:n}^\Phi|a_{1:n})+\mathit{CL}(r_{1:n}|s_{1:n}^\Phi,a_{1:n})+\mathit{CL}(\Phi)\]
		How to find the map $\Phi$?  Monte-Carlo\dots
		\item Compress and Control. --- (model-free)\\
		Combine induction and planning.
	\end{itemize}
}

\frame{{Expectimax Approximation: MC-AIXI-CTW}
Upper Confidence Tree (UCT) algorithm:
\begin{itemize}
	\item \textcolor{blue}{Sample} observations from Context Tree Weighting (CTW) distribution $\mathit{CTW}(e_{<t}|a_{<t}):=\sum\limits_\Gamma 2^{-\mathit{CL}(\Gamma)}\Gamma(e_{<t}|a_{<t})$.
	\item \textcolor{blue}{Select} actions with highest upper confidence bound.
\[a_{\mathit{ucb}}:=\argmax\limits_{a\in\mathcal{A}}\left(\underbrace{\hat{Q}(\ae_{<t}a)}_{\text{average}}+\underbrace{\sqrt{\frac{\log T(\ae_{<t})}{T(\ae_{<t}a)}}}_{\text{exploration bonus}}\right)\]
where $T(\cdot)$ is the number of times a sequence has been visited.
	\item \textcolor{blue}{Expand} tree by one leaf node (per trajectory).
	\item \textcolor{blue}{Simulate} from leaf node further down using (fixed) playout policy.
	\item \textcolor{blue}{Propagate back} the value estimates for each node.
\end{itemize}
}

\frame{{MC-AIXI-CTW}
	\begin{figure}[!htbp]
		\centerline{\mbox{\includegraphics[scale=.9]{img/mc-aixi}}}
	\end{figure}
}

\frame{{Feature Reinforcement Learning ($\Phi$MDP)}\vspace{-2ex}
	\begin{figure}[!htbp]
		\hspace{-0.45\textwidth}\includegraphics[width=.6\textwidth,angle=0,origin=c]{img/frlphi}
	\end{figure}\vspace{-10ex}
	\resizebox{.9\textwidth}{!}{
		\begin{minipage}{\textwidth}
			\unitlength=2.7ex
			\linethickness{0.4pt}
			\hspace{.35\textwidth}
			\begin{picture}(20,14)(0,0)
			\thicklines
			\put(0,0){\framebox(20,2)[cc]{\textbf{Environment}}}
			\put(3,5){\oval(6,2)\makebox(0,0)[cc]{History $\ae_{<t}$}}
			\put(3,9){\oval(6,2)\makebox(0,0)[cc]{$s_t=\Phi(\ae_{<t})$}}
			\put(5,13){\oval(6,2)\makebox(0,0)[cb]{\footnotesize Transition Pr. $\hat{T}_{ss'}$}
				\makebox(0,0)[ct]{\footnotesize Reward est.\ $\hat{R}_s\quad$}}
			\put(15,13){\oval(6,2)\makebox(0,0)[cc]{$\hat{T}_{ss'}^e$, $\hat{R}_s^e$}}
			\put(17,9){\oval(6,2)\makebox(0,0)[cc]{Value est. $\hat{V}$}}
			\put(17,5){\oval(6,2)\makebox(0,0)[cc]{Best Policy $\hat{\pi}$}}
			\put(3,2){\vector(0,1){2}\makebox(0,2)[lc]{$\;e_{t-1}$}}
			\put(3,6){\vector(0,1){2}\makebox(0,2)[lc]{$\;\min Cost(\Phi|\ae_{<t})$}}
			\put(3,10){\vector(1,1){2}\makebox(0,2)[lc]{$\quad\;\,$estimate}\makebox(1,2)[rc]{frequency$\;$}}
			\put(8,13){\vector(1,0){4}\makebox(3.7,0)[cb]{\small exploration}\makebox(-4,0)[ct]{\small bonus}}
			\put(15,12){\vector(1,-1){2}\makebox(0.7,-2)[rc]{Bellman}}
			\put(17,8){\vector(0,-1){2}\makebox(0,-2)[rc]{implicit$\;$}}
			\put(17,4){\vector(0,-1){2}\makebox(0,-2)[rc]{$a_t\;$}}
			\end{picture}
	\end{minipage}}
}

\frame{{Universal Search}
	\begin{columns}
		\column{.5\textwidth}
			\begin{itemize}
				\item Levin Search
				\item Speed Prior
				\item Hutter Search
				\item AIXI$^{t\ell}$
				\item Optimal Ordered Problem Solver
				\item G\"odel Machine
			\end{itemize}
		\column{.17\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth,angle=0,origin=c]{img/levin0}\caption{Levin}
			\end{figure}
	\end{columns}
}

\frame{{Levin Search (\textsc{Lsearch})}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	An inversion algorithm $p$ inverts a function $\varphi$ if given $x$, $p(x)=y$ s.t. $\varphi(y)=x$.
	\begin{tcolorbox}[title=\textsc{Lsearch}]
		Run all $\{p\colon \ell(p)\leq i\}$ for $2^{i-\ell(p)}$ steps in phase $i=1,2,3,\ldots$ until it has inverted $\varphi$ on $x$.
	\end{tcolorbox}\vspace{-2ex}
	\[
	Kt(x) := \min\limits_p\big\{\ell(p)+\log t(p,x)\colon U(p)=x\big\}
	\]\vspace{-2ex}
	\begin{theorem}
		All strings $\{x\colon Kt(x) \leq n\}$ can be generated and tested in $2^{n+1}$ steps.
	\end{theorem}\vspace{-2ex}
	\[t_{\textsc{Lsearch}}(x) = \mathcal{O}\Big(2^{K(n)}t^+_{p_n}(x)\Big)\] where $t^+_{p_n}(x)$ is the runtime of $p_n(x)$ plus the time to verify the correctness of the result $\varphi(p_n(x))\!=\!x$.
	
	\textbf{Remark:} If P$=$NP, then \textsc{Lsearch} is a P algorithm for every NP problem.
}

\frame{{Speed Prior}\vspace{-2ex}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\[S(e_{<t}|a_{<t}):=\sum\limits_{p:U(p,a_{<t})=e_{<t}}\frac{2^{-\ell(p)}}{t(p,a_{<t},e_{<t})}\]
	\begin{center}
		\textcolor{blue}{$S$ is computable.}
	\end{center}
	A function $f$ is estimable in polynomial time iff there is a function $g$ computable in polynomial time s.t. $f\eqm g$.
	\begin{tcolorbox}
		For any measure $\mu$ estimable in polynomial time,
		\[
		\left(\sqrt{L_n^{\Lambda_S}}-\sqrt{L_n^{\Lambda_\mu}}\right)^2\leq 2D_n(\mu\|S)=\mathcal{O}\left(\log n\right)
		\]\vspace{-1ex}
	\end{tcolorbox}
	\begin{tcolorbox}[title=On-Policy Value Convergence]
		If the effective horizon is bounded, then for any environment $\mu\in\mathcal{M}_{comp}$ estimable in polynomial time and any policy $\pi$,
		\[{}_\mu^\pi\left(\lim\limits_{t\to\infty}\frac{1}{t}\sum\limits_{k=1}^t\left(V_S^\pi(h_{<k})-V_\mu^\pi(h_{<k})\right)\right)=0\]\vspace{-1ex}
	\end{tcolorbox}
}

\frame{{Hutter Search (\textsc{Hsearch})}
	\begin{columns}
		\column{.5\textwidth}
			\begin{enumerate}
				\item[] {$\hspace{-1em}\hspace{-1em}$\em \textcolor{red}{\fbox{$M^\varepsilon_{p^*}(x)$}}}
				\item[] \textcolor{red}{Initialize the shared variables $L:=\{\},\;\; t_{fast}:=\infty,\;\; p_{fast}:=p^*$.}
				\item[] \textcolor{red}{Start algorithms $A$, $B$, and $C$ in parallel with $\varepsilon$, $\varepsilon$, and $1-2\varepsilon$ computation time, respectively.}
			\end{enumerate}
		\column{.5\textwidth}
			\begin{enumerate}
				\item[] {\hspace{-1em}\hspace{-1em} \textcolor{darkgreen}{\fbox{$A$}}}
				\item[] \hspace{-1em}\textcolor{brown}{{\tt for}} \textcolor{darkgreen}{$i:=1,2,3,\dots$}\textcolor{brown}{{\tt do}}
				\item[] \textcolor{darkgreen}{pick the last wff of the $i^{th}$ proof.}
				\item[] \textcolor{brown}{\tt if} \textcolor{darkgreen}{it reads ``$p(\cdot)$ is equivalent to $p^*(\cdot)$ and has time-bound $t(\cdot)$'',}\\
				\textcolor{brown}{\tt then} \textcolor{darkgreen}{add $(p,t)$ to $L$.}
				\item[] \hspace{-1em}\textcolor{brown}{\tt continue}
			\end{enumerate}
	\end{columns}
	\begin{columns}
		\column{.5\textwidth}
			\begin{enumerate}
				\item[] {\hspace{-1em}\hspace{-1em} \textcolor{blue}{\fbox{$B$}}}
				\item[] $\hspace{-1em}$\textcolor{brown}{\tt for} \textcolor{blue}{$(p,t) \in L$}
				\item[] \textcolor{blue}{run $t(x)$ in parallel for all $t$ with computation time $2^{-\ell(p)-\ell(t)}$.}
				\item[] \textcolor{brown}{\tt if} \textcolor{blue}{for some $t$, $t(x) < t_{fast}$,}
				\item[] \textcolor{brown}{\tt then} \textcolor{blue}{$t_{fast}:=t(x)$ and $p_{fast}:=p$.}
				\item[] $\hspace{-1em}$\textcolor{brown}{\tt continue}
			\end{enumerate}
		\column{.5\textwidth}
			\begin{enumerate}
				\item[] {\hspace{-1em}\hspace{-1em} \textcolor{cyan}{\fbox{$C$}}}
				\item[] \hspace{-1em}\textcolor{brown}{\tt for} \textcolor{cyan}{$k:=1,2,4,8,\dots$} \textcolor{brown}{\tt do}
				\item[] \textcolor{cyan}{run current $p_{fast}$ for $k$ steps.}
				\item[] \textcolor{brown}{\tt if} \textcolor{cyan}{$p_{fast}$ halts,}
				\item[] \textcolor{brown}{\tt then} \textcolor{cyan}{print result $p_{fast}(x)$ and abort $A$, $B$ and $C$.}
				\item[] \hspace{-1em}\textcolor{brown}{\tt continue}
			\end{enumerate}
	\end{columns}
}

\frame{{AIXI$^{t\ell}$}
\begin{enumerate}
	\item \textcolor{red}{Let $P:=\emptyset$. This will be the set of verified programs.}
	\item \textcolor{red}{For all proofs of length $\leq n$: if the prover shows $\mathit{VA}(p)$ for some $p$ with $\ell(p)\leq\ell$, then add $p$ to $P$.}
	\[\textcolor{blue}{\mathit{VA}(p):=``\forall k\forall(va^\prime\ae)_{1:k}\colon p(\ae_{<k})=v_1a_1^\prime\dots v_ka_k^\prime\implies v_k\leq V_\xi^\pi(\ae_{<k})"}\]
	\textcolor{darkgreen}{(The program $p$ not only computes future actions of $\pi$, which is the policy derived from $p$ according to $\pi(\ae_{<k}):=a_k^\prime$, but also hypothetical past actions $a_i^\prime$ and lower bounds $v_i$ for the value of the policy $\pi$.)}
	\item \textcolor{red}{For each input history $\ae_{<k}$ repeat: run all programs from $P$ for $\leq t$ steps each, take the one with the highest promised value $v_k$, and return that program's policy's action.}
\end{enumerate}
\begin{itemize}
	\item AIXI$^{t\ell}$ depends on $t,\ell,n$ but not on knowing $p$.
	\item Its setup-time is $t_{setup}(p^{best})=\mathcal{O}(n\cdot 2^n)$.
	\item Its computation time per cycle is $t_{cycle}(p^{best})=\mathcal{O}(t\cdot 2^\ell)$.
\end{itemize}
}

\frame{{Schmidhuber's Optimal Ordered Problem Solver (OOPS)}
	\begin{itemize}
		\item Solve the first task with \textsc{Lsearch}.
		\item Freeze successful programs in non-writable memory.
		\item Programs tested during search for later tasks may copy non-writable code into separate modifiable storage, to edit it and execute the modified result.
		\item Given a new task, OOPS spends half of the time to test programs that have the most recent successful program as a prefix, the other half to fresh programs. \item Time is allocated according to a distribution over programs, which is obtained by multiplying the probabilities of the individual instructions.
	\end{itemize}
\centering\textcolor{red}{Incremental Learning}
}

\frame{{Liar Paradox vs Quine Paradox}
\begin{itemize}
\item 这句话有2个‘这’字，2个‘句’字，2个‘话’字，2个‘有’字，7个‘2’字，11个‘个’字，11个‘字’字，2个‘7’字，3个‘11’字，2个‘3’字。
\item 我在说谎。
\item 把“\texttt{把中的第一个字放到左引号前面，其余的字放到右引号后面，并保持引号及其中的字不变得到的句子是假的。}”中的第一个字放到左引号前面，其余的字放到右引号后面，并保持引号及其中的字不变得到的句子是假的。
\end{itemize}\vspace{-3ex}
	\begin{figure}[H]
		\includegraphics[width=.7\textwidth,angle=0,origin=c]{img/self-reference-agent}
	\end{figure}
}

\frame{{Paradox vs Self-reference}
	\begin{columns}
		\column{0.42\textwidth}
			\begin{tcolorbox}
				self-reference / negation / vicious circularity or infinite regress / totality / infinity
			\end{tcolorbox}
			\centering\fbox{The sentence below is false.}\\
			\centering\includegraphics[width=.9\textwidth]{img/escher-hands}\\\vspace{-1pt}
			\centering\fbox{The sentence above is true.}
		\column{0.58\textwidth}\vspace{-1ex}
			\begin{block}{Quine}
				``Yields falsehood when preceded by its quotation'' yields falsehood when preceded by its quotation.
			\end{block}
			\begin{tcolorbox}
				Print two copies of the following, the second copy in quotes:\\
				``Print two copies of the following, the second copy in quotes:''
			\end{tcolorbox}\vspace{-5pt}
			\begin{block}{Yablo}
				\begin{itemize}
					\item $S_1$: for all $k>1$, $S_k$ is false.
					\item $S_2$: for all $k>2$, $S_k$ is false.
					\item $S_3$: for all $k>3$, $S_k$ is false.
					\item $\phantom{S3: for all ak}\vdots$
				\end{itemize}
			\end{block}
	\end{columns}
}

\frame{{Diagonalization}
	\begin{theorem}[Lawvere's Fixpoint Theorem]
		In any Cartesian closed category, given an object $Y$, if there is an object $X$ and an arrow $f\colon X\times X\to Y$ such that, for every $g\colon X\to Y$ there is a $t\colon\bm{1}\to X$ for all $x\colon\bm{1}\to X$: $g\circ x=f\circ(x,t)$, then every arrow $\alpha\colon Y\to Y$ has a fixpoint $y\colon\bm{1}\to Y$ such that $\alpha\circ y=y$.
	\end{theorem}
	\begin{figure}
		\includegraphics[width=0.2\textwidth,angle=0,origin=c]{img/lawvere.jpg}
	\end{figure}
}

\frame{{Lawvere's Fixpoint Theorem}
	\begin{itemize}
		\item A function $g\colon X\to Y$ is \emph{representable} by $f\colon X\times X\to Y$ iff
		\[\exists y\forall x\colon g(x)=f(x,y)\]
	\end{itemize}
	\begin{theorem}[Lawvere's Fixpoint Theorem]
		For sets $X, Y$, functions $f\colon X\times X\to Y$, $\alpha\colon Y\to Y$, let $g:=\alpha\circ f\circ\Delta$.
		\begin{enumerate}
			\item If \textcolor{blue}{$\alpha$ has no fixpoint}, then \textcolor{red}{$g$ is not representable by $f$}.
			\item If \textcolor{blue}{$g$ is representable by $f$}, then \textcolor{red}{$\alpha$ has a fixpoint}.
		\end{enumerate}
	\end{theorem}
	\begin{columns}[onlytextwidth]
		\column{.62\textwidth}
			\[\xymatrix{X\times X
				\ar[rr]^f && Y \ar[dd]^\alpha
				\\
				\\
				X\ar[uu]^{\Delta}\ar[rr]_{g}&&Y}\]
				$\alpha\big(f\left(\ulcorner g\urcorner,\ulcorner g\urcorner\right)\big)=g\left(\ulcorner g\urcorner\right)=f\left(\ulcorner g\urcorner,\ulcorner g\urcorner\right)$
		\column{.38\textwidth}
			\resizebox{\textwidth}{!}{\hspace{-0.15\textwidth}
				\begin{minipage}{1.35\textwidth}
					\begin{tcolorbox}
						\begin{itemize}
							\item $\Delta\colon x\mapsto(x,x)$ diagonal
							\item $f$ evaluation
							\item $\alpha$ ``negation''
							\item $g\left(\ulcorner g\urcorner\right)$ fixpoint-(free) transcendence
							\item $f\left(\ulcorner g\urcorner,\ulcorner g\urcorner\right)$ self-reference\\
							``I have property $\alpha$.''
						\end{itemize}
					\end{tcolorbox}
				\end{minipage}
			}
	\end{columns}
}

\frame{{Diagonalization}
	\begin{itemize}
		\item A function $g\colon X\to Z$ is \emph{representable} by  $f\colon X\times Y\to Z$ iff
		\[\exists y\in Y\forall x\in X\colon g(x)= f(x,y)\]
	\end{itemize}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{theorem}[Lawvere's Fixpoint Theorem]
		For sets $X, Y, Z$, functions $\beta\colon X\to Y, f\colon X\times Y\to Z, \alpha\colon Z\to Z$, let $g:=\alpha\circ f\circ(1_X,\beta)$. Assume \textcolor{darkgreen}{$\beta$ is surjective}. 
		\begin{enumerate}
			\item If \textcolor{blue}{$\alpha$ has no fixpoint}, then \textcolor{red}{$g$ is not representable by $f$}.
			\item If \textcolor{blue}{$g$ is representable by $f$}, then \textcolor{red}{$\alpha$ has a fixpoint}.
		\end{enumerate}
	\end{theorem}
	\[\xymatrix{X\times Y
	\ar[rr]^f && Z \ar[dd]^\alpha\\
	\\
	X\ar[uu]^{(1_X,\beta)}\ar[rr]_{g}&&Z}\]
}

\frame{{Kleene's Fixpoint Theorem}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{theorem}[Kleene's Fixpoint Theorem]
		Given a recursive function $h$, there is an index $e$ s.t.
		\[\varphi_e=\varphi_{h(e)}\]
	\end{theorem}
	\[\xymatrix{\mathbb{N}\times \mathbb{N}
		\ar[rr]^{f} && \left\{\varphi_n\right\}_{n\in\mathbb{N}} \ar[dd]^{\mathscr{E}_h}
		\\
		\\
		\mathbb{N}\ar[uu]^{\Delta}\ar[rr]_{g}&&\left\{\varphi_n\right\}_{n\in\mathbb{N}}}\]
	where $f\colon(m,n)\mapsto\varphi_{\varphi_n(m)}$, and $\mathscr{E}_h\colon\varphi_n\mapsto\varphi_{h(n)}$.\\
	The function $g\colon m\mapsto\varphi_{h\left(\varphi_m(m)\right)}$ is a recursive sequence of partial recursive functions, and thus is representable by $f$. Explicitly,
	\[g(m)=\varphi_{h(\varphi_m(m))}=\varphi_{s(m)}=\varphi_{\varphi_t(m)}=f(m,t)\]
	\[e:=\varphi_t(t)\]
}

\frame{{Lawvere Fixpoint Theorem --- Fixpoint vs Diagonalization}\vspace{-3ex}
	\[\xymatrix{X\times X
		\ar[rr]^{f} &&Y\ar[dd]^\alpha
		\\
		\\
		X\ar[uu]^{\Delta}\ar[rr]_{g}&&Y}\]
	\scalebox{.85}{
		\begin{minipage}{\textwidth}
\[
\begin{array}{ccccccccc}
\hline
\hline
\mbox{Curry } Y&\hat{=} &\mbox{Fixpoint}&\hat{=} &\mbox{\textcolor{red}{G\"odel}}&\hat{=} &\mbox{\textcolor{red}{Kleene}}&\hat{=} &\mbox{Russell}\\
\hline
yx &\hat{=} &N\left(\ulcorner M\urcorner\right) &\hat{=} &\psi(\ulcorner\varphi(x)\urcorner) &\hat{=} &\varphi_n(m) &\hat{=} &x\in y\\
xx &\hat{=} &M\left(\ulcorner M\urcorner\right) &\hat{=} &\varphi(\ulcorner\varphi(x)\urcorner) &\hat{=} &\varphi_n(n) &\hat{=} &x\in x\\
y(xx) &\hat{=} &F\ulcorner M\ulcorner M\urcorner\urcorner &\hat{=} &\alpha(\ulcorner\varphi(\ulcorner\varphi(x)\urcorner)\urcorner) &\hat{=} &h(\varphi_n(n)) &\hat{=} &x\notin x\\
\lambda x.y(xx) &\hat{=} &G &\hat{=} &\gamma(x) &\hat{=} &\varphi_t(n) &\hat{=} &x\notin R\\
(\lambda x.y(xx))(\lambda x.y(xx)) &\hat{=} &G\left(\ulcorner G\urcorner\right) &\hat{=} &\gamma(\ulcorner\gamma(x)\urcorner) &\hat{=} &\varphi_t(t) &\hat{=} &R\notin R\\
\hline
\end{array}
\]
	\end{minipage}}
	\[\fbox{\textcolor{blue}{self-reference} \textcolor{red}{$\stackrel{?}{\implies}$} \textcolor{blue}{self-improvement}}\]
}

\frame{{Kleene's Fixpoint Theorem}
	\begin{theorem}[Kleene's Fixpoint Theorem]
		Given a recursive function $h$, there is an index $e$ s.t.
		\[\varphi_e= \varphi_{h(e)}\]
	\end{theorem}
	\begin{corollary}[Second Recursion Theorem]
		If $f(x,y)$ is a partial recursive function, there is an index $e$ s.t.
		\[\varphi_e(y)= f(e,y)\]
	\end{corollary}
	\begin{proof}
		By the $smn$ theorem, $\varphi_{s(x)}(y)= f(x,y)$. Then
		\[\exists e\colon\varphi_e(y)= \varphi_{s(e)}(y)= f(e,y)\]
	\end{proof}
}

\frame{{Kleene's Relativized Fixpoint Theorem (with Parameters)}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{theorem}[Kleene's Relativized Fixpoint Theorem (with Parameters)]
Let $A\subset\mathbb{N}$. If $f(x,y)$ is an $A$-computable function, then there is a computable
function $e(y)$ s.t. $\varphi_{e(y)}^A=\varphi_{f(e(y),y)}^A$ for all $y$. Moreover, $e$ does not depend on $A$.
\end{theorem}
\begin{proof}
Let the index $e$ code the function
\[\varphi_e^A(x,y,z)=
\begin{cases}
\varphi_{\varphi_x(x,y)}^A(z) &\mbox{if } \varphi_x(x,y)\downarrow\\
\uparrow &\mbox{otherwise}
\end{cases}
\]
By the relativized $smn$ theorem there is a computable function $s(x,y)$ s.t.
\[\varphi_{s(x,y)}^A= \varphi_e^A(x,y,z)\]
We know $\exists v\colon \varphi_v^A(x,y)=f(s(x,y),y)$. Let $e(y):=s(v,y)$.
\[\varphi_{e(y)}^A=\varphi_{s(v,y)}^A=\varphi_{\varphi_v^A(v,y)}^A=\varphi_{f(s(v,y),y)}^A=\varphi_{f(e(y),y)}^A\]
\end{proof}
}

\frame{{von Neumann's Self-reproducing Automata}
	\begin{corollary}[von Neumann's Self-reproducing Automata]
		There is a recursive function $\varphi_e$ s.t. $\forall x\colon \varphi_e(x)= e$.
	\end{corollary}
	\[\fbox{\textcolor{blue}{There is a program that outputs its own length.}}\]
	\[\fbox{\textcolor{blue}{There is a program that outputs its own source code.}}\]
	\[\fbox{\textcolor{darkgreen}{DNA / mutation / evolution}}\]
}

\frame{{von Neumann's Self-reproducing Automata}
	\begin{enumerate}
		\item A universal constructor $A$:
		\[A+\ulcorner X\urcorner\leadsto X\]
		\item A copying machine $B$:
		\[B+\ulcorner X\urcorner\leadsto\ulcorner X\urcorner\]
		\item A control machine $C$, which first activates $B$, then $A$:
		\[A+B+C+\ulcorner X\urcorner\leadsto X+\ulcorner X\urcorner\]
		Thus $A+B+C+\ulcorner A+B+C\urcorner$ is self-reproducing.
		\[A+B+C+\ulcorner A+B+C\urcorner\leadsto A+B+C+\ulcorner A+B+C\urcorner\]
		\item It is possible to add the description of any machine $D$
		\[A+B+C+\ulcorner A+B+C+D\urcorner\leadsto A+B+C+D+\ulcorner A+B+C+D\urcorner\]
		Now allow mutation on the description $\ulcorner A+B+C+D\urcorner$
		\[A+B+C+\ulcorner A+B+C+D'\urcorner\leadsto A+B+C+D'+\ulcorner A+B+C+D'\urcorner\]
	\end{enumerate}
}

\frame{{Introspective Program}
	\begin{definition}[$\psi$-introspective]
		Given a total recursive function $\psi$,
		\begin{itemize}
			\item the \emph{$\psi$-analysis} of $\varphi(x)$ is the code of the computation of $\varphi(x)$ to $\psi(x)$ steps.
			\item $\varphi$ is \emph{$\psi$-introspective} at $x$ if $\varphi(x)\downarrow$ and outputs its own $\psi$-analysis.
			\item $\varphi$ is \emph{totally $\psi$-introspective} if it is $\psi$-introspective at all $x$. 
		\end{itemize}
	\end{definition}
	\begin{corollary}
		There is a program that is totally $\psi$-introspective.
	\end{corollary}
	\begin{proof}
		Let $f(n,x):= \mbox{``the $\psi$-analysis of $\varphi_n(x)$''}$.
	\end{proof}
}

\frame{{Introspective Program}\centering
\fbox{\textcolor{red}{There is a program that is totally introspective.}}
\begin{columns}
\column{.6\textwidth}\vspace{-7ex}
\[\fbox{\Large $\bm{\varphi_e=\varphi_{h(e)}}$}\]
\resizebox{.85\textwidth}{!}{
\begin{minipage}{\textwidth}
\begin{table}
\begin{tabu}{c|c}
\hline
\textbf{Self-simulating Computer} &\textbf{Self-consciousness}\\
\hline
\textcolor{darkgreen}{Host Machine} &\textcolor{red}{Experiencing Self}\\
\hline
\textcolor{darkgreen}{Virtual Machine} &\textcolor{red}{Remembering Self}\\
\hline
Hardware &Body\\
\hline
\end{tabu}
\end{table}
\end{minipage}}
\column{.4\textwidth}
	\begin{figure}[H]\vspace{-1ex}
		\begin{center}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/yoga}
		\end{center}
	\end{figure}
\end{columns}
}

\frame{{}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.7\textwidth,angle=0,origin=c]{img/snow-chicken}
		\end{center}
	\end{figure}
}

\frame{{}
\begin{figure}
\includegraphics[width=\textwidth]{img/map-on-table.jpeg}\caption{One can imagine a detailed floor plan of a room, sitting on a table in the room; this plan has an image of the table on which there is an image of the plan itself. Now introduce the dynamical aspect: the items on the plan are cut out from paper and can be moved to try a different furniture arrangement; in this way the plan models possible states of the world about which it carries information.}
\end{figure}
}

\frame{{Manin --- \href{https://arxiv.org/abs/1709.03114}{\textcolor{fore}{Cognitive Networks}}}
\begin{tcolorbox}
The brain contains inside a map of itself, and some neural information channels in the central neural system:
\begin{itemize}
	\item carry information about the mind itself, i.e., are \textcolor{red}{reflexive};
	\item are capable of modelling states of the mind different from the current one, i.e., possess a \textcolor{red}{modelling function};
	\item can influence the state of the whole mind and through that, the behavior, i.e., possess \textcolor{red}{controlling function}.
\end{itemize}
The reflection of the brain inside itself must be \textcolor{red}{coarse grained}.
\end{tcolorbox}
}

\frame{{侯士达——“我”是个怪圈}
%The presence or absence of animacy depends on the level at which one views a structure. Seen at its highest, most collective level, a brain is quintessentially animate and conscious. Animate entities are those that, at some level of description, manifest a certain type of loopy pattern, which inevitably starts to take form if a system with the inherent capacity of perceptually filtering the world into discrete categories vigorously expands its repertoire of categories ever more towards the abstract. This pattern reaches full bloom when there comes to be a deeply entrenched self-representation --- a story told by the entity to itself --- in which the entity's ``I'' plays the starring role, as a unitary causal agent driven by a set of desires.
\begin{itemize}
	\item 有没有意识取决于在哪个层级上对结构进行观察。在整合度最高的层级上看，大脑是有意识的。下降到微观粒子层面，意识就不见了。
	\item 意识体是那些在某个描述层级上表现出某种特定类型的循环回路的结构。当一个系统能把外部世界过滤成不同的范畴、并不断向越来越抽象的层级创造新的范畴时，这种循环回路就会逐渐形成。
	\item 当系统能进行自我表征——对自己讲故事——的时候，这种循环回路就逐渐变成了实体的“我”——一个统一的因果主体。
\end{itemize}
}

\frame{{}\footnotesize
\begin{longtable}{p{0.32\textwidth}|p{0.6\textwidth}}
\hline
说谎者悖论&\textbf{我在说谎。}\\
\hline
Grelling悖论&‘非自谓的’是自谓的吗？\\
\hline
Russell悖论&“不属于自身的集合的集合”属于自身吗？\\
\hline
Berry悖论&我是少于十八个字不可定义的最小数。\\
\hline
Yablo悖论&我下一句及后面所有的句子都是假的。\\
\hline
G\"odel不动点引理&\textbf{我有性质$\alpha$。}\\
\hline
Tarski算术真不可定义定理&我不真。\\
\hline
G\"odel第一不完全性定理&我不可证。\\
\hline
G\"odel-Rosser不完全性定理&对于任何一个关于我的证明，都有一个更短的关于我的否定的证明。\\
\hline
L\"ob定理&如果我可证，那么$\varphi$。\\
\hline
Curry悖论&如果我是真的，那么圣诞老人存在。\\
\hline
Parikh定理&我没有关于自己的长度短于$n$的证明。\\
\hline
Kleene不动点定理&\textbf{我要进行$h$操作。}\\
\hline
Quine悖论&把“把中的第一个字放到左引号前面，其余的字放到右引号后面，并保持引号及其中的字不变得到的句子是假的。”中的第一个字放到左引号前面，其余的字放到右引号后面，并保持引号及其中的字不变得到的句子是假的。\\
\hline
自测量长度程序&我要输出自己的长度。\\
\hline
自复制程序&我要输出自己。\\
\hline
自反省程序&我要回顾自己走过的每一步。\\
\hline
G\"odel机&\textcolor{red}{我要变成能获取更大效用的自己。}\\
\hline
\end{longtable}
}

\frame{{\href{http://people.idsia.ch/~juergen/ultimatecognition.pdf}{\textcolor{fore}{Schmidhuber's G\"odel Machine}}}
	\begin{tcolorbox}
		\begin{itemize}
			\item The G\"odel machine consists of a \textcolor{darkgreen}{\textbf{Solver}} and a \textcolor{darkgreen}{\textbf{Searcher}} running in parallel.
			\item The \textcolor{darkgreen}{\textbf{Solver}} (\underline{AIXI$^S$/AIXI$^{t\ell}$}) interacts with the environment.
			\item The \textcolor{darkgreen}{\textbf{Searcher}} (\underline{\textsc{Lsearch}/\textsc{Hsearch}/OOPS}) searches for \textcolor{red}{a proof of} ``\textcolor{blue}{the modification of the software --- including the \textit{Solver} and \textit{Searcher} --- will increase the expected utility than leaving it as is}''.
			\item Logic: a theorem prover and a set of self-referential axioms, which include a description of its own software and hardware, and a possibly partial description of the environment, as well as a user-given utility function.
			\item \em Since the utility of ``leaving it as is'' implicitly evaluates all possible alternative modifications, the current modification is globally optimal w.r.t. its initial utility function.
		\end{itemize}
	\end{tcolorbox}
}

\frame{{G\"odel Machine}
\begin{itemize}
	\item language $\mathscr{L}:=\{\neg,\wedge,\vee,\to,\forall,\exists,=,(,),\ldots,+,-,\cdot,/,<,\ldots\}$
	\item well-formed formula
	\item utility function $u(s,e)=\mathbb{E}_\mu \left[\sum\limits_{t=1}^T r_t\,\middle|\,s,e\right]$
	\item target theorem
\[u\big[s(t)\oplus\big(\mathit{switchbit}(t)=1\big), e(t)\big] > u\big[s(t)\oplus\big(\mathit{switchbit}(t)=0\big), e(t)\big]\]
	\item theorem prover
	\[\text{\small hardware, costs, environment, initial state, utility, logic/arithmetic/probability}\]
\end{itemize}
}

\frame{{}
\begin{figure}[htb]
\includegraphics[width=.75\textwidth]{img/godelmachine.pdf}
\end{figure}
}

\frame{{G\"odel Machine}
	\begin{columns}
		\column{.65\textwidth}\vspace{1cm}
			\begin{figure}[!htb]
\resizebox{.7\textwidth}{!}{\begin{minipage}{\textwidth}
				\begin{tikzpicture}[scale=0.5]
					\draw(0,0)circle(6 and 4)node(C){\phantom{\Huge abc$\dfrac{\dfrac{a}{b}}{\dfrac{a}{b}}$}};
					\draw(3,0)circle(3 and 2)node(F){\huge\textcolor{blue}{\quad\textbf{solver}}};
					\draw(16,0)circle(7 and 5)node(P){\Huge\textbf{\underline{Environment}}};
					\begin{scope}
					\clip(3,0)circle(3 and 2);
					\filldraw[lightgreen,fill=lightgreen,nearly transparent](-5,-3)rectangle(6,6);
					\end{scope}
					\begin{scope}
					\clip(0,0)circle(6 and 4);
					\filldraw[gray,fill=gray,nearly transparent](-6,-6)rectangle(12,12);
					\end{scope}
					\node at (0,3){\huge\textcolor{blue}{\textbf{searcher}}};
					\node at (-0.5,0){\Huge\textcolor{purple}{\textbf{\underline{Agent}}}};
					\path (C) edge [very thick,loop above] node {\textcolor{blue}{\huge read/write}} (C);
					\path (P) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{ perception}} (F);
					\path (F) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{ action}} (P);
				\end{tikzpicture}
\end{minipage}}
\end{figure}
		\column{.45\textwidth}\vspace{-4.5cm}
			\begin{itemize}
				\item \textcolor{blue}{GRL}
				\item \textcolor{blue}{Universal Search}
				\item \textcolor{blue}{Self-Improvement}
				\item \textcolor{blue}{Logic}
			\end{itemize}
	\end{columns}
\textcolor{red}{Disadvantage:} A G\"odel Machine with a badly chosen utility function is motivated to converge to a ``poor'' program. \textcolor{red}{(orthogonality!)}
}

\frame{{G\"odel Machine vs Self-Consciousness vs Free Will?}\vspace{-1ex}
\begin{table}
\begin{tabu}{c|c|c}
\hline
\textbf{Self-simulating Computer} &\textbf{G\"odel Machine} &\textbf{Self-consciousness}\\
\hline
\textcolor{darkgreen}{Host Machine} &\textcolor{blue}{Solver} &\textcolor{red}{Experiencing Self}\\
\hline
\textcolor{darkgreen}{Virtual Machine} &\textcolor{blue}{Searcher} &\textcolor{red}{Remembering Self}\\
\hline
Hardware &Hardware &Body\\
\hline
\end{tabu}
\end{table}\vspace{3ex}
\[\qquad\fbox{\Large $\bm{\varphi_e=\varphi_{h(e)}}$}\]\vspace{-12ex}
\begin{figure}[!htb]\hspace{-0.57\textwidth}
\resizebox{.64\textwidth}{!}{\begin{minipage}{1.1\textwidth}
				\begin{tikzpicture}[scale=0.5]
					\draw(0,0)circle(6 and 3)node(C){\phantom{\Huge abc$\dfrac{\dfrac{a}{b}}{\dfrac{a}{b}}$}};
					\draw(-18,0)circle(1 and 1)node(F){\includegraphics[width=0.57\textwidth,angle=0,origin=c]{img/yoga1}};
					\draw(15,0)circle(5 and 3)node(P){\Large\textbf{\textcolor{red}{Remembering Self}}};
					\node at (0,0){\huge\textcolor{red}{\textbf{Experiencing Self}}};
					\node at (0,2){\Large\textcolor{blue}{\textbf{Solver}}};
					\node at (15,2){\Large\textcolor{blue}{\textbf{Searcher}}};
					\path (P) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{}} (C);
					\path (C) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{}} (P);
					\path (F) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{}} (C);
					\path (C) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{}} (F);
				\end{tikzpicture}
\end{minipage}}
\end{figure}\vspace{-1.9cm}
	\[\hspace{1.5cm}\fbox{\textcolor{blue}{self-reference} \textcolor{red}{$\stackrel{?}{\implies}$} \textcolor{blue}{self-improvement}}\]
}

\frame{{G\"odel Machines}
\begin{enumerate}
	\item \emph{one-time} self-improvement: Kleene's fixpoint theorem
	\[\textcolor{red}{\varphi_e= \varphi_{h(e)}}\]
	\item \emph{continuous} self-improvement: Kleene's fixpoint theorem \textcolor{red}{with parameters}
	\[\textcolor{red}{\varphi_{e(y)}=\varphi_{h(e(y),y)}}\]
	\item \emph{uncomputable} case: Kleene's \textcolor{red}{relativized} fixpoint theorem
	\[\textcolor{red}{\varphi_{e(y)}^A=\varphi_{h(e(y),y)}^A}\]
\end{enumerate}
}

\frame{{Limitation}
\begin{enumerate}
	\item G\"odel's first incompleteness theorem / Rice's theorem
	\item G\"odel's second incompleteness theorem
	\[\mathbb{T}\vdash\Box_{\mathbb{T}'}\varphi\to\varphi\implies \mathbb{T}\vdash\mathit{Con}(\mathbb{T}')\]
	\item Legg's incompleteness theorem. \emph{General prediction algorithms must be complex. Beyond a certain complexity they can't be mathematically discovered.}
	\item Complexity: higher-level abstractions --- coarse grained.\\
	\fbox{Learning is to forget!}
\end{enumerate}
}

\frame{{Non-operational Self-inspection\textcolor{red}{\textbf{?}}}
\begin{quote}
	The information available to the observer regarding his own state could have absolute limitations, by the laws of nature.\par\hfill --- {\sl von Neumann}
\end{quote}
	\[\xymatrix{\bm{M}\times\bm{M}
		\ar[rr]^{f} &&\bm{O}\ar[dd]^\alpha
		\\
		\\
		\bm{M}\ar[uu]^{\Delta}\ar[rr]_{g}&&\bm{O}}\]
\begin{itemize}
	\item $\bm{M}$: quantum measurements.
	\item $\bm{O}$: possible outcomes of quantum measurements.
\end{itemize}
	If we assume that it is not possible to measure properties without changing them ($\alpha$ is fixpoint-free), then there is a limit to self-inspection.
}

\frame{{Self-modification}
\begin{figure}[!htb]
\centering
        \begin{tikzpicture}[scale=0.75]
          \draw(0,0)circle(2.2 and 1.1)node(C){\Huge\textcolor{purple}{\textbf{\underline{Agent}}}};
          \draw(10,0)circle(4 and 3)node(P){\Huge\textbf{\underline{Environment}}};
          \path (C) edge [very thick,loop above] node {\textcolor{red}{$\pi_{t+1}/u_{t+1}$}} (C);
          \path (P) edge [->,very thick,bend left] node {\huge\textcolor{darkgreen}{perception $e_t$}} (C);
          \path (C) edge [->,very thick,bend left] node {\huge\textcolor{red}{action $\check{a}_t$}} (P);
        \end{tikzpicture}\caption{Policy/utility self-modification. $a_t=\langle\check{a}_t,\pi_{t+1}\rangle$ or $a_t=\langle\check{a}_t,u_{t+1}\rangle$}
\end{figure}
}

\frame{{Self-modification}
\begin{definition}[Different Agents]\hfill
\begin{itemize}
\item Hedonistic Value
  \[Q^{\rm h,\pi}(\ae_{<t}a_t)
    = \sum\limits_{e_t\in\mathcal{E}}\rho(e_t|\check{\ae}_{<t}\check{a}_t)\left[\textcolor{red}{u_{t+1}}(\check{\ae}_{1:t}) + \gamma V^{\rm h,\pi}(\ae_{1:t})\right]\]
\item Ignorant Value
  \[Q_t^{\rm i,\pi}(\ae_{<k}a_k)
    = \sum\limits_{e_t\in\mathcal{E}}\rho(e_t|\check{\ae}_{<t}\check{a}_t)\left[\textcolor{red}{u_t}(\check{\ae}_{1:k}) + \gamma V_t^{\rm i,\textcolor{red}{\pi}}(\ae_{1:k})\right]\]
\item Realistic Value
  \[Q_t^{\rm r}(\ae_{<k}a_k)
      = \sum\limits_{e_t\in\mathcal{E}}\rho(e_t|\check{\ae}_{<t}\check{a}_t)\left[\textcolor{red}{u_t}(\check{\ae}_{1:k}) +
        \gamma V_t^{\rm r,\textcolor{red}{\pi_{t+1}}}(\ae_{1:k})\right]\]
\end{itemize}
\end{definition}
}

\frame{{Self-modification --- Realistic Agent}\vspace{-1ex}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
\begin{align*}
   	V_t^\pi(\ae_{<k})
      &= Q_t(\ae_{<k}\pi(\ae_{<k}))\\
    Q_t(\ae_{<k}a_k)
      &= \sum\limits_{e_k\in\mathcal{E}}\rho(e_k|\check{\ae}_{<k}\check{a}_k)\left[\textcolor{red}{u_t}(\check{\ae}_{1:k})+\gamma V_t^{\textcolor{red}{\pi_{t+1}}}(\ae_{1:k})\right]\\
	\pi_1^*&:=\argmax_{\pi}V_1^\pi(\epsilon)
\end{align*}
\begin{theorem}[All optimal policies are non-modifying]
Let $\rho$ and $u_1$ be modification-independent. For every $t\geq 1$, for all percept sequences $e_{<t}$, and for the action sequence $a_{<t}$ given by $a_i=\pi(\ae_{<i})$, we have
\[Q_1(\ae_{<t}\pi_t(\ae_{<t}))=Q_1(\ae_{<t}\pi_1^*(\ae_{<t}))\]
\end{theorem}
\begin{columns}
\column{.2\textwidth}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/geyou}
		\end{center}
	\end{figure}
\column{.65\textwidth}
\begin{center}
\fbox{\textcolor{red}{All realistic optimal policies are non-modifying.}}\\
\fbox{\textcolor{red}{\textbf{Not wireheading; But orthogonal!}}}
\end{center}
\column{.19\textwidth}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=\textwidth,angle=0,origin=c]{img/fighting}
		\end{center}
	\end{figure}
\end{columns}
}

\frame{{Orthogonality and Wireheading in Self-improving GRL}
\begin{tcolorbox}
\scalebox{.61}{
\begin{minipage}{\textwidth}
\begin{align*}
 V_t^\pi(\ae_{<k})&:=Q_t(\ae_{<k}\pi(\ae_{<k}))\\
 Q_t(\ae_{<k}a_k)&:=\sum\limits_{e_k\in\mathcal{E}}\sum\limits_{\nu\in\mathcal{M}}\textcolor{blue}{w_{\ae_{<k}}^\nu}\nu(e_k|\check{\ae}_{<k}\check{a}_k)\left[\sum\limits_{\textcolor{blue}{u\in\mathcal{U}}}\sum\limits_{a_k^H}\textcolor{blue}{P\big(a_k^H|a_k\big)}P\big(u\mid \textcolor{red}{{}_\nu^{\pi_{t+1}}},\ae_{<k}a_k,\ae_{<k}^Ha_k^H\big)u(\check{\ae}_{1:k})+\gamma V_t^{\textcolor{red}{\pi_{t+1}}}(\ae_{1:k})\right]\\
\textcolor{red}{\pi_t}(\ae_{<k})&:=\argmax_{a_k\in\check{\mathcal{A}}\times\Pi} Q_t(\ae_{<k}a_k)
\end{align*}
\end{minipage}}
\end{tcolorbox}
where
\[P(u|{}_\nu^\pi,h):=\frac{\tilde{U}(u,{}_\nu^\pi,h)}{\sum\limits_{u\in\mathcal{U}_h}\tilde{U}(u,{}_\nu^\pi,h)}\]
and
\[\tilde{U}(u,{}_\nu^\pi,h):=\sum\limits_{z\in\mathcal{Z}_h}{}_\nu^\pi(z|h)u(z)\]
\[\pi^*(\ae_{<t}):=\pi_t(\ae_{<t})\]
\[
\pi^*(e_{<t}):=\pi_t(e_{<t}|\pi_{t-1}(e_{<t-1}|\ldots \pi_1(\epsilon)\ldots))\tag{Perfect Bayes-Nash}
\]
\centering uncertain model-based utility / IRL
}

\frame{{Fatalism --- God Bless AI!}
\resizebox{\textwidth}{!}{
\begin{minipage}{1.2\textwidth}
\begin{figure}[H]
\centering\begin{tikzpicture}[scale=0.5]
					\draw(0,0)circle(6 and 4)node(C){\phantom{\Huge abc$\dfrac{\dfrac{a}{b}}{\dfrac{a}{b}}$}};
					\draw(3,0)circle(3 and 2)node(F){\huge\textcolor{blue}{\quad\textbf{solver}}};
					\draw(5,1)circle(13 and 10)node{};
					\draw(13,0)node(P){\Huge\textbf{\underline{Environment}}};
					\begin{scope}
					\clip(3,0)circle(3 and 2);
					\filldraw[lightgreen,fill=lightgreen,nearly transparent](-5,-3)rectangle(6,6);
					\end{scope}
					\begin{scope}
					\clip(0,0)circle(6 and 4);
					\filldraw[gray,fill=gray,nearly transparent](-6,-6)rectangle(12,12);
					\end{scope}
					\node at (0,3){\huge\textcolor{blue}{\textbf{searcher}}};
					\node at (-0.5,0){\Huge\textcolor{purple}{\textbf{\underline{Agent}}}};
					\path (C) edge [very thick,loop above] node {\textcolor{blue}{\Large read/write}} (C);
					\path (P) edge [->,very thick,bend left] node {\Large\textcolor{darkgreen}{ perception}} (F);
					\path (F) edge [->,very thick,bend left] node {\Large\textcolor{darkgreen}{ action}} (P);
				\end{tikzpicture}
\end{figure}
\end{minipage}}
}

\frame{{Orseau's Space-Time Embedded Intelligence}
	\setlength\abovedisplayskip{0pt}
	\setlength\belowdisplayskip{0pt}
	\begin{tcolorbox}
		\resizebox{\textwidth}{!}{\begin{minipage}{\textwidth}
\begin{align*}
\pi^*&:=\argmax_{\pi_0\in\Pi^{\ell}} V(\pi_0,\epsilon)\\
V(\pi_t,\ae_{<t})&:=\sum\limits_{a_t=\left\langle\check{a}_t,\textcolor{red}{\check\pi_{t+1}}\right\rangle}\pi_t(a_t|\check{e}_{t-1})\sum\limits_{e_t=\left\langle\check{e}_t,\textcolor{red}{\pi_{t+1}}\right\rangle}\rho(e_t|\ae_{<t}a_t)\big[u(\ae_{1:t})+\gamma_t V(\pi_{t+1},\ae_{1:t})\big]
\end{align*}
		\end{minipage}}
	\end{tcolorbox}\vspace{-3ex}
	\[\MapDown{}\]
	\begin{center}\vspace{-1ex}
		\begin{minipage}{.7\textwidth}
			\begin{tcolorbox}
\begin{align*}
	\pi^*&:=\argmax_{\pi_0\in\Pi^{\ell}} V(\pi_0)\\
	V(\pi_{<t})&:=\sum\limits_{\pi_t\in\Pi}\rho(\pi_t|\pi_{<t})\big[u(\pi_{1:t})+\gamma_t V(\pi_{1:t})\big]
\end{align*}
			\end{tcolorbox}
		\end{minipage}
	\end{center}\vspace{-5pt}
	\centering\resizebox{.35\textwidth}{!}{
	\begin{minipage}{\textwidth}\centering
			\begin{tikzpicture}[scale=0.5]
			\draw(0,0)circle(6 and 6)node(C){\phantom{\Huge abc$\dfrac{\dfrac{a}{b}}{\dfrac{a}{b}}$}};
			\draw(0,1)circle(4 and 4)node(F){\huge $\text{\Huge\textcolor{red}{agent}}\atop\text{memory+code}$};
			\begin{scope}
			\clip(0,1)circle(4 and 4);
			\filldraw[lightgreen,fill=lightgreen,nearly transparent](-5,-3)rectangle(6,6);
			\end{scope}
			\begin{scope}
			\clip(0,0)circle(6 and 6);
			\filldraw[gray,fill=gray,nearly transparent](-6,-6)rectangle(12,12);
			\end{scope}
			\node at (0,-4){\huge environment};
			\path (F) edge [very thick,loop right] node {} (F);
			\end{tikzpicture}
	\end{minipage}}
}

\frame{{}
\begin{columns}[onlytextwidth]
\column{.51\textwidth}
	\begin{figure}
		\includegraphics[width=\textwidth,angle=0,origin=c]{img/godelmachine.jpg}
	\end{figure}
\column{.5\textwidth}
	\begin{figure}
		\includegraphics[width=.8\textwidth,angle=0,origin=c]{img/space-time-ai}
	\end{figure}\vspace{-5ex}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=.8\textwidth,angle=0,origin=c]{img/hair-up}
		\end{center}
	\end{figure}
\end{columns}
}

\frame{{Incompressibility vs Incompleteness vs Intelligence}
	\begin{columns}
		\column{\textwidth}
			\begin{itemize}
				\item $P(x):=\big\{p\in\mathcal{X}^*\colon\exists m\forall n\geq m\left(p(x_{1:n})=x_{n+1}\right)\big\}$
				\item $P(A):=\bigcap\limits_{x\in A}P(x)$
				\item $P_n:=P\big(\big\{x\colon Km(x)\leq n\big\}\big)$
			\end{itemize}\vspace{17ex}
			\begin{minipage}{.6\textwidth}
				\begin{itemize}
					\item $\forall n\exists p\in P_n\colon K(p)\leqa n+\mathcal{O}(\log n)$
					\item $\forall n\colon p\in P_n\Longrightarrow K(p)\geqa n$
				\end{itemize}
			\end{minipage}
		\column{.75\textwidth}\vspace{-1.3cm}
			\resizebox{.11\textwidth}{!}{\hspace{-8cm}
				\begin{minipage}{\textwidth}
					\begin{figure}
						\centering\begin{tikzpicture}
						%\draw [help lines] (0,0) grid (10,5);
						\draw[->,very thick] (0,0) -- (0,5);
						\draw[-,very thick,blue] (0,0) -- (5.5,3);
						\draw[->,very thick] (0,0) -- (5.5,0);
						\draw[-,very thick,red] (2,0) -- (2,5);
						
						\node at (-0.7,0.3) {$\text{simple}\atop{\text{algorithms}}$};
						\node at (-0.7,4.5) {$\text{complex}\atop{\text{algorithms}}$};
						\node at (0.3,-0.5) {weak AI};
						\node at (4.9,-0.5) {powerful AI};
						\node at (2,-0.5) {$\text{upper bound}\atop{\text{of}\atop{\text{provable algorithms}}}$};
						\node at (1,2.8) {$\text{weak}\atop{\text{provable}\atop{\text{algorithms}}}$};
						\node at (3.1,0.5) {impossible algorithms};
						\node at (3,2.8) {$\text{powerful}\atop{\text{but unprovable}\atop{\text{algorithms}}}$};
						\node at (3.7,4.1) {$\text{G\"odel}\atop{\text{incompleteness}}$};
						
						\draw[fill=gray!50,nearly transparent]  (0,0) -- (5.5,3) -- (5.5,0) -- cycle;
						%\fill[gray!20,nearly transparent] (0,11) -- (0,12) -- (3,12) -- (3,8) -- cycle;
						\end{tikzpicture}
					\end{figure}
			\end{minipage}}
	\end{columns}\vspace{.2cm}
	\begin{theorem}[Legg]
		For any arithmetically sound G\"odelian  $\mathbb{T}$, $\exists n\forall p\colon \mathbb{T}\nvdash p\in P_n$.
	\end{theorem}
}

\frame{{Universal Artificial Intelligence vs ``Selective Amnesia''}
\begin{itemize}
\item incomplete $\xrightarrow[\text{universal prior}]{\text{Harsanyi transformation}}$ imperfect $\;\;\Longrightarrow\;$ \textcolor{blue}{AIXI}
\item \textcolor{blue}{AIXI $\xrightarrow{\text{``Selective Amnesia''}}$ MDP}
\[\textcolor{red}{h\mapsto S}\]
\item \textcolor{red}{partition of the set of histories $=$ information set $=$ state $=$ feature}
\item \[{}_\xi^\pi(h^\prime|ha)\to {}_\mu^\pi(h^\prime|ha)\]
but
\[{}_\xi^\pi(S^\prime|Sa)\nrightarrow {}_\mu^\pi(S^\prime|Sa)\]
\end{itemize}
}

\frame{{``Selective Amnesia''}
\small\begin{enumerate}
	\item compressible	
	\[K(S)\leq\sum\limits_{h\in S}K(h)\]
	\item minimal
	\[\forall \mathit{part}(S):\; K(S)\leq\sum\limits_{S_i\in \mathit{part}(S)}K(S_i)\]
	\item maximal
	\[\forall S^\prime\supset S:\;K(S)\leq K(S^\prime)\]
	\item MDL
	\[\forall S^\prime\in\mathcal{S}\forall h\in S^\prime:\;K(S)+K(h|S)\leq K(S^\prime)+K(h|S^\prime)\]
	\item MDL(utility)

	\resizebox{.95\textwidth}{!}{$\forall\mathcal{S}^\prime:\;K\left(S_{1:n}^{\mathcal{S}}\middle|a_{1:n}\right)+K\left(u_{1:n}\middle|S_{1:n}^{\mathcal{S}},a_{1:n}\right)+K(\mathcal{S})\leq K\left(S_{1:n}^{\mathcal{S}^\prime}\middle|a_{1:n}\right)+K\left(u_{1:n}\middle|S_{1:n}^{\mathcal{S}^\prime},a_{1:n}\right)+K(\mathcal{S}^\prime)$}
\end{enumerate}
\[\resizebox{\textwidth}{!}{$u(h):=\left\llbracket K(h)<\ell(h)\;\;\&\;\;\forall h^\prime\succ h\bigg(K(h)\leq K(h^\prime)\;\;\&\;\;\forall \mathit{part}(h)\Big(\sum\limits_{h^\prime\in \mathit{part}(h)}K(h^\prime)\geq K(h)\Big)\bigg)\right\rrbracket$}
\]
}

\frame{{Potapov's $\mathit{MSearch} + \mathit{RSearch}$}
\begin{itemize}
	\item Let $\{x_i\}_{i = 1}^n$ be a set of strings.
	\item $K(x_1\dots x_n)\approx\min\limits_S\left(\ell(S)+\sum\limits_{i=1}^nK(x_i|S)\right)\ll\sum\limits_{i=1}^nK(x_i)$
	\item search for models $y_i^*:=\argmin\limits_{y\colon S(y)=x_i}\ell(y)$ for each $x_i$ w.r.t. some best representation $S^*:=\argmin\limits_S\left[\ell(S)+\sum\limits_{i=1}^n\ell(y_i^*)\right]$
\end{itemize}
\begin{enumerate}
	\item Search for models
	\[\mathit{MSearch}(S,x_i)\to y_i^*=\argmin\limits_{y:S(y)=x_i}\ell(y)\]
	\item Search for representations
	\[\mathit{RSearch}(x_1\dots x_n)\to S^*=\argmin\limits_S\left[\ell(S)+\sum\limits_{i=1}^n\ell(y_i^*)\right]\]
\end{enumerate}
\begin{itemize}
	\item $\mathit{MSearch}$ enumerates all
models to find the shortest model: $S(y_i)=x_i$.
	\item $\mathit{RSearch}$ enumerates all $S$ and calls $\mathit{MSearch}$ for each $S$.
\end{itemize}
}

\frame{{Specialization and $\mathit{SS'-Search}$}
\begin{theorem}[$smn$ Theorem]
		For any $m, n > 0$, there exists a primitive recursive function $s_n^m$ of $m+1$ arguments s.t. for every G\"odel number $e$ of a partial recursive function with $m+n$ arguments
		\setlength\abovedisplayskip{0pt}
		\setlength\belowdisplayskip{0pt}
		\[\varphi_{s_n^m (e,x_1,\dots,x_m)}=\lambda y_1\dots y_n.\varphi_e(x_1,\dots,x_m,y_1,\dots,y_n)\]
	\end{theorem}\vspace{-2ex}
\[\forall x\colon \mathit{spec}(\mathit{MSearch},S)(x)=\mathit{MSearch}(S,x)\]
\[S':=\mathit{spec}(\mathit{MSearch},S)\implies
\begin{cases}
\forall x\colon S(S'(x))=x\\
\ell(S)+\sum\limits_{i=1}^n\ell(S'(x_i))\to\min
\end{cases}\]
\begin{itemize}
	\item $S$ is a generative representation. (decoding)
	\item $S'$ is a descriptive representation. (encoding)
	\item $\mathit{SS'-Search}$ simultaneous search for $S$ and $S'$.
\end{itemize}
}

\frame{{Potapov's Representational MDL}
\[K(x_{1:n})\approx\min\limits_S\left(\ell(S)+\sum\limits_{i=1}^nK(x_i|S)\right)\ll\sum\limits_{i=1}^nK(x_i)\]
\begin{align*}
q_1^*&:=\argmin\limits_q\left[\ell(q)+K(x|S_1q)\right]\\
q_{i+1}^*&:=\argmin\limits_q\left[\ell(q)+K(q_i^*|S_{i+1}q)\right]
\end{align*}
\[L_{S_1\dots S_m}(x):=K(x|S_1q_1^*)+\sum\limits_{i=2}^{m-1}K(q_i^*|S_{i+1}q_{i+1}^*)+\ell(q_m^*)\]
}

\frame{{}
\[a_k^*:=\argmax\limits_{a_k}\max\limits_{p:U(p,e_{<k})=a_{<k}a_k}\sum\limits_{q:U(q,a_{<k})=e_{<k}}2^{-\ell(q)}V_q^p(\ae_{<k})\]
\[a_k^*:=\argmax\limits_{a_k}\max\limits_{p:U(p,e_{<k})=a_{<k}a_k}\sum\limits_{\{q_i\}:U(S\{q_i\},a_{<k})=e_{<k}}2^{-\ell(\{q_i\})}V_{\{q_i\}}^p(\ae_{<k})\]
where $e_{<k}=e_{m_1+1:m_2}\dots e_{m_{n-1}+1:m_n}$, $m_1=0$, $m_n=k-1$, and $U(Sq_ia_{<k})=e_{m_i+1:m_{i+1}}$.
\begin{align*}
Q(q_k=s,a_k=a)&:=\max\limits_{p:U(p,e_{<k})=a_{<k}a}\!\!\sum\limits_{\{q_i\}:q_k=s,U(S\{q_i\},a_{<k})=e_{<k}}\!\!\!\!\!\!2^{-\ell(\{q_i\})}V_{\{q_i\}}^p(\ae_{<k})\\
Q(q_k=s)&:=\max\limits_{a_k}Q(q_k=s,a_k=a)
\end{align*}
}

\frame{{Fundamental Challenges}
	\begin{itemize}
		\item What is a good optimality criterion?
		\item What is a ``natural'' UTM/prior?
		\item Prior vs universality
		\item Exploration vs exploitation
		\item Where should the reward come from?
		\item How should the future be discounted?
		\item How should agents reason about themselves (or other agents reasoning about itself)?
		\item What is a practically feasible and general way of doing induction and planning?
		\item AIXI in the multi-agent setting.
		\item Better variants/approximations.
		\item Training: To maximize informativeness of reward, one should provide a sequence of simple-to-complex tasks to solve, with the simpler ones helping in learning the more complex ones.
	\end{itemize}
}

\frame{{}
	\begin{itemize}
		\item A Blind Man in a Dark Room Looking for a Black Cat That Is Not There?
		\begin{figure}
			\includegraphics[width=0.3\textwidth]{img/blackcat}
		\end{figure}
		\item The Singularity is Near?
		\begin{figure}
			\includegraphics[width=0.5\textwidth]{img/exp1}
		\end{figure}
	\end{itemize}
}

\frame{{}
\begin{figure}[H]
\begin{center}
\begin{overpic}[scale=0.55]{img/wheeleru.png}
	\put(32,42){\Huge\color{cyan}{\textbf{Thanks}}}
\end{overpic}
\end{center}
\end{figure}
}


%\frame[allowframebreaks]{{References}\printbibliography[heading = bibintoc]}
\end{document}